{
  "paragraphs": [
    {
      "text": "\r\n%spark.pyspark\r\n\r\nimport pyspark\r\nfrom pyspark.sql import SparkSession\r\n\r\n\r\nconfig \u003d pyspark.SparkConf()\r\nconfig.setMaster(\"spark://spark-master:7077\")\r\nspark \u003d SparkSession.builder \\\r\n            .appName(\"J2SchemaTeest\") \\\r\n            .config(\"forward_spark_s3_credentials\",True) \\\r\n            .config(\"fs.s3a.access.key\",\"\") \\\r\n            .config(\"fs.s3a.secret.key\",\"\") \\\r\n            .getOrCreate()\r\n            \r\n            ",
      "user": "anonymous",
      "dateUpdated": "2020-01-27 11:38:27.667",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "lineNumbers": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1579631520487_1647101872",
      "id": "20200121-183200_1639671760",
      "dateCreated": "2020-01-21 18:32:00.488",
      "dateStarted": "2020-01-26 20:25:35.043",
      "dateFinished": "2020-01-26 20:25:35.137",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\ndf \u003d spark.read.json(\"s3a://j2training/sample.json\")\n#df \u003d spark.readStream.json(\"s3a://j2training/data\")",
      "user": "anonymous",
      "dateUpdated": "2020-01-24 21:27:53.618",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "lineNumbers": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1579631556269_189961766",
      "id": "20200121-183236_977276561",
      "dateCreated": "2020-01-21 18:32:36.269",
      "dateStarted": "2020-01-26 20:25:35.145",
      "dateFinished": "2020-01-26 20:25:43.517",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime\nfrom pyspark.sql.functions import udf\n\nsourceExtractor \u003d udf(lambda source: BeautifulSoup(source).string.encode(\"utf8\"))",
      "user": "anonymous",
      "dateUpdated": "2020-01-24 21:28:07.217",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "lineNumbers": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1579714888235_-1166710382",
      "id": "20200122-174128_1762695518",
      "dateCreated": "2020-01-22 17:41:28.236",
      "dateStarted": "2020-01-26 20:25:43.595",
      "dateFinished": "2020-01-26 20:25:43.820",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\r\ndf2 \u003d df.select(sourceExtractor(\"source\").alias(\"source\"),\"created_at\",\"lang\").groupBy(\"source\",\"created_at\",\"lang\").count()",
      "user": "anonymous",
      "dateUpdated": "2020-01-24 21:28:07.640",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "lineNumbers": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1579714361458_1119114388",
      "id": "20200122-173241_479707786",
      "dateCreated": "2020-01-22 17:32:41.458",
      "dateStarted": "2020-01-26 20:25:43.896",
      "dateFinished": "2020-01-26 20:25:44.146",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\ndf2.toJSON()",
      "user": "anonymous",
      "dateUpdated": "2020-01-24 21:28:08.277",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "MapPartitionsRDD[127] at toJavaRDD at NativeMethodAccessorImpl.java:0"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1579724173800_-1587629187",
      "id": "20200122-201613_89995087",
      "dateCreated": "2020-01-22 20:16:13.800",
      "dateStarted": "2020-01-26 20:25:44.199",
      "dateFinished": "2020-01-26 20:25:44.990",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\r\nfrom datetime import datetime\r\nfrom pyspark.sql.functions import lit\r\nfrom pyspark.sql.functions import col\r\n\r\nrawJson \u003d spark.readStream \\\r\n                .json(\"s3a://j2training/data/raw/*/*/*/**\",df.schema) \\\r\n            \r\ndataDf \u003d rawJson.select(sourceExtractor(\"source\").alias(\"source\"),\"created_at\",col(\"lang\").alias(\"language\"),\"retweet_count\",\"text\",\"quote_count\",\"timestamp_ms\",col(\"user.id\").alias(\"user_id\"))\r\n                \r\n            \r\nnewUserDataDf \u003d rawJson.select(col(\"user.id\").alias(\"id_str\"),\"user.friends_count\",\"user.favourites_count\",\"user.description\",\"user.created_at\",\"user.name\",\"user.screen_name\",\"user.statuses_count\",\"user.location\").dropDuplicates(subset\u003d[\"id_str\"])",
      "user": "anonymous",
      "dateUpdated": "2020-01-24 21:28:09.610",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "lineNumbers": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1579632889697_2116010947",
      "id": "20200121-185449_2110594246",
      "dateCreated": "2020-01-21 18:54:49.697",
      "dateStarted": "2020-01-26 20:25:45.012",
      "dateFinished": "2020-01-26 20:25:50.649",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n",
      "user": "anonymous",
      "dateUpdated": "2020-01-24 21:28:14.810",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1579899285228_440500529",
      "id": "20200124-205445_317659596",
      "dateCreated": "2020-01-24 20:54:45.228",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\nfrom datetime import datetime\nimport json\n\nclass DateTimeEncoder(json.JSONEncoder):\n    def default(self, o):\n        if isinstance(o, datetime):\n            return o.isoformat()\n\n        return json.JSONEncoder.default(self, o)",
      "user": "anonymous",
      "dateUpdated": "2020-01-24 21:28:14.860",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1579724949258_326461629",
      "id": "20200122-202909_568994244",
      "dateCreated": "2020-01-22 20:29:09.258",
      "dateStarted": "2020-01-26 20:25:50.678",
      "dateFinished": "2020-01-26 20:25:50.740",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\ndef saveFileName(filename,s3Dir):\n    import psycopg2\n    con \u003d psycopg2.connect(database\u003d\"airflow\", user\u003d\"airflow\", password\u003d\"airflow\", host\u003d\"postgres\", port\u003d\"5432\")\n    cur \u003d con.cursor()\n    cur.execute(\"CREATE TABLE IF NOT EXISTS processed_files (id SERIAL PRIMARY KEY, file_name text,created_at TIMESTAMP,output_dir text )\")\n    cur.execute(\"\"\"INSERT INTO processed_files(file_name,created_at,output_dir) values ( %s ,CURRENT_TIMESTAMP, %s )\"\"\",(filename,s3Dir))\n    print(\"File name saved\")\n    con.commit()\n    con.close()",
      "user": "anonymous",
      "dateUpdated": "2020-01-24 21:28:15.043",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "lineNumbers": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1579830521503_1081135624",
      "id": "20200124-014841_1071891767",
      "dateCreated": "2020-01-24 01:48:41.503",
      "dateStarted": "2020-01-26 20:25:50.778",
      "dateFinished": "2020-01-26 20:25:50.841",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\nimport time\n\nimport uuid\nimport json\n\nrowToJson \u003d lambda x: x\n\nfrom datetime import date, datetime\n\ndef json_serial(obj):\n    \"\"\"JSON serializer for objects not serializable by default json code\"\"\"\n\n    if isinstance(obj, (datetime, date)):\n        return obj.isoformat()\n    raise TypeError (\"Type %s not serializable\" % type(obj))\n        \ndef mapJsonDF(rows):\n    print rows\n    row_list \u003d list(rows)\n    jsonList \u003d [json.dumps(row.asDict(),cls\u003dDateTimeEncoder) for row in row_list]\n    return jsonList\n\ndef sendToS3(rdd,s3Dir):\n    import boto3\n    s3 \u003d boto3.resource(\n        \u0027s3\u0027,\n        region_name\u003d\u0027us-east-1\u0027,\n        aws_access_key_id\u003d\"\",\n        aws_secret_access_key\u003d\"\"\n        )\n    row_list \u003d list(rdd)\n    year \u003d datetime.today().year\n    month \u003d datetime.today().month\n    day \u003d datetime.today().day\n    hour \u003d datetime.today().hour\n    minutes \u003d datetime.today().minute\n    print \u0027data/{0}/{1}/{2}/{3}/tweets-{4}-{5}.json\u0027.format(year,month,day,hour,minutes,str(uuid.uuid4()))\n    data \u003d str.join(\u0027\\n\u0027, map(rowToJson,row_list))\n    bites \u003d bytes(data.encode(\"utf-8\"))\n    file_name \u003d \u0027{0}/{1}/{2}/{3}/{4}/tweets-{5}-{6}.json\u0027.format(s3Dir,year,month,day,hour,minutes,str(uuid.uuid4()))\n    s3.Object(\u0027j2training\u0027,file_name ).put(Body\u003dbites) \n    saveFileName(file_name,s3Dir)\n    \ndef foreach_batch_function2(df, epoch_id,s3Dir):\n    dfJson \u003d df\n    from datetime import datetime\n\n    year \u003d datetime.today().year\n    month \u003d datetime.today().month\n    day \u003d datetime.today().day\n    hour \u003d datetime.today().hour\n    minutes \u003d datetime.today().minute\n    print(\"dfis \" + str(dfJson.rdd))\n    dfJson.rdd.mapPartitions(mapJsonDF).foreachPartition(lambda rdd:sendToS3(rdd,s3Dir))\n",
      "user": "anonymous",
      "dateUpdated": "2020-01-27 11:38:34.125",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1579669861048_-818468490",
      "id": "20200122-051101_1395511314",
      "dateCreated": "2020-01-22 05:11:01.048",
      "dateStarted": "2020-01-26 20:25:50.879",
      "dateFinished": "2020-01-26 20:25:50.979",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\ndef watchQuery(query):\n    terminate \u003d False\n    while not terminate:\n        time.sleep(30)\n        print \"looping\"\n        print query.status[\"isDataAvailable\"]\n        terminate \u003d  query.status[\"isDataAvailable\"] \u003d\u003d False\n    \n        query.stop()",
      "user": "anonymous",
      "dateUpdated": "2020-01-24 21:28:15.469",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1579808411120_912892058",
      "id": "20200123-194011_573367669",
      "dateCreated": "2020-01-23 19:40:11.120",
      "dateStarted": "2020-01-26 20:25:51.078",
      "dateFinished": "2020-01-26 20:25:51.179",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\nimport time\nimport threading\n\nquery \u003d newUserDataDf.repartition(2).writeStream \\\n            .option(\"checkpointLocation\", \"s3a://j2training/spark/checkpoint/users\") \\\n            .outputMode(\"update\") \\\n            .foreachBatch(lambda df,epoch:foreach_batch_function2(df,epoch,\"data/processed/users\")) \\\n            .start()\n\nprint query\n    \nt1 \u003d threading.Thread(target\u003dwatchQuery, args\u003d[query])\nt1.start()   \n",
      "user": "anonymous",
      "dateUpdated": "2020-01-24 21:28:15.599",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u003cpyspark.sql.streaming.StreamingQuery object at 0x7faec7578990\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1579806470113_-1039981002",
      "id": "20200123-190750_1440586516",
      "dateCreated": "2020-01-23 19:07:50.113",
      "dateStarted": "2020-01-26 20:25:51.279",
      "dateFinished": "2020-01-26 20:25:52.404",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\nimport time\n\nquery2 \u003d dataDf.repartition(4).writeStream\\\n            .option(\"checkpointLocation\", \"s3a://j2training/spark/checkpoint/tweets\") \\\n            .outputMode(\"update\") \\\n            .foreachBatch(lambda df,epoch:foreach_batch_function2(df,epoch,\"data/processed/tweets\")) \\\n            .start()\n\nprint query2\n\nt2 \u003d threading.Thread(target\u003dwatchQuery, args\u003d[query2])\nt2.start()  \nt1.join()\nt2.join()",
      "user": "anonymous",
      "dateUpdated": "2020-01-24 21:28:21.671",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u003cpyspark.sql.streaming.StreamingQuery object at 0x7faec7578f50\u003e\ndfis MapPartitionsRDD[143] at javaToPython at NativeMethodAccessorImpl.java:0\ndfis MapPartitionsRDD[157] at javaToPython at NativeMethodAccessorImpl.java:0\nlooping\nTrue\nlooping\nTrue\nlooping\nFalse\nlooping\nFalse\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1579722275187_979573224",
      "id": "20200122-194435_1903717418",
      "dateCreated": "2020-01-22 19:44:35.187",
      "dateStarted": "2020-01-26 20:25:52.502",
      "dateFinished": "2020-01-26 20:32:01.091",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n",
      "user": "anonymous",
      "dateUpdated": "2020-01-24 21:38:01.271",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1579806705884_-249656992",
      "id": "20200123-191145_1973897025",
      "dateCreated": "2020-01-23 19:11:45.884",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "ExportToSchema",
  "id": "2EXQEGYMG",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "spark:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {}
}