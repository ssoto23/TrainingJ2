 WARN [2020-01-22 16:47:07,487] ({main} ZeppelinConfiguration.java[create]:117) - Failed to load configuration, proceeding with a default
 INFO [2020-01-22 16:47:07,617] ({main} ZeppelinConfiguration.java[create]:129) - Server Host: 0.0.0.0
 INFO [2020-01-22 16:47:07,617] ({main} ZeppelinConfiguration.java[create]:131) - Server Port: 8090
 INFO [2020-01-22 16:47:07,617] ({main} ZeppelinConfiguration.java[create]:135) - Context Path: /
 INFO [2020-01-22 16:47:07,623] ({main} ZeppelinConfiguration.java[create]:136) - Zeppelin Version: 0.8.1
 INFO [2020-01-22 16:47:07,675] ({main} Log.java[initialized]:193) - Logging initialized @808ms to org.eclipse.jetty.util.log.Slf4jLog
 WARN [2020-01-22 16:47:07,937] ({main} ServerConnector.java[setSoLingerTime]:458) - Ignoring deprecated socket close linger time
 INFO [2020-01-22 16:47:08,056] ({main} ZeppelinServer.java[setupWebAppContext]:413) - ZeppelinServer Webapp path: /zeppelin/webapps
 INFO [2020-01-22 16:47:08,230] ({main} ZeppelinServer.java[main]:239) - Starting zeppelin server
 INFO [2020-01-22 16:47:08,239] ({main} Server.java[doStart]:370) - jetty-9.4.14.v20181114; built: 2018-11-14T21:20:31.478Z; git: c4550056e785fb5665914545889f21dc136ad9e6; jvm 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12
 INFO [2020-01-22 16:47:14,208] ({main} StandardDescriptorProcessor.java[visitServlet]:283) - NO JSP Support for /, did not find org.eclipse.jetty.jsp.JettyJspServlet
 INFO [2020-01-22 16:47:14,254] ({main} DefaultSessionIdManager.java[doStart]:365) - DefaultSessionIdManager workerName=node0
 INFO [2020-01-22 16:47:14,255] ({main} DefaultSessionIdManager.java[doStart]:370) - No SessionScavenger set, using defaults
 INFO [2020-01-22 16:47:14,261] ({main} HouseKeeper.java[startScavenging]:149) - node0 Scavenging every 660000ms
 INFO [2020-01-22 16:47:14,930] ({main} SchedulerFactory.java[<init>]:59) - Scheduler Thread Pool Size: 100
 WARN [2020-01-22 16:47:14,959] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir
 WARN [2020-01-22 16:47:14,962] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir
 INFO [2020-01-22 16:47:15,110] ({main} InterpreterSettingManager.java[<init>]:165) - Using RecoveryStorage: org.apache.zeppelin.interpreter.recovery.NullRecoveryStorage
 INFO [2020-01-22 16:47:15,112] ({main} InterpreterSettingManager.java[<init>]:169) - Using LifecycleManager: org.apache.zeppelin.interpreter.lifecycle.NullLifecycleManager
 INFO [2020-01-22 16:47:15,203] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: cassandra
 INFO [2020-01-22 16:47:15,212] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: md
 INFO [2020-01-22 16:47:15,225] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: sap
 INFO [2020-01-22 16:47:15,236] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: sh
 INFO [2020-01-22 16:47:15,243] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: angular
 WARN [2020-01-22 16:47:15,256] ({main} InterpreterSettingManager.java[init]:331) - No interpreter-setting.json found in /zeppelin/interpreter/lib
 INFO [2020-01-22 16:47:15,260] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: hbase
 INFO [2020-01-22 16:47:15,267] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: elasticsearch
 WARN [2020-01-22 16:47:15,692] ({main} InterpreterSettingManager.java[init]:331) - No interpreter-setting.json found in /zeppelin/interpreter/scio
 INFO [2020-01-22 16:47:15,717] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: alluxio
 INFO [2020-01-22 16:47:15,725] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: jdbc
 INFO [2020-01-22 16:47:15,747] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: neo4j
 INFO [2020-01-22 16:47:15,750] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: python
 INFO [2020-01-22 16:47:15,755] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: pig
 INFO [2020-01-22 16:47:15,787] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: spark
 INFO [2020-01-22 16:47:15,790] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: lens
 INFO [2020-01-22 16:47:15,793] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: bigquery
 INFO [2020-01-22 16:47:15,797] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: file
 INFO [2020-01-22 16:47:15,808] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: ignite
 INFO [2020-01-22 16:47:15,858] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: kylin
 INFO [2020-01-22 16:47:15,903] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: livy
 INFO [2020-01-22 16:47:15,909] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: groovy
 INFO [2020-01-22 16:47:15,946] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: flink
 WARN [2020-01-22 16:47:15,958] ({main} InterpreterSettingManager.java[init]:331) - No interpreter-setting.json found in /zeppelin/interpreter/${interpreter.name}
 INFO [2020-01-22 16:47:15,958] ({main} LocalConfigStorage.java[loadInterpreterSettings]:63) - Load Interpreter Setting from file: /zeppelin/conf/interpreter.json
 INFO [2020-01-22 16:47:16,180] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting ignite from interpreter.json
 INFO [2020-01-22 16:47:16,181] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting python from interpreter.json
 INFO [2020-01-22 16:47:16,183] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting sap from interpreter.json
 INFO [2020-01-22 16:47:16,185] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting jdbc from interpreter.json
 INFO [2020-01-22 16:47:16,187] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting lens from interpreter.json
 INFO [2020-01-22 16:47:16,190] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting pig from interpreter.json
 INFO [2020-01-22 16:47:16,190] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting flink from interpreter.json
 INFO [2020-01-22 16:47:16,191] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting angular from interpreter.json
 INFO [2020-01-22 16:47:16,215] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting livy from interpreter.json
 INFO [2020-01-22 16:47:16,218] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting neo4j from interpreter.json
 INFO [2020-01-22 16:47:16,219] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting file from interpreter.json
 INFO [2020-01-22 16:47:16,219] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting groovy from interpreter.json
 INFO [2020-01-22 16:47:16,220] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting elasticsearch from interpreter.json
 INFO [2020-01-22 16:47:16,222] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting cassandra from interpreter.json
 INFO [2020-01-22 16:47:16,225] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting sh from interpreter.json
 INFO [2020-01-22 16:47:16,227] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting spark from interpreter.json
 INFO [2020-01-22 16:47:16,228] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting md from interpreter.json
 INFO [2020-01-22 16:47:16,229] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting alluxio from interpreter.json
 INFO [2020-01-22 16:47:16,240] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting bigquery from interpreter.json
 INFO [2020-01-22 16:47:16,241] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting hbase from interpreter.json
 INFO [2020-01-22 16:47:16,242] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting kylin from interpreter.json
 INFO [2020-01-22 16:47:16,396] ({main} LocalConfigStorage.java[save]:53) - Save Interpreter Setting to /zeppelin/conf/interpreter.json
 INFO [2020-01-22 16:47:16,963] ({main} VfsLog.java[info]:138) - Using "/tmp/vfs_cache" as temporary files store.
 INFO [2020-01-22 16:47:17,143] ({main} GitNotebookRepo.java[<init>]:64) - Opening a git repo at '/zeppelin/notebook'
 INFO [2020-01-22 16:47:17,572] ({main} GitNotebookRepo.java[<init>]:67) - Git repo /zeppelin/notebook/.git does not exist, creating a new one
 INFO [2020-01-22 16:47:17,935] ({main} NotebookRepoSync.java[<init>]:77) - Instantiate NotebookRepo: org.apache.zeppelin.notebook.repo.GitNotebookRepo
 WARN [2020-01-22 16:47:18,213] ({main} NotebookAuthorization.java[getInstance]:86) - Notebook authorization module was called without initialization, initializing with default configuration
 WARN [2020-01-22 16:47:18,215] ({main} LocalConfigStorage.java[loadNotebookAuthorization]:77) - NotebookAuthorization file /zeppelin/conf/notebook-authorization.json is not existed
 INFO [2020-01-22 16:47:18,216] ({main} Credentials.java[loadFromFile]:121) - /zeppelin/conf/credentials.json
 INFO [2020-01-22 16:47:18,292] ({main} StdSchedulerFactory.java[instantiate]:1184) - Using default implementation for ThreadExecutor
 INFO [2020-01-22 16:47:18,302] ({main} SimpleThreadPool.java[initialize]:268) - Job execution threads will use class loader of thread: main
 INFO [2020-01-22 16:47:18,346] ({main} SchedulerSignalerImpl.java[<init>]:61) - Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
 INFO [2020-01-22 16:47:18,359] ({main} QuartzScheduler.java[<init>]:240) - Quartz Scheduler v.2.2.1 created.
 INFO [2020-01-22 16:47:18,361] ({main} RAMJobStore.java[initialize]:155) - RAMJobStore initialized.
 INFO [2020-01-22 16:47:18,367] ({main} QuartzScheduler.java[initialize]:305) - Scheduler meta-data: Quartz Scheduler (v2.2.1) 'DefaultQuartzScheduler' with instanceId 'NON_CLUSTERED'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'org.quartz.simpl.SimpleThreadPool' - with 10 threads.
  Using job-store 'org.quartz.simpl.RAMJobStore' - which does not support persistence. and is not clustered.

 INFO [2020-01-22 16:47:18,368] ({main} StdSchedulerFactory.java[instantiate]:1339) - Quartz scheduler 'DefaultQuartzScheduler' initialized from default resource file in Quartz package: 'quartz.properties'
 INFO [2020-01-22 16:47:18,369] ({main} StdSchedulerFactory.java[instantiate]:1343) - Quartz scheduler version: 2.2.1
 INFO [2020-01-22 16:47:18,370] ({main} QuartzScheduler.java[start]:575) - Scheduler DefaultQuartzScheduler_$_NON_CLUSTERED started.
 INFO [2020-01-22 16:47:18,378] ({main} Notebook.java[<init>]:127) - Notebook indexing started...
 INFO [2020-01-22 16:47:18,406] ({main} LuceneSearch.java[addIndexDocs]:305) - Indexing 0 notebooks took 28ms
 INFO [2020-01-22 16:47:18,410] ({main} Notebook.java[<init>]:129) - Notebook indexing finished: 0 indexed in 0s
 INFO [2020-01-22 16:47:18,411] ({main} Helium.java[loadConf]:103) - Add helium local registry /zeppelin/helium
 INFO [2020-01-22 16:47:18,414] ({main} Helium.java[loadConf]:100) - Add helium online registry https://s3.amazonaws.com/helium-package/helium.json
 WARN [2020-01-22 16:47:18,419] ({main} Helium.java[loadConf]:111) - /zeppelin/conf/helium.json does not exists
 INFO [2020-01-22 16:47:20,794] ({main} ContextHandler.java[doStart]:855) - Started o.e.j.w.WebAppContext@5af97850{zeppelin-web,/,file:///zeppelin/webapps/webapp/,AVAILABLE}{/zeppelin/zeppelin-web-0.8.1.war}
 INFO [2020-01-22 16:47:20,811] ({main} AbstractConnector.java[doStart]:292) - Started ServerConnector@5974f626{HTTP/1.1,[http/1.1]}{0.0.0.0:8090}
 INFO [2020-01-22 16:47:20,811] ({main} Server.java[doStart]:407) - Started @13949ms
 INFO [2020-01-22 16:47:20,812] ({main} ZeppelinServer.java[main]:249) - Done, zeppelin server started
 INFO [2020-01-22 16:47:48,883] ({Thread-34} ZeppelinServer.java[run]:253) - Shutting down Zeppelin Server ... 
 INFO [2020-01-22 16:47:48,897] ({Thread-34} AbstractConnector.java[doStop]:341) - Stopped ServerConnector@5974f626{HTTP/1.1,[http/1.1]}{0.0.0.0:8090}
 INFO [2020-01-22 16:47:48,898] ({Thread-34} HouseKeeper.java[stopScavenging]:167) - node0 Stopped scavenging
 INFO [2020-01-22 16:47:50,482] ({Thread-34} ContextHandler.java[doStop]:1045) - Stopped o.e.j.w.WebAppContext@5af97850{zeppelin-web,/,null,UNAVAILABLE}{/zeppelin/zeppelin-web-0.8.1.war}
 INFO [2020-01-22 16:47:50,484] ({Thread-35} InterpreterSetting.java[close]:483) - Close InterpreterSetting: ignite
 INFO [2020-01-22 16:47:50,484] ({Thread-36} InterpreterSetting.java[close]:483) - Close InterpreterSetting: ignite
 INFO [2020-01-22 16:47:50,485] ({Thread-39} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sap
 INFO [2020-01-22 16:47:50,486] ({Thread-38} InterpreterSetting.java[close]:483) - Close InterpreterSetting: python
 INFO [2020-01-22 16:47:50,485] ({Thread-41} InterpreterSetting.java[close]:483) - Close InterpreterSetting: lens
 INFO [2020-01-22 16:47:50,485] ({Thread-40} InterpreterSetting.java[close]:483) - Close InterpreterSetting: jdbc
 INFO [2020-01-22 16:47:50,486] ({Thread-46} InterpreterSetting.java[close]:483) - Close InterpreterSetting: jdbc
 INFO [2020-01-22 16:47:50,484] ({Thread-37} InterpreterSetting.java[close]:483) - Close InterpreterSetting: python
 INFO [2020-01-22 16:47:50,486] ({Thread-45} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sap
 INFO [2020-01-22 16:47:50,487] ({Thread-47} InterpreterSetting.java[close]:483) - Close InterpreterSetting: lens
 INFO [2020-01-22 16:47:50,485] ({Thread-42} InterpreterSetting.java[close]:483) - Close InterpreterSetting: pig
 INFO [2020-01-22 16:47:50,488] ({Thread-49} InterpreterSetting.java[close]:483) - Close InterpreterSetting: flink
 INFO [2020-01-22 16:47:50,487] ({Thread-48} InterpreterSetting.java[close]:483) - Close InterpreterSetting: pig
 INFO [2020-01-22 16:47:50,488] ({Thread-50} InterpreterSetting.java[close]:483) - Close InterpreterSetting: angular
 INFO [2020-01-22 16:47:50,488] ({Thread-51} InterpreterSetting.java[close]:483) - Close InterpreterSetting: livy
 INFO [2020-01-22 16:47:50,488] ({Thread-52} InterpreterSetting.java[close]:483) - Close InterpreterSetting: neo4j
 INFO [2020-01-22 16:47:50,489] ({Thread-53} InterpreterSetting.java[close]:483) - Close InterpreterSetting: file
 INFO [2020-01-22 16:47:50,489] ({Thread-54} InterpreterSetting.java[close]:483) - Close InterpreterSetting: groovy
 INFO [2020-01-22 16:47:50,489] ({Thread-55} InterpreterSetting.java[close]:483) - Close InterpreterSetting: elasticsearch
 INFO [2020-01-22 16:47:50,489] ({Thread-56} InterpreterSetting.java[close]:483) - Close InterpreterSetting: cassandra
 INFO [2020-01-22 16:47:50,490] ({Thread-58} InterpreterSetting.java[close]:483) - Close InterpreterSetting: spark
 INFO [2020-01-22 16:47:50,490] ({Thread-57} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sh
 INFO [2020-01-22 16:47:50,506] ({Thread-43} InterpreterSetting.java[close]:483) - Close InterpreterSetting: flink
 INFO [2020-01-22 16:47:50,507] ({Thread-59} InterpreterSetting.java[close]:483) - Close InterpreterSetting: md
 INFO [2020-01-22 16:47:50,508] ({Thread-75} InterpreterSetting.java[close]:483) - Close InterpreterSetting: hbase
 INFO [2020-01-22 16:47:50,508] ({Thread-69} InterpreterSetting.java[close]:483) - Close InterpreterSetting: cassandra
 INFO [2020-01-22 16:47:50,508] ({Thread-64} InterpreterSetting.java[close]:483) - Close InterpreterSetting: livy
 INFO [2020-01-22 16:47:50,508] ({Thread-44} InterpreterSetting.java[close]:483) - Close InterpreterSetting: angular
 INFO [2020-01-22 16:47:50,508] ({Thread-65} InterpreterSetting.java[close]:483) - Close InterpreterSetting: neo4j
 INFO [2020-01-22 16:47:50,508] ({Thread-66} InterpreterSetting.java[close]:483) - Close InterpreterSetting: file
 INFO [2020-01-22 16:47:50,508] ({Thread-67} InterpreterSetting.java[close]:483) - Close InterpreterSetting: groovy
 INFO [2020-01-22 16:47:50,508] ({Thread-68} InterpreterSetting.java[close]:483) - Close InterpreterSetting: elasticsearch
 INFO [2020-01-22 16:47:50,508] ({Thread-70} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sh
 INFO [2020-01-22 16:47:50,508] ({Thread-71} InterpreterSetting.java[close]:483) - Close InterpreterSetting: spark
 INFO [2020-01-22 16:47:50,508] ({Thread-72} InterpreterSetting.java[close]:483) - Close InterpreterSetting: md
 INFO [2020-01-22 16:47:50,508] ({Thread-74} InterpreterSetting.java[close]:483) - Close InterpreterSetting: bigquery
 INFO [2020-01-22 16:47:50,508] ({Thread-73} InterpreterSetting.java[close]:483) - Close InterpreterSetting: alluxio
 INFO [2020-01-22 16:47:50,511] ({Thread-76} InterpreterSetting.java[close]:483) - Close InterpreterSetting: kylin
 INFO [2020-01-22 16:47:50,508] ({Thread-60} InterpreterSetting.java[close]:483) - Close InterpreterSetting: alluxio
 INFO [2020-01-22 16:47:50,508] ({Thread-61} InterpreterSetting.java[close]:483) - Close InterpreterSetting: bigquery
 INFO [2020-01-22 16:47:50,508] ({Thread-62} InterpreterSetting.java[close]:483) - Close InterpreterSetting: hbase
 INFO [2020-01-22 16:47:50,508] ({Thread-63} InterpreterSetting.java[close]:483) - Close InterpreterSetting: kylin
 INFO [2020-01-22 16:47:50,518] ({Thread-34} NotebookRepoSync.java[close]:428) - Closing all notebook storages
 INFO [2020-01-22 16:47:53,539] ({Thread-34} ZeppelinServer.java[run]:264) - Bye
 WARN [2020-01-22 17:05:06,214] ({main} ZeppelinConfiguration.java[create]:117) - Failed to load configuration, proceeding with a default
 INFO [2020-01-22 17:05:06,629] ({main} ZeppelinConfiguration.java[create]:129) - Server Host: 0.0.0.0
 INFO [2020-01-22 17:05:06,631] ({main} ZeppelinConfiguration.java[create]:131) - Server Port: 8080
 INFO [2020-01-22 17:05:06,635] ({main} ZeppelinConfiguration.java[create]:135) - Context Path: /
 INFO [2020-01-22 17:05:06,667] ({main} ZeppelinConfiguration.java[create]:136) - Zeppelin Version: 0.8.2
 INFO [2020-01-22 17:05:06,824] ({main} Log.java[initialized]:193) - Logging initialized @3112ms to org.eclipse.jetty.util.log.Slf4jLog
 WARN [2020-01-22 17:05:07,786] ({main} ServerConnector.java[setSoLingerTime]:458) - Ignoring deprecated socket close linger time
 INFO [2020-01-22 17:05:08,231] ({main} ZeppelinServer.java[setupWebAppContext]:413) - ZeppelinServer Webapp path: /zeppelin/webapps
 INFO [2020-01-22 17:05:09,010] ({main} ZeppelinServer.java[main]:239) - Starting zeppelin server
 INFO [2020-01-22 17:05:09,018] ({main} Server.java[doStart]:370) - jetty-9.4.14.v20181114; built: 2018-11-14T21:20:31.478Z; git: c4550056e785fb5665914545889f21dc136ad9e6; jvm 1.8.0_222-8u222-b10-1ubuntu1~16.04.1-b10
 INFO [2020-01-22 17:05:23,013] ({main} StandardDescriptorProcessor.java[visitServlet]:283) - NO JSP Support for /, did not find org.eclipse.jetty.jsp.JettyJspServlet
 INFO [2020-01-22 17:05:23,091] ({main} DefaultSessionIdManager.java[doStart]:365) - DefaultSessionIdManager workerName=node0
 INFO [2020-01-22 17:05:23,092] ({main} DefaultSessionIdManager.java[doStart]:370) - No SessionScavenger set, using defaults
 INFO [2020-01-22 17:05:23,106] ({main} HouseKeeper.java[startScavenging]:149) - node0 Scavenging every 600000ms
 INFO [2020-01-22 17:05:25,198] ({main} SchedulerFactory.java[<init>]:59) - Scheduler Thread Pool Size: 100
 WARN [2020-01-22 17:05:25,264] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir
 WARN [2020-01-22 17:05:25,277] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir
 INFO [2020-01-22 17:05:25,645] ({main} InterpreterSettingManager.java[<init>]:165) - Using RecoveryStorage: org.apache.zeppelin.interpreter.recovery.NullRecoveryStorage
 INFO [2020-01-22 17:05:25,654] ({main} InterpreterSettingManager.java[<init>]:169) - Using LifecycleManager: org.apache.zeppelin.interpreter.lifecycle.NullLifecycleManager
 INFO [2020-01-22 17:05:25,922] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: cassandra
 INFO [2020-01-22 17:05:25,940] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: md
 INFO [2020-01-22 17:05:25,984] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: sap
 INFO [2020-01-22 17:05:26,026] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: sh
 INFO [2020-01-22 17:05:26,043] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: angular
 WARN [2020-01-22 17:05:26,087] ({main} InterpreterSettingManager.java[init]:331) - No interpreter-setting.json found in /zeppelin/interpreter/lib
 INFO [2020-01-22 17:05:26,109] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: hbase
 INFO [2020-01-22 17:05:26,128] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: elasticsearch
 WARN [2020-01-22 17:05:26,617] ({main} InterpreterSettingManager.java[init]:331) - No interpreter-setting.json found in /zeppelin/interpreter/scio
 INFO [2020-01-22 17:05:26,683] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: alluxio
 INFO [2020-01-22 17:05:26,711] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: jdbc
 INFO [2020-01-22 17:05:26,748] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: neo4j
 INFO [2020-01-22 17:05:26,787] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: python
 INFO [2020-01-22 17:05:26,801] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: pig
 INFO [2020-01-22 17:05:26,842] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: spark
 INFO [2020-01-22 17:05:26,869] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: lens
 INFO [2020-01-22 17:05:26,920] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: bigquery
 INFO [2020-01-22 17:05:26,952] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: file
 INFO [2020-01-22 17:05:26,972] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: ignite
 INFO [2020-01-22 17:05:27,010] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: kylin
 INFO [2020-01-22 17:05:27,063] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: livy
 INFO [2020-01-22 17:05:27,091] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: groovy
 INFO [2020-01-22 17:05:27,118] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: flink
 WARN [2020-01-22 17:05:27,143] ({main} InterpreterSettingManager.java[init]:331) - No interpreter-setting.json found in /zeppelin/interpreter/${interpreter.name}
 WARN [2020-01-22 17:05:27,146] ({main} LocalConfigStorage.java[loadInterpreterSettings]:60) - Interpreter Setting file /zeppelin/conf/interpreter.json is not existed
 INFO [2020-01-22 17:05:27,454] ({main} LocalConfigStorage.java[save]:53) - Save Interpreter Setting to /zeppelin/conf/interpreter.json
 INFO [2020-01-22 17:05:28,656] ({main} VfsLog.java[info]:138) - Using "/tmp/vfs_cache" as temporary files store.
 INFO [2020-01-22 17:05:29,030] ({main} GitNotebookRepo.java[<init>]:64) - Opening a git repo at '/zeppelin/notebook'
 INFO [2020-01-22 17:05:29,863] ({main} GitNotebookRepo.java[<init>]:67) - Git repo /zeppelin/notebook/.git does not exist, creating a new one
 INFO [2020-01-22 17:05:30,128] ({main} NotebookRepoSync.java[<init>]:77) - Instantiate NotebookRepo: org.apache.zeppelin.notebook.repo.GitNotebookRepo
 WARN [2020-01-22 17:05:30,484] ({main} NotebookAuthorization.java[getInstance]:86) - Notebook authorization module was called without initialization, initializing with default configuration
 WARN [2020-01-22 17:05:30,487] ({main} LocalConfigStorage.java[loadNotebookAuthorization]:77) - NotebookAuthorization file /zeppelin/conf/notebook-authorization.json is not existed
 INFO [2020-01-22 17:05:30,490] ({main} Credentials.java[loadFromFile]:121) - /zeppelin/conf/credentials.json
 INFO [2020-01-22 17:05:30,649] ({main} StdSchedulerFactory.java[instantiate]:1184) - Using default implementation for ThreadExecutor
 INFO [2020-01-22 17:05:30,654] ({main} SimpleThreadPool.java[initialize]:268) - Job execution threads will use class loader of thread: main
 INFO [2020-01-22 17:05:30,740] ({main} SchedulerSignalerImpl.java[<init>]:61) - Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
 INFO [2020-01-22 17:05:30,750] ({main} QuartzScheduler.java[<init>]:240) - Quartz Scheduler v.2.2.1 created.
 INFO [2020-01-22 17:05:30,754] ({main} RAMJobStore.java[initialize]:155) - RAMJobStore initialized.
 INFO [2020-01-22 17:05:30,772] ({main} QuartzScheduler.java[initialize]:305) - Scheduler meta-data: Quartz Scheduler (v2.2.1) 'DefaultQuartzScheduler' with instanceId 'NON_CLUSTERED'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'org.quartz.simpl.SimpleThreadPool' - with 10 threads.
  Using job-store 'org.quartz.simpl.RAMJobStore' - which does not support persistence. and is not clustered.

 INFO [2020-01-22 17:05:30,773] ({main} StdSchedulerFactory.java[instantiate]:1339) - Quartz scheduler 'DefaultQuartzScheduler' initialized from default resource file in Quartz package: 'quartz.properties'
 INFO [2020-01-22 17:05:30,773] ({main} StdSchedulerFactory.java[instantiate]:1343) - Quartz scheduler version: 2.2.1
 INFO [2020-01-22 17:05:30,776] ({main} QuartzScheduler.java[start]:575) - Scheduler DefaultQuartzScheduler_$_NON_CLUSTERED started.
 INFO [2020-01-22 17:05:30,814] ({main} Notebook.java[<init>]:127) - Notebook indexing started...
 INFO [2020-01-22 17:05:30,854] ({main} LuceneSearch.java[addIndexDocs]:305) - Indexing 0 notebooks took 39ms
 INFO [2020-01-22 17:05:30,877] ({main} Notebook.java[<init>]:129) - Notebook indexing finished: 0 indexed in 0s
 INFO [2020-01-22 17:05:30,879] ({main} Helium.java[loadConf]:103) - Add helium local registry /zeppelin/helium
 INFO [2020-01-22 17:05:30,897] ({main} Helium.java[loadConf]:100) - Add helium online registry https://s3.amazonaws.com/helium-package/helium.json
 WARN [2020-01-22 17:05:30,929] ({main} Helium.java[loadConf]:111) - /zeppelin/conf/helium.json does not exists
 INFO [2020-01-22 17:05:39,852] ({main} ContextHandler.java[doStart]:855) - Started o.e.j.w.WebAppContext@5af97850{zeppelin-web,/,file:///zeppelin/webapps/webapp/,AVAILABLE}{/zeppelin/zeppelin-web-0.8.2.war}
 INFO [2020-01-22 17:05:39,923] ({main} AbstractConnector.java[doStart]:292) - Started ServerConnector@3c0bbc9f{HTTP/1.1,[http/1.1]}{0.0.0.0:8080}
 INFO [2020-01-22 17:05:39,924] ({main} Server.java[doStart]:407) - Started @36225ms
 INFO [2020-01-22 17:05:39,924] ({main} ZeppelinServer.java[main]:249) - Done, zeppelin server started
 WARN [2020-01-22 17:06:11,146] ({qtp2107447833-37} SecurityRestApi.java[ticket]:88) - {"status":"OK","message":"","body":{"principal":"anonymous","ticket":"anonymous","roles":"[]"}}
 INFO [2020-01-22 17:06:11,336] ({qtp2107447833-12} NotebookServer.java[onOpen]:151) - New connection from 192.168.99.1 : 14918
 INFO [2020-01-22 17:06:11,418] ({qtp2107447833-9} NotebookServer.java[sendNote]:828) - New operation from 192.168.99.1 : 14918 : anonymous : GET_NOTE : 2EZEECRFJ
 WARN [2020-01-22 17:06:11,530] ({qtp2107447833-9} GitNotebookRepo.java[revisionHistory]:158) - No Head found for 2EZEECRFJ, No HEAD exists and no explicit starting revision was specified
 INFO [2020-01-22 17:06:13,503] ({qtp2107447833-38} NotebookServer.java[onClose]:372) - Closed connection to 192.168.99.1 : 14918. (1001) null
 WARN [2020-01-22 17:06:13,654] ({qtp2107447833-36} SecurityRestApi.java[ticket]:88) - {"status":"OK","message":"","body":{"principal":"anonymous","ticket":"anonymous","roles":"[]"}}
 INFO [2020-01-22 17:06:13,674] ({qtp2107447833-38} NotebookServer.java[onOpen]:151) - New connection from 192.168.99.1 : 14920
 INFO [2020-01-22 17:08:19,643] ({qtp2107447833-15} NotebookServer.java[onClose]:372) - Closed connection to 192.168.99.1 : 14920. (1001) null
 WARN [2020-01-22 17:08:19,778] ({qtp2107447833-11} SecurityRestApi.java[ticket]:88) - {"status":"OK","message":"","body":{"principal":"anonymous","ticket":"anonymous","roles":"[]"}}
 INFO [2020-01-22 17:08:19,797] ({qtp2107447833-13} NotebookServer.java[onOpen]:151) - New connection from 192.168.99.1 : 14928
 INFO [2020-01-22 17:08:20,718] ({qtp2107447833-13} NotebookServer.java[onClose]:372) - Closed connection to 192.168.99.1 : 14928. (1001) null
 WARN [2020-01-22 17:08:20,824] ({qtp2107447833-15} SecurityRestApi.java[ticket]:88) - {"status":"OK","message":"","body":{"principal":"anonymous","ticket":"anonymous","roles":"[]"}}
 INFO [2020-01-22 17:08:20,853] ({qtp2107447833-16} NotebookServer.java[onOpen]:151) - New connection from 192.168.99.1 : 14929
 INFO [2020-01-22 17:08:44,808] ({Thread-13} ZeppelinServer.java[run]:253) - Shutting down Zeppelin Server ... 
 INFO [2020-01-22 17:08:44,829] ({qtp2107447833-13} NotebookServer.java[onClose]:372) - Closed connection to 192.168.99.1 : 14929. (1000) null
 INFO [2020-01-22 17:08:44,832] ({Thread-13} AbstractConnector.java[doStop]:341) - Stopped ServerConnector@3c0bbc9f{HTTP/1.1,[http/1.1]}{0.0.0.0:8080}
 INFO [2020-01-22 17:08:44,833] ({Thread-13} HouseKeeper.java[stopScavenging]:167) - node0 Stopped scavenging
 INFO [2020-01-22 17:08:46,661] ({Thread-13} ContextHandler.java[doStop]:1045) - Stopped o.e.j.w.WebAppContext@5af97850{zeppelin-web,/,null,UNAVAILABLE}{/zeppelin/zeppelin-web-0.8.2.war}
 INFO [2020-01-22 17:08:46,664] ({Thread-19} InterpreterSetting.java[close]:483) - Close InterpreterSetting: python
 INFO [2020-01-22 17:08:46,666] ({Thread-38} InterpreterSetting.java[close]:483) - Close InterpreterSetting: hbase
 INFO [2020-01-22 17:08:46,666] ({Thread-37} InterpreterSetting.java[close]:483) - Close InterpreterSetting: bigquery
 INFO [2020-01-22 17:08:46,666] ({Thread-36} InterpreterSetting.java[close]:483) - Close InterpreterSetting: alluxio
 INFO [2020-01-22 17:08:46,666] ({Thread-35} InterpreterSetting.java[close]:483) - Close InterpreterSetting: md
 INFO [2020-01-22 17:08:46,666] ({Thread-34} InterpreterSetting.java[close]:483) - Close InterpreterSetting: spark
 INFO [2020-01-22 17:08:46,666] ({Thread-33} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sh
 INFO [2020-01-22 17:08:46,665] ({Thread-32} InterpreterSetting.java[close]:483) - Close InterpreterSetting: cassandra
 INFO [2020-01-22 17:08:46,665] ({Thread-31} InterpreterSetting.java[close]:483) - Close InterpreterSetting: elasticsearch
 INFO [2020-01-22 17:08:46,665] ({Thread-30} InterpreterSetting.java[close]:483) - Close InterpreterSetting: groovy
 INFO [2020-01-22 17:08:46,665] ({Thread-29} InterpreterSetting.java[close]:483) - Close InterpreterSetting: file
 INFO [2020-01-22 17:08:46,665] ({Thread-28} InterpreterSetting.java[close]:483) - Close InterpreterSetting: neo4j
 INFO [2020-01-22 17:08:46,671] ({Thread-18} InterpreterSetting.java[close]:483) - Close InterpreterSetting: ignite
 INFO [2020-01-22 17:08:46,665] ({Thread-27} InterpreterSetting.java[close]:483) - Close InterpreterSetting: livy
 INFO [2020-01-22 17:08:46,665] ({Thread-26} InterpreterSetting.java[close]:483) - Close InterpreterSetting: angular
 INFO [2020-01-22 17:08:46,665] ({Thread-25} InterpreterSetting.java[close]:483) - Close InterpreterSetting: flink
 INFO [2020-01-22 17:08:46,675] ({Thread-43} InterpreterSetting.java[close]:483) - Close InterpreterSetting: lens
 INFO [2020-01-22 17:08:46,665] ({Thread-23} InterpreterSetting.java[close]:483) - Close InterpreterSetting: pig
 INFO [2020-01-22 17:08:46,664] ({Thread-22} InterpreterSetting.java[close]:483) - Close InterpreterSetting: lens
 INFO [2020-01-22 17:08:46,664] ({Thread-21} InterpreterSetting.java[close]:483) - Close InterpreterSetting: jdbc
 INFO [2020-01-22 17:08:46,664] ({Thread-20} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sap
 INFO [2020-01-22 17:08:46,675] ({Thread-44} InterpreterSetting.java[close]:483) - Close InterpreterSetting: pig
 INFO [2020-01-22 17:08:46,675] ({Thread-42} InterpreterSetting.java[close]:483) - Close InterpreterSetting: jdbc
 INFO [2020-01-22 17:08:46,675] ({Thread-39} InterpreterSetting.java[close]:483) - Close InterpreterSetting: kylin
 INFO [2020-01-22 17:08:46,677] ({Thread-48} InterpreterSetting.java[close]:483) - Close InterpreterSetting: neo4j
 INFO [2020-01-22 17:08:46,672] ({Thread-41} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sap
 INFO [2020-01-22 17:08:46,672] ({Thread-40} InterpreterSetting.java[close]:483) - Close InterpreterSetting: python
 INFO [2020-01-22 17:08:46,671] ({Thread-24} InterpreterSetting.java[close]:483) - Close InterpreterSetting: ignite
 INFO [2020-01-22 17:08:46,677] ({Thread-46} InterpreterSetting.java[close]:483) - Close InterpreterSetting: angular
 INFO [2020-01-22 17:08:46,678] ({Thread-51} InterpreterSetting.java[close]:483) - Close InterpreterSetting: elasticsearch
 INFO [2020-01-22 17:08:46,676] ({Thread-47} InterpreterSetting.java[close]:483) - Close InterpreterSetting: livy
 INFO [2020-01-22 17:08:46,676] ({Thread-45} InterpreterSetting.java[close]:483) - Close InterpreterSetting: flink
 INFO [2020-01-22 17:08:46,678] ({Thread-50} InterpreterSetting.java[close]:483) - Close InterpreterSetting: groovy
 INFO [2020-01-22 17:08:46,678] ({Thread-49} InterpreterSetting.java[close]:483) - Close InterpreterSetting: file
 INFO [2020-01-22 17:08:46,679] ({Thread-52} InterpreterSetting.java[close]:483) - Close InterpreterSetting: cassandra
 INFO [2020-01-22 17:08:46,679] ({Thread-53} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sh
 INFO [2020-01-22 17:08:46,680] ({Thread-54} InterpreterSetting.java[close]:483) - Close InterpreterSetting: spark
 INFO [2020-01-22 17:08:46,680] ({Thread-55} InterpreterSetting.java[close]:483) - Close InterpreterSetting: md
 INFO [2020-01-22 17:08:46,680] ({Thread-56} InterpreterSetting.java[close]:483) - Close InterpreterSetting: alluxio
 INFO [2020-01-22 17:08:46,684] ({Thread-57} InterpreterSetting.java[close]:483) - Close InterpreterSetting: bigquery
 INFO [2020-01-22 17:08:46,686] ({Thread-58} InterpreterSetting.java[close]:483) - Close InterpreterSetting: hbase
 INFO [2020-01-22 17:08:46,687] ({Thread-59} InterpreterSetting.java[close]:483) - Close InterpreterSetting: kylin
 INFO [2020-01-22 17:08:46,687] ({Thread-13} NotebookRepoSync.java[close]:428) - Closing all notebook storages
 INFO [2020-01-22 17:08:49,780] ({Thread-13} ZeppelinServer.java[run]:264) - Bye
 WARN [2020-01-22 17:08:56,495] ({main} ZeppelinConfiguration.java[create]:117) - Failed to load configuration, proceeding with a default
 INFO [2020-01-22 17:08:56,880] ({main} ZeppelinConfiguration.java[create]:129) - Server Host: 0.0.0.0
 INFO [2020-01-22 17:08:56,882] ({main} ZeppelinConfiguration.java[create]:131) - Server Port: 8080
 INFO [2020-01-22 17:08:56,882] ({main} ZeppelinConfiguration.java[create]:135) - Context Path: /
 INFO [2020-01-22 17:08:56,886] ({main} ZeppelinConfiguration.java[create]:136) - Zeppelin Version: 0.8.2
 INFO [2020-01-22 17:08:56,996] ({main} Log.java[initialized]:193) - Logging initialized @3138ms to org.eclipse.jetty.util.log.Slf4jLog
 WARN [2020-01-22 17:08:57,831] ({main} ServerConnector.java[setSoLingerTime]:458) - Ignoring deprecated socket close linger time
 INFO [2020-01-22 17:08:58,334] ({main} ZeppelinServer.java[setupWebAppContext]:413) - ZeppelinServer Webapp path: /zeppelin/webapps
 INFO [2020-01-22 17:08:59,055] ({main} ZeppelinServer.java[main]:239) - Starting zeppelin server
 INFO [2020-01-22 17:08:59,081] ({main} Server.java[doStart]:370) - jetty-9.4.14.v20181114; built: 2018-11-14T21:20:31.478Z; git: c4550056e785fb5665914545889f21dc136ad9e6; jvm 1.8.0_222-8u222-b10-1ubuntu1~16.04.1-b10
 INFO [2020-01-22 17:09:12,898] ({main} StandardDescriptorProcessor.java[visitServlet]:283) - NO JSP Support for /, did not find org.eclipse.jetty.jsp.JettyJspServlet
 INFO [2020-01-22 17:09:13,030] ({main} DefaultSessionIdManager.java[doStart]:365) - DefaultSessionIdManager workerName=node0
 INFO [2020-01-22 17:09:13,030] ({main} DefaultSessionIdManager.java[doStart]:370) - No SessionScavenger set, using defaults
 INFO [2020-01-22 17:09:13,045] ({main} HouseKeeper.java[startScavenging]:149) - node0 Scavenging every 600000ms
 INFO [2020-01-22 17:09:15,607] ({main} SchedulerFactory.java[<init>]:59) - Scheduler Thread Pool Size: 100
 WARN [2020-01-22 17:09:15,685] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir
 WARN [2020-01-22 17:09:15,702] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir
 INFO [2020-01-22 17:09:16,149] ({main} InterpreterSettingManager.java[<init>]:165) - Using RecoveryStorage: org.apache.zeppelin.interpreter.recovery.NullRecoveryStorage
 INFO [2020-01-22 17:09:16,162] ({main} InterpreterSettingManager.java[<init>]:169) - Using LifecycleManager: org.apache.zeppelin.interpreter.lifecycle.NullLifecycleManager
 INFO [2020-01-22 17:09:16,419] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: cassandra
 INFO [2020-01-22 17:09:16,441] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: md
 INFO [2020-01-22 17:09:16,478] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: sap
 INFO [2020-01-22 17:09:16,512] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: sh
 INFO [2020-01-22 17:09:16,550] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: angular
 WARN [2020-01-22 17:09:16,587] ({main} InterpreterSettingManager.java[init]:331) - No interpreter-setting.json found in /zeppelin/interpreter/lib
 INFO [2020-01-22 17:09:16,623] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: hbase
 INFO [2020-01-22 17:09:16,642] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: elasticsearch
 WARN [2020-01-22 17:09:16,971] ({main} InterpreterSettingManager.java[init]:331) - No interpreter-setting.json found in /zeppelin/interpreter/scio
 INFO [2020-01-22 17:09:16,994] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: alluxio
 INFO [2020-01-22 17:09:17,028] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: jdbc
 INFO [2020-01-22 17:09:17,071] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: neo4j
 INFO [2020-01-22 17:09:17,122] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: python
 INFO [2020-01-22 17:09:17,154] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: pig
 INFO [2020-01-22 17:09:17,204] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: spark
 INFO [2020-01-22 17:09:17,218] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: lens
 INFO [2020-01-22 17:09:17,254] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: bigquery
 INFO [2020-01-22 17:09:17,294] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: file
 INFO [2020-01-22 17:09:17,330] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: ignite
 INFO [2020-01-22 17:09:17,361] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: kylin
 INFO [2020-01-22 17:09:17,406] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: livy
 INFO [2020-01-22 17:09:17,450] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: groovy
 INFO [2020-01-22 17:09:17,482] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: flink
 WARN [2020-01-22 17:09:17,496] ({main} InterpreterSettingManager.java[init]:331) - No interpreter-setting.json found in /zeppelin/interpreter/${interpreter.name}
 INFO [2020-01-22 17:09:17,500] ({main} LocalConfigStorage.java[loadInterpreterSettings]:63) - Load Interpreter Setting from file: /zeppelin/conf/interpreter.json
 INFO [2020-01-22 17:09:17,954] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting ignite from interpreter.json
 INFO [2020-01-22 17:09:17,963] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting python from interpreter.json
 INFO [2020-01-22 17:09:17,973] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting sap from interpreter.json
 INFO [2020-01-22 17:09:17,983] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting jdbc from interpreter.json
 INFO [2020-01-22 17:09:17,991] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting lens from interpreter.json
 INFO [2020-01-22 17:09:18,001] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting pig from interpreter.json
 INFO [2020-01-22 17:09:18,006] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting flink from interpreter.json
 INFO [2020-01-22 17:09:18,007] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting angular from interpreter.json
 INFO [2020-01-22 17:09:18,016] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting livy from interpreter.json
 INFO [2020-01-22 17:09:18,021] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting neo4j from interpreter.json
 INFO [2020-01-22 17:09:18,031] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting file from interpreter.json
 INFO [2020-01-22 17:09:18,034] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting groovy from interpreter.json
 INFO [2020-01-22 17:09:18,040] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting elasticsearch from interpreter.json
 INFO [2020-01-22 17:09:18,042] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting cassandra from interpreter.json
 INFO [2020-01-22 17:09:18,051] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting sh from interpreter.json
 INFO [2020-01-22 17:09:18,054] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting spark from interpreter.json
 INFO [2020-01-22 17:09:18,069] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting md from interpreter.json
 INFO [2020-01-22 17:09:18,081] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting alluxio from interpreter.json
 INFO [2020-01-22 17:09:18,082] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting bigquery from interpreter.json
 INFO [2020-01-22 17:09:18,083] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting hbase from interpreter.json
 INFO [2020-01-22 17:09:18,086] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting kylin from interpreter.json
 INFO [2020-01-22 17:09:18,274] ({main} LocalConfigStorage.java[save]:53) - Save Interpreter Setting to /zeppelin/conf/interpreter.json
 INFO [2020-01-22 17:09:18,902] ({main} VfsLog.java[info]:138) - Using "/tmp/vfs_cache" as temporary files store.
 INFO [2020-01-22 17:09:19,026] ({main} GitNotebookRepo.java[<init>]:64) - Opening a git repo at '/zeppelin/notebook'
 INFO [2020-01-22 17:09:19,326] ({main} NotebookRepoSync.java[<init>]:77) - Instantiate NotebookRepo: org.apache.zeppelin.notebook.repo.GitNotebookRepo
 WARN [2020-01-22 17:09:19,566] ({main} NotebookAuthorization.java[getInstance]:86) - Notebook authorization module was called without initialization, initializing with default configuration
 WARN [2020-01-22 17:09:19,568] ({main} LocalConfigStorage.java[loadNotebookAuthorization]:77) - NotebookAuthorization file /zeppelin/conf/notebook-authorization.json is not existed
 INFO [2020-01-22 17:09:19,570] ({main} Credentials.java[loadFromFile]:121) - /zeppelin/conf/credentials.json
 INFO [2020-01-22 17:09:19,696] ({main} StdSchedulerFactory.java[instantiate]:1184) - Using default implementation for ThreadExecutor
 INFO [2020-01-22 17:09:19,714] ({main} SimpleThreadPool.java[initialize]:268) - Job execution threads will use class loader of thread: main
 INFO [2020-01-22 17:09:19,739] ({main} SchedulerSignalerImpl.java[<init>]:61) - Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
 INFO [2020-01-22 17:09:19,748] ({main} QuartzScheduler.java[<init>]:240) - Quartz Scheduler v.2.2.1 created.
 INFO [2020-01-22 17:09:19,749] ({main} RAMJobStore.java[initialize]:155) - RAMJobStore initialized.
 INFO [2020-01-22 17:09:19,753] ({main} QuartzScheduler.java[initialize]:305) - Scheduler meta-data: Quartz Scheduler (v2.2.1) 'DefaultQuartzScheduler' with instanceId 'NON_CLUSTERED'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'org.quartz.simpl.SimpleThreadPool' - with 10 threads.
  Using job-store 'org.quartz.simpl.RAMJobStore' - which does not support persistence. and is not clustered.

 INFO [2020-01-22 17:09:19,754] ({main} StdSchedulerFactory.java[instantiate]:1339) - Quartz scheduler 'DefaultQuartzScheduler' initialized from default resource file in Quartz package: 'quartz.properties'
 INFO [2020-01-22 17:09:19,757] ({main} StdSchedulerFactory.java[instantiate]:1343) - Quartz scheduler version: 2.2.1
 INFO [2020-01-22 17:09:19,758] ({main} QuartzScheduler.java[start]:575) - Scheduler DefaultQuartzScheduler_$_NON_CLUSTERED started.
 INFO [2020-01-22 17:09:20,280] ({main} FolderView.java[createFolder]:107) - Create folder /
 INFO [2020-01-22 17:09:20,284] ({main} Folder.java[setParent]:169) - Set parent of / to /
 INFO [2020-01-22 17:09:20,284] ({main} Folder.java[addNote]:185) - Add note 2EXBGWKU1 to folder /
 WARN [2020-01-22 17:09:20,290] ({main} Notebook.java[refreshCron]:981) - execution of the cron job is skipped cron is not enabled from Zeppelin server
 INFO [2020-01-22 17:09:20,375] ({main} Folder.java[addNote]:185) - Add note 2EXQEGYMG to folder /
 WARN [2020-01-22 17:09:20,376] ({main} Notebook.java[refreshCron]:981) - execution of the cron job is skipped cron is not enabled from Zeppelin server
 INFO [2020-01-22 17:09:20,379] ({main} Notebook.java[<init>]:127) - Notebook indexing started...
 INFO [2020-01-22 17:09:21,367] ({main} LuceneSearch.java[addIndexDocs]:305) - Indexing 2 notebooks took 986ms
 INFO [2020-01-22 17:09:21,373] ({main} Notebook.java[<init>]:129) - Notebook indexing finished: 2 indexed in 0s
 INFO [2020-01-22 17:09:21,375] ({main} Helium.java[loadConf]:103) - Add helium local registry /zeppelin/helium
 INFO [2020-01-22 17:09:21,381] ({main} Helium.java[loadConf]:100) - Add helium online registry https://s3.amazonaws.com/helium-package/helium.json
 WARN [2020-01-22 17:09:21,394] ({main} Helium.java[loadConf]:111) - /zeppelin/conf/helium.json does not exists
 INFO [2020-01-22 17:09:29,046] ({main} ContextHandler.java[doStart]:855) - Started o.e.j.w.WebAppContext@5af97850{zeppelin-web,/,file:///zeppelin/webapps/webapp/,AVAILABLE}{/zeppelin/zeppelin-web-0.8.2.war}
 INFO [2020-01-22 17:09:29,119] ({main} AbstractConnector.java[doStart]:292) - Started ServerConnector@79a41e8{HTTP/1.1,[http/1.1]}{0.0.0.0:8080}
 INFO [2020-01-22 17:09:29,122] ({main} Server.java[doStart]:407) - Started @35279ms
 INFO [2020-01-22 17:09:29,123] ({main} ZeppelinServer.java[main]:249) - Done, zeppelin server started
 INFO [2020-01-22 17:09:29,938] ({qtp2107447833-14} NotebookServer.java[onOpen]:151) - New connection from 192.168.99.1 : 14957
 INFO [2020-01-22 17:09:29,984] ({qtp2107447833-9} NotebookServer.java[onClose]:372) - Closed connection to 192.168.99.1 : 14957. (1001) null
 WARN [2020-01-22 17:09:29,987] ({qtp2107447833-9} NotebookServer.java[removeUserConnection]:383) - Closing connection that is absent in user connections
 WARN [2020-01-22 17:09:34,637] ({qtp2107447833-15} SecurityRestApi.java[ticket]:88) - {"status":"OK","message":"","body":{"principal":"anonymous","ticket":"anonymous","roles":"[]"}}
 INFO [2020-01-22 17:09:34,717] ({qtp2107447833-11} NotebookServer.java[onOpen]:151) - New connection from 192.168.99.1 : 14962
 INFO [2020-01-22 17:09:39,377] ({qtp2107447833-9} NotebookServer.java[sendNote]:828) - New operation from 192.168.99.1 : 14962 : anonymous : GET_NOTE : 2EXBGWKU1
 WARN [2020-01-22 17:09:40,062] ({qtp2107447833-9} GitNotebookRepo.java[revisionHistory]:158) - No Head found for 2EXBGWKU1, No HEAD exists and no explicit starting revision was specified
 WARN [2020-01-22 17:09:40,072] ({qtp2107447833-9} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:09:40,081] ({qtp2107447833-9} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:09:40,082] ({qtp2107447833-9} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:09:40,082] ({qtp2107447833-9} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:09:40,084] ({qtp2107447833-9} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:09:40,084] ({qtp2107447833-9} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:09:40,103] ({qtp2107447833-9} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: spark.dep
 WARN [2020-01-22 17:09:40,113] ({qtp2107447833-9} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: spark.pyspark
 WARN [2020-01-22 17:09:40,115] ({qtp2107447833-9} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: spark.pyspark
 WARN [2020-01-22 17:09:40,117] ({qtp2107447833-9} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: spark.pyspark
 WARN [2020-01-22 17:09:40,117] ({qtp2107447833-9} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: spark.pyspark
 INFO [2020-01-22 17:09:42,555] ({qtp2107447833-16} LocalConfigStorage.java[save]:53) - Save Interpreter Setting to /zeppelin/conf/interpreter.json
 WARN [2020-01-22 17:09:42,569] ({qtp2107447833-16} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:09:42,571] ({qtp2107447833-16} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:09:42,573] ({qtp2107447833-16} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:09:42,573] ({qtp2107447833-16} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:09:42,575] ({qtp2107447833-16} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:09:42,575] ({qtp2107447833-16} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:09:48,386] ({qtp2107447833-11} SecurityRestApi.java[ticket]:88) - {"status":"OK","message":"","body":{"principal":"anonymous","ticket":"anonymous","roles":"[]"}}
 INFO [2020-01-22 17:09:48,412] ({qtp2107447833-16} NotebookServer.java[onOpen]:151) - New connection from 192.168.99.1 : 14970
 WARN [2020-01-22 17:09:48,527] ({qtp2107447833-11} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:09:48,529] ({qtp2107447833-11} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:09:48,530] ({qtp2107447833-11} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:09:48,531] ({qtp2107447833-11} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:09:48,532] ({qtp2107447833-11} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:09:48,533] ({qtp2107447833-11} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 INFO [2020-01-22 17:10:59,547] ({qtp2107447833-12} InterpreterRestApi.java[updateSetting]:137) - Update interpreterSetting spark
 INFO [2020-01-22 17:10:59,549] ({qtp2107447833-12} InterpreterSetting.java[close]:483) - Close InterpreterSetting: spark
 INFO [2020-01-22 17:10:59,549] ({qtp2107447833-12} LocalConfigStorage.java[save]:53) - Save Interpreter Setting to /zeppelin/conf/interpreter.json
ERROR [2020-01-22 17:10:59,876] ({Thread-37} InterpreterSetting.java[run]:840) - Error while downloading repos for interpreter group : spark, go to interpreter setting page click on edit and save it again to make this interpreter work properly. : Cannot fetch dependencies for org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0
org.sonatype.aether.RepositoryException: Cannot fetch dependencies for org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0
	at org.apache.zeppelin.dep.DependencyResolver.getArtifactsWithDep(DependencyResolver.java:179)
	at org.apache.zeppelin.dep.DependencyResolver.loadFromMvn(DependencyResolver.java:128)
	at org.apache.zeppelin.dep.DependencyResolver.load(DependencyResolver.java:76)
	at org.apache.zeppelin.dep.DependencyResolver.load(DependencyResolver.java:93)
	at org.apache.zeppelin.dep.DependencyResolver.load(DependencyResolver.java:85)
	at org.apache.zeppelin.interpreter.InterpreterSetting$1.run(InterpreterSetting.java:832)
Caused by: java.lang.NullPointerException
	at org.sonatype.aether.impl.internal.DefaultRepositorySystem.resolveDependencies(DefaultRepositorySystem.java:352)
	at org.apache.zeppelin.dep.DependencyResolver.getArtifactsWithDep(DependencyResolver.java:176)
	... 5 more
 INFO [2020-01-22 17:11:07,165] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXBGWKU1
 INFO [2020-01-22 17:11:07,181] ({qtp2107447833-14} InterpreterSetting.java[getOrCreateInterpreterGroup]:419) - Create InterpreterGroup with groupId: spark:shared_process for user: anonymous and note: 2EXBGWKU1
 INFO [2020-01-22 17:11:07,183] ({qtp2107447833-14} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.SparkInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 17:11:07,184] ({qtp2107447833-14} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.SparkSqlInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 17:11:07,184] ({qtp2107447833-14} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.DepInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 17:11:07,185] ({qtp2107447833-14} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.PySparkInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 17:11:07,185] ({qtp2107447833-14} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.IPySparkInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 17:11:07,186] ({qtp2107447833-14} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.SparkRInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 17:11:07,186] ({qtp2107447833-14} ManagedInterpreterGroup.java[getOrCreateSession]:158) - Create Session: shared_session in InterpreterGroup: spark:shared_process for user: anonymous
 INFO [2020-01-22 17:11:07,198] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200120-222802_2095718758 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:11:07,204] ({pool-2-thread-2} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200120-222802_2095718758, interpreter: spark.dep, note_id: 2EXBGWKU1, user: anonymous]
 INFO [2020-01-22 17:11:07,205] ({pool-2-thread-2} ManagedInterpreterGroup.java[getOrCreateInterpreterProcess]:61) - Create InterpreterProcess for InterpreterGroup: spark:shared_process
 INFO [2020-01-22 17:11:07,206] ({pool-2-thread-2} ShellScriptLauncher.java[launch]:48) - Launching Interpreter: spark
 INFO [2020-01-22 17:11:07,216] ({pool-2-thread-2} SparkInterpreterLauncher.java[buildEnvFromProperties]:108) - Run Spark under non-secure mode as no keytab and principal is specified
 INFO [2020-01-22 17:11:07,221] ({pool-2-thread-2} RemoteInterpreterManagedProcess.java[start]:115) - Thrift server for callback will start. Port: 40595
 INFO [2020-01-22 17:11:07,238] ({pool-2-thread-2} RemoteInterpreterManagedProcess.java[start]:190) - Run interpreter process [/zeppelin/bin/interpreter.sh, -d, /zeppelin/interpreter/spark, -c, 172.18.0.2, -p, 40595, -r, :, -l, /usr/local/local-repo/spark, -g, spark]
 INFO [2020-01-22 17:11:10,008] ({pool-8-thread-1} RemoteInterpreterManagedProcess.java[callback]:123) - RemoteInterpreterServer Registered: CallbackInfo(host:172.18.0.2, port:46823)
 INFO [2020-01-22 17:11:10,068] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkInterpreter
 INFO [2020-01-22 17:11:10,153] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkSqlInterpreter
 INFO [2020-01-22 17:11:10,155] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.DepInterpreter
 INFO [2020-01-22 17:11:10,162] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.PySparkInterpreter
 INFO [2020-01-22 17:11:10,165] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.IPySparkInterpreter
 INFO [2020-01-22 17:11:10,169] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkRInterpreter
 INFO [2020-01-22 17:11:10,172] ({pool-2-thread-2} RemoteInterpreter.java[call]:142) - Open RemoteInterpreter org.apache.zeppelin.spark.DepInterpreter
 INFO [2020-01-22 17:11:10,172] ({pool-2-thread-2} RemoteInterpreter.java[pushAngularObjectRegistryToRemote]:436) - Push local angular object registry from ZeppelinServer to remote interpreter group spark:shared_process
 INFO [2020-01-22 17:11:33,378] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2314) - Job 20200120-222802_2095718758 is finished successfully, status: FINISHED
 INFO [2020-01-22 17:11:33,431] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2EXBGWKU1
 INFO [2020-01-22 17:11:33,447] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200120-222802_2095718758 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:13:37,095] ({qtp2107447833-57} VFSNotebookRepo.java[save]:196) - Saving note:2EXBGWKU1
 INFO [2020-01-22 17:13:37,110] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195314_1536993808 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:13:37,111] ({pool-2-thread-2} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200118-195314_1536993808, interpreter: spark.pyspark, note_id: 2EXBGWKU1, user: anonymous]
 INFO [2020-01-22 17:13:37,111] ({pool-2-thread-2} RemoteInterpreter.java[call]:142) - Open RemoteInterpreter org.apache.zeppelin.spark.PySparkInterpreter
 INFO [2020-01-22 17:13:53,678] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2314) - Job 20200118-195314_1536993808 is finished successfully, status: FINISHED
 INFO [2020-01-22 17:13:53,725] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2EXBGWKU1
 INFO [2020-01-22 17:13:53,739] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195314_1536993808 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:14:44,175] ({qtp2107447833-57} VFSNotebookRepo.java[save]:196) - Saving note:2EXBGWKU1
 INFO [2020-01-22 17:14:44,190] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20200120-223154_1310614256 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:14:44,191] ({pool-2-thread-3} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200120-223154_1310614256, interpreter: spark.pyspark, note_id: 2EXBGWKU1, user: anonymous]
 INFO [2020-01-22 17:14:46,368] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2314) - Job 20200120-223154_1310614256 is finished successfully, status: FINISHED
 INFO [2020-01-22 17:14:46,429] ({pool-2-thread-3} VFSNotebookRepo.java[save]:196) - Saving note:2EXBGWKU1
 INFO [2020-01-22 17:14:46,461] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20200120-223154_1310614256 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 WARN [2020-01-22 17:14:51,869] ({qtp2107447833-57} SecurityRestApi.java[ticket]:88) - {"status":"OK","message":"","body":{"principal":"anonymous","ticket":"anonymous","roles":"[]"}}
 INFO [2020-01-22 17:14:51,900] ({qtp2107447833-15} NotebookServer.java[onOpen]:151) - New connection from 192.168.99.1 : 15052
 INFO [2020-01-22 17:14:51,982] ({qtp2107447833-57} NotebookServer.java[sendNote]:828) - New operation from 192.168.99.1 : 15052 : anonymous : GET_NOTE : 2EXQEGYMG
 WARN [2020-01-22 17:14:51,990] ({qtp2107447833-57} GitNotebookRepo.java[revisionHistory]:158) - No Head found for 2EXQEGYMG, No HEAD exists and no explicit starting revision was specified
 WARN [2020-01-22 17:14:52,030] ({qtp2107447833-13} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:14:52,031] ({qtp2107447833-13} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:14:52,032] ({qtp2107447833-13} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:14:52,032] ({qtp2107447833-13} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:14:52,032] ({qtp2107447833-13} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:14:52,033] ({qtp2107447833-13} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:14:52,423] ({qtp2107447833-14} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: spark
 WARN [2020-01-22 17:14:52,625] ({qtp2107447833-11} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: dep
 WARN [2020-01-22 17:14:52,627] ({qtp2107447833-14} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: spark.pyspark
 WARN [2020-01-22 17:14:52,629] ({qtp2107447833-11} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: spark.pyspark
 WARN [2020-01-22 17:14:52,631] ({qtp2107447833-11} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: spark.pyspark
 WARN [2020-01-22 17:14:52,632] ({qtp2107447833-11} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: spark.pyspark
 INFO [2020-01-22 17:14:54,225] ({qtp2107447833-14} LocalConfigStorage.java[save]:53) - Save Interpreter Setting to /zeppelin/conf/interpreter.json
 WARN [2020-01-22 17:14:54,227] ({qtp2107447833-14} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:14:54,227] ({qtp2107447833-14} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:14:54,228] ({qtp2107447833-14} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:14:54,228] ({qtp2107447833-14} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:14:54,228] ({qtp2107447833-14} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:14:54,229] ({qtp2107447833-14} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 INFO [2020-01-22 17:15:01,327] ({qtp2107447833-11} InterpreterRestApi.java[restartSetting]:180) - Restart interpreterSetting spark, msg=
 INFO [2020-01-22 17:15:01,328] ({qtp2107447833-11} InterpreterSetting.java[close]:483) - Close InterpreterSetting: spark
 INFO [2020-01-22 17:15:01,328] ({qtp2107447833-11} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: spark:shared_process
 INFO [2020-01-22 17:15:01,329] ({qtp2107447833-11} ManagedInterpreterGroup.java[close]:100) - Close Session: shared_session for interpreter setting: spark
 WARN [2020-01-22 17:15:01,329] ({qtp2107447833-11} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.SparkInterpreter
 WARN [2020-01-22 17:15:01,330] ({qtp2107447833-11} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.SparkSqlInterpreter
 WARN [2020-01-22 17:15:01,680] ({qtp2107447833-11} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.IPySparkInterpreter
 WARN [2020-01-22 17:15:01,680] ({qtp2107447833-11} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.SparkRInterpreter
 INFO [2020-01-22 17:15:01,681] ({qtp2107447833-11} ManagedInterpreterGroup.java[close]:105) - Remove this InterpreterGroup: spark:shared_process as all the sessions are closed
 INFO [2020-01-22 17:15:01,682] ({qtp2107447833-11} ManagedInterpreterGroup.java[close]:108) - Kill RemoteInterpreterProcess
 INFO [2020-01-22 17:15:01,682] ({qtp2107447833-11} RemoteInterpreterManagedProcess.java[stop]:220) - Kill interpreter process
ERROR [2020-01-22 17:15:02,384] ({Thread-39} RemoteInterpreterEventPoller.java[run]:257) - Can not get RemoteInterpreterEvent because it is shutdown.
ERROR [2020-01-22 17:15:02,384] ({pool-7-thread-1} AppendOutputRunner.java[run]:68) - Wait for OutputBuffer queue interrupted: null
 WARN [2020-01-22 17:15:04,208] ({qtp2107447833-11} RemoteInterpreterManagedProcess.java[stop]:230) - ignore the exception when shutting down
 INFO [2020-01-22 17:15:04,210] ({qtp2107447833-11} RemoteInterpreterManagedProcess.java[stop]:238) - Remote process terminated
 INFO [2020-01-22 17:15:04,218] ({Exec Default Executor} RemoteInterpreterManagedProcess.java[onProcessFailed]:250) - Interpreter process failed {}
org.apache.commons.exec.ExecuteException: Process exited with an error: 143 (Exit value: 143)
	at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:404)
	at org.apache.commons.exec.DefaultExecutor.access$200(DefaultExecutor.java:48)
	at org.apache.commons.exec.DefaultExecutor$1.run(DefaultExecutor.java:200)
	at java.lang.Thread.run(Thread.java:748)
 INFO [2020-01-22 17:15:10,357] ({qtp2107447833-15} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:15:10,387] ({qtp2107447833-15} InterpreterSetting.java[getOrCreateInterpreterGroup]:419) - Create InterpreterGroup with groupId: spark:shared_process for user: anonymous and note: 2EXQEGYMG
 INFO [2020-01-22 17:15:10,388] ({qtp2107447833-15} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.SparkInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 17:15:10,388] ({qtp2107447833-15} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.SparkSqlInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 17:15:10,389] ({qtp2107447833-15} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.DepInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 17:15:10,389] ({qtp2107447833-15} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.PySparkInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 17:15:10,390] ({qtp2107447833-15} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.IPySparkInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 17:15:10,390] ({qtp2107447833-15} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.SparkRInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 17:15:10,391] ({qtp2107447833-15} ManagedInterpreterGroup.java[getOrCreateSession]:158) - Create Session: shared_session in InterpreterGroup: spark:shared_process for user: anonymous
 INFO [2020-01-22 17:15:10,403] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200121-185651_2144996466 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:15:10,404] ({pool-2-thread-4} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-185651_2144996466, interpreter: dep, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 17:15:10,406] ({pool-2-thread-4} ManagedInterpreterGroup.java[getOrCreateInterpreterProcess]:61) - Create InterpreterProcess for InterpreterGroup: spark:shared_process
 INFO [2020-01-22 17:15:10,406] ({pool-2-thread-4} ShellScriptLauncher.java[launch]:48) - Launching Interpreter: spark
 INFO [2020-01-22 17:15:10,407] ({pool-2-thread-4} SparkInterpreterLauncher.java[buildEnvFromProperties]:108) - Run Spark under non-secure mode as no keytab and principal is specified
 INFO [2020-01-22 17:15:10,408] ({pool-2-thread-4} RemoteInterpreterManagedProcess.java[start]:115) - Thrift server for callback will start. Port: 41163
 INFO [2020-01-22 17:15:10,914] ({pool-2-thread-4} RemoteInterpreterManagedProcess.java[start]:190) - Run interpreter process [/zeppelin/bin/interpreter.sh, -d, /zeppelin/interpreter/spark, -c, 172.18.0.2, -p, 41163, -r, :, -l, /usr/local/local-repo/spark, -g, spark]
 INFO [2020-01-22 17:15:13,889] ({pool-10-thread-1} RemoteInterpreterManagedProcess.java[callback]:123) - RemoteInterpreterServer Registered: CallbackInfo(host:172.18.0.2, port:38261)
 INFO [2020-01-22 17:15:13,899] ({pool-2-thread-4} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkInterpreter
 INFO [2020-01-22 17:15:13,994] ({pool-2-thread-4} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkSqlInterpreter
 INFO [2020-01-22 17:15:13,996] ({pool-2-thread-4} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.DepInterpreter
 INFO [2020-01-22 17:15:13,999] ({pool-2-thread-4} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.PySparkInterpreter
 INFO [2020-01-22 17:15:14,004] ({pool-2-thread-4} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.IPySparkInterpreter
 INFO [2020-01-22 17:15:14,008] ({pool-2-thread-4} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkRInterpreter
 INFO [2020-01-22 17:15:14,011] ({pool-2-thread-4} RemoteInterpreter.java[call]:142) - Open RemoteInterpreter org.apache.zeppelin.spark.DepInterpreter
 INFO [2020-01-22 17:15:14,011] ({pool-2-thread-4} RemoteInterpreter.java[pushAngularObjectRegistryToRemote]:436) - Push local angular object registry from ZeppelinServer to remote interpreter group spark:shared_process
 INFO [2020-01-22 17:18:33,089] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-185651_2144996466 is finished successfully, status: FINISHED
 INFO [2020-01-22 17:18:33,108] ({pool-2-thread-4} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:18:33,143] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200121-185651_2144996466 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:19:59,095] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:19:59,119] ({pool-2-thread-7} SchedulerFactory.java[jobStarted]:114) - Job 20200121-183200_1639671760 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:19:59,119] ({pool-2-thread-7} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-183200_1639671760, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 17:19:59,121] ({pool-2-thread-7} RemoteInterpreter.java[call]:142) - Open RemoteInterpreter org.apache.zeppelin.spark.PySparkInterpreter
 INFO [2020-01-22 17:20:13,595] ({pool-2-thread-7} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-183200_1639671760 is finished successfully, status: FINISHED
 INFO [2020-01-22 17:20:13,626] ({pool-2-thread-7} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:20:13,642] ({pool-2-thread-7} SchedulerFactory.java[jobFinished]:120) - Job 20200121-183200_1639671760 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:22:07,305] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:22:07,322] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20200121-183236_977276561 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:22:07,322] ({pool-2-thread-3} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-183236_977276561, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 17:22:10,749] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:22:10,797] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200121-185449_2110594246 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:22:10,819] ({pool-2-thread-2} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-185449_2110594246, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 17:22:20,438] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-183236_977276561 is finished successfully, status: FINISHED
 INFO [2020-01-22 17:22:20,486] ({pool-2-thread-3} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:22:20,518] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20200121-183236_977276561 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:22:20,804] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-185449_2110594246 is finished successfully, status: FINISHED
 INFO [2020-01-22 17:22:20,826] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:22:20,849] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200121-185449_2110594246 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:22:48,641] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:22:48,657] ({pool-2-thread-9} SchedulerFactory.java[jobStarted]:114) - Job 20200121-185449_2110594246 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:22:48,658] ({pool-2-thread-9} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-185449_2110594246, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 17:22:50,564] ({pool-2-thread-9} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-185449_2110594246 is finished successfully, status: FINISHED
 INFO [2020-01-22 17:22:50,583] ({pool-2-thread-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:22:50,595] ({pool-2-thread-9} SchedulerFactory.java[jobFinished]:120) - Job 20200121-185449_2110594246 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:23:05,519] ({qtp2107447833-13} NotebookServer.java[broadcastNewParagraph]:688) - Broadcasting paragraph on run call instead of note.
 INFO [2020-01-22 17:23:05,540] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:23:05,569] ({pool-2-thread-8} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:23:05,570] ({pool-2-thread-8} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 17:23:05,992] ({pool-2-thread-8} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-051101_1395511314 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mAnalysisException[0mTraceback (most recent call last)
[0;32m<ipython-input-12-3b493563ff93>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdataDf[0m[0;34m.[0m[0mshow[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py[0m in [0;36mshow[0;34m(self, n, truncate, vertical)[0m
[1;32m    376[0m         """
[1;32m    377[0m         [0;32mif[0m [0misinstance[0m[0;34m([0m[0mtruncate[0m[0;34m,[0m [0mbool[0m[0;34m)[0m [0;32mand[0m [0mtruncate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 378[0;31m             [0;32mprint[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mshowString[0m[0;34m([0m[0mn[0m[0;34m,[0m [0;36m20[0m[0;34m,[0m [0mvertical[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    379[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    380[0m             [0;32mprint[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mshowString[0m[0;34m([0m[0mn[0m[0;34m,[0m [0mint[0m[0;34m([0m[0mtruncate[0m[0;34m)[0m[0;34m,[0m [0mvertical[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     67[0m                                              e.java_exception.getStackTrace()))
[1;32m     68[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.AnalysisException: '[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 69[0;31m                 [0;32mraise[0m [0mAnalysisException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     70[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.catalyst.analysis'[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     71[0m                 [0;32mraise[0m [0mAnalysisException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m

[0;31mAnalysisException[0m: u'Queries with streaming sources must be executed with writeStream.start();;\nFileSource[s3a://j2training/data]'
 INFO [2020-01-22 17:23:06,014] ({pool-2-thread-8} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:23:06,048] ({pool-2-thread-8} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 WARN [2020-01-22 17:23:06,075] ({pool-9-thread-1} AppendOutputRunner.java[run]:88) - Processing time for buffered append-output is high: 13 milliseconds.
 INFO [2020-01-22 17:23:26,712] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:23:26,728] ({pool-2-thread-1} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:23:26,728] ({pool-2-thread-1} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 17:23:27,157] ({pool-2-thread-1} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-051101_1395511314 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mAnalysisException[0mTraceback (most recent call last)
[0;32m<ipython-input-14-293220493fe2>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdataDf[0m[0;34m.[0m[0mdescribe[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py[0m in [0;36mdescribe[0;34m(self, *cols)[0m
[1;32m   1170[0m         [0;32mif[0m [0mlen[0m[0;34m([0m[0mcols[0m[0;34m)[0m [0;34m==[0m [0;36m1[0m [0;32mand[0m [0misinstance[0m[0;34m([0m[0mcols[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m,[0m [0mlist[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m   1171[0m             [0mcols[0m [0;34m=[0m [0mcols[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m[0m[0m
[0;32m-> 1172[0;31m         [0mjdf[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mdescribe[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jseq[0m[0;34m([0m[0mcols[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1173[0m         [0;32mreturn[0m [0mDataFrame[0m[0;34m([0m[0mjdf[0m[0;34m,[0m [0mself[0m[0;34m.[0m[0msql_ctx[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1174[0m [0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     67[0m                                              e.java_exception.getStackTrace()))
[1;32m     68[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.AnalysisException: '[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 69[0;31m                 [0;32mraise[0m [0mAnalysisException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     70[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.catalyst.analysis'[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     71[0m                 [0;32mraise[0m [0mAnalysisException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m

[0;31mAnalysisException[0m: u'Queries with streaming sources must be executed with writeStream.start();;\nFileSource[s3a://j2training/data]'
 INFO [2020-01-22 17:23:27,218] ({pool-2-thread-1} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:23:27,231] ({pool-2-thread-1} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:23:36,286] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:23:36,304] ({pool-2-thread-10} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:23:36,305] ({pool-2-thread-10} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 17:23:36,472] ({pool-2-thread-10} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 17:23:36,503] ({pool-2-thread-10} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:23:36,521] ({pool-2-thread-10} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:32:41,475] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:32:41,486] ({qtp2107447833-11} NotebookServer.java[broadcastNewParagraph]:688) - Broadcasting paragraph on run call instead of note.
 INFO [2020-01-22 17:32:45,872] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:32:45,883] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:32:45,884] ({pool-2-thread-5} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 17:32:51,563] ({pool-2-thread-5} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-173241_479707786 is finished successfully, status: FINISHED
 INFO [2020-01-22 17:32:51,630] ({pool-2-thread-5} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:32:51,654] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:33:04,391] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:33:04,413] ({pool-2-thread-11} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:33:04,413] ({pool-2-thread-11} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 17:33:04,541] ({pool-2-thread-11} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-173241_479707786 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mAttributeError[0mTraceback (most recent call last)
[0;32m<ipython-input-20-ceb4a49b3f19>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdf[0m[0;34m.[0m[0mdisplay[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py[0m in [0;36m__getattr__[0;34m(self, name)[0m
[1;32m   1298[0m         [0;32mif[0m [0mname[0m [0;32mnot[0m [0;32min[0m [0mself[0m[0;34m.[0m[0mcolumns[0m[0;34m:[0m[0;34m[0m[0m
[1;32m   1299[0m             raise AttributeError(
[0;32m-> 1300[0;31m                 "'%s' object has no attribute '%s'" % (self.__class__.__name__, name))
[0m[1;32m   1301[0m         [0mjc[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mapply[0m[0;34m([0m[0mname[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1302[0m         [0;32mreturn[0m [0mColumn[0m[0;34m([0m[0mjc[0m[0;34m)[0m[0;34m[0m[0m

[0;31mAttributeError[0m: 'DataFrame' object has no attribute 'display'
 INFO [2020-01-22 17:33:04,559] ({pool-2-thread-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:33:04,573] ({pool-2-thread-11} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:33:09,205] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:33:09,222] ({pool-2-thread-12} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:33:09,222] ({pool-2-thread-12} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 17:33:09,305] ({pool-2-thread-12} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-173241_479707786 is finished successfully, status: FINISHED
 INFO [2020-01-22 17:33:09,330] ({pool-2-thread-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:33:09,342] ({pool-2-thread-12} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:33:15,807] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:33:15,833] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:33:15,834] ({pool-2-thread-4} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 17:33:15,896] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-173241_479707786 is finished successfully, status: FINISHED
 INFO [2020-01-22 17:33:15,923] ({pool-2-thread-4} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:33:15,943] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:33:42,937] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:34:42,567] ({qtp2107447833-15} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:34:42,586] ({pool-2-thread-13} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:34:42,588] ({pool-2-thread-13} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 17:34:42,729] ({pool-2-thread-13} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-173241_479707786 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mNameError[0mTraceback (most recent call last)
[0;32m<ipython-input-26-90d93c9c3602>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdf[0m[0;34m.[0m[0mselect[0m[0;34m([0m[0mcol[0m[0;34m([0m[0;34m"soucre"[0m[0;34m)[0m[0;34m,[0m[0mcol[0m[0;34m([0m[0;34m"created_at"[0m[0;34m)[0m[0;34m,[0m[0mcol[0m[0;34m([0m[0;34m"lang"[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;31mNameError[0m: name 'col' is not defined
 INFO [2020-01-22 17:34:42,746] ({pool-2-thread-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:34:42,776] ({pool-2-thread-13} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:34:58,133] ({qtp2107447833-57} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:34:58,155] ({pool-2-thread-7} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:34:58,156] ({pool-2-thread-7} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 17:34:58,228] ({pool-2-thread-7} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-173241_479707786 is finished, status: ERROR, exception: null, result: %text [0;36m  File [0;32m"<ipython-input-28-079f1cc0af7e>"[0;36m, line [0;32m1[0m
[0;31m    df.select($"soucre",$"created_at",$"lang")[0m
[0m              ^[0m
[0;31mSyntaxError[0m[0;31m:[0m invalid syntax

 INFO [2020-01-22 17:34:58,243] ({pool-2-thread-7} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:34:58,267] ({pool-2-thread-7} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:35:26,068] ({qtp2107447833-15} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:35:26,085] ({pool-2-thread-14} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:35:26,085] ({pool-2-thread-14} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 17:35:26,334] ({pool-2-thread-14} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-173241_479707786 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mAnalysisException[0mTraceback (most recent call last)
[0;32m<ipython-input-30-d3d4fb9319b8>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdf[0m[0;34m.[0m[0mselect[0m[0;34m([0m[0;34m"soucre"[0m[0;34m,[0m[0;34m"created_at"[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py[0m in [0;36mselect[0;34m(self, *cols)[0m
[1;32m   1318[0m         [0;34m[[0m[0mRow[0m[0;34m([0m[0mname[0m[0;34m=[0m[0;34mu'Alice'[0m[0;34m,[0m [0mage[0m[0;34m=[0m[0;36m12[0m[0;34m)[0m[0;34m,[0m [0mRow[0m[0;34m([0m[0mname[0m[0;34m=[0m[0;34mu'Bob'[0m[0;34m,[0m [0mage[0m[0;34m=[0m[0;36m15[0m[0;34m)[0m[0;34m][0m[0;34m[0m[0m
[1;32m   1319[0m         """
[0;32m-> 1320[0;31m         [0mjdf[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mselect[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jcols[0m[0;34m([0m[0;34m*[0m[0mcols[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1321[0m         [0;32mreturn[0m [0mDataFrame[0m[0;34m([0m[0mjdf[0m[0;34m,[0m [0mself[0m[0;34m.[0m[0msql_ctx[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1322[0m [0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     67[0m                                              e.java_exception.getStackTrace()))
[1;32m     68[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.AnalysisException: '[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 69[0;31m                 [0;32mraise[0m [0mAnalysisException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     70[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.catalyst.analysis'[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     71[0m                 [0;32mraise[0m [0mAnalysisException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m

[0;31mAnalysisException[0m: u"cannot resolve '`soucre`' given input columns: [extended_entities, coordinates, favorite_count, timestamp_ms, filter_level, in_reply_to_screen_name, quote_count, truncated, text, user, in_reply_to_user_id_str, in_reply_to_status_id_str, place, possibly_sensitive, in_reply_to_user_id, reply_count, retweeted, retweet_count, id_str, quoted_status_id, id, lang, created_at, quoted_status_permalink, extended_tweet, quoted_status, source, display_text_range, contributors, geo, favorited, in_reply_to_status_id, retweeted_status, quoted_status_id_str, entities, is_quote_status];;\n'Project ['soucre, created_at#8, lang#25]\n+- Relation[contributors#6,coordinates#7,created_at#8,display_text_range#9,entities#10,extended_entities#11,extended_tweet#12,favorite_count#13L,favorited#14,filter_level#15,geo#16,id#17L,id_str#18,in_reply_to_screen_name#19,in_reply_to_status_id#20L,in_reply_to_status_id_str#21,in_reply_to_user_id#22L,in_reply_to_user_id_str#23,is_quote_status#24,lang#25,place#26,possibly_sensitive#27,quote_count#28L,quoted_status#29,... 12 more fields] json\n"
 INFO [2020-01-22 17:35:26,354] ({pool-2-thread-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:35:26,366] ({pool-2-thread-14} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:35:32,886] ({qtp2107447833-57} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:35:32,911] ({pool-2-thread-15} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:35:32,913] ({pool-2-thread-15} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 17:35:33,018] ({pool-2-thread-15} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-173241_479707786 is finished successfully, status: FINISHED
 INFO [2020-01-22 17:35:33,047] ({pool-2-thread-15} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:35:33,084] ({pool-2-thread-15} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:35:38,677] ({qtp2107447833-57} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:35:38,689] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:35:38,690] ({pool-2-thread-3} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 17:35:40,299] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-173241_479707786 is finished successfully, status: FINISHED
 INFO [2020-01-22 17:35:40,318] ({pool-2-thread-3} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:35:40,341] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:37:02,351] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:37:02,371] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:37:02,372] ({pool-2-thread-2} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 17:37:02,594] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-173241_479707786 is finished successfully, status: FINISHED
 INFO [2020-01-22 17:37:02,611] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:37:02,637] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:37:10,009] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:37:10,022] ({pool-2-thread-16} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:37:10,023] ({pool-2-thread-16} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 17:37:10,150] ({pool-2-thread-16} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-173241_479707786 is finished successfully, status: FINISHED
 INFO [2020-01-22 17:37:10,174] ({pool-2-thread-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:37:10,189] ({pool-2-thread-16} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:37:22,977] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:37:22,991] ({pool-2-thread-9} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:37:22,991] ({pool-2-thread-9} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 17:37:30,205] ({pool-2-thread-9} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-173241_479707786 is finished successfully, status: FINISHED
 INFO [2020-01-22 17:37:30,249] ({pool-2-thread-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:37:30,299] ({pool-2-thread-9} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:41:28,249] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:41:28,269] ({qtp2107447833-13} NotebookServer.java[broadcastNewParagraph]:688) - Broadcasting paragraph on run call instead of note.
 INFO [2020-01-22 17:41:30,686] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:41:30,762] ({pool-2-thread-32} SchedulerFactory.java[jobStarted]:114) - Job 20200122-174128_1762695518 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:41:30,763] ({pool-2-thread-32} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-174128_1762695518, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 17:41:31,155] ({pool-2-thread-32} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-174128_1762695518 is finished successfully, status: FINISHED
 INFO [2020-01-22 17:41:31,169] ({pool-2-thread-32} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:41:31,189] ({pool-2-thread-32} SchedulerFactory.java[jobFinished]:120) - Job 20200122-174128_1762695518 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:42:38,421] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:42:38,457] ({pool-2-thread-8} SchedulerFactory.java[jobStarted]:114) - Job 20200122-174128_1762695518 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:42:38,457] ({pool-2-thread-8} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-174128_1762695518, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 17:42:38,578] ({pool-2-thread-8} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-174128_1762695518 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mNameError[0mTraceback (most recent call last)
[0;32m<ipython-input-44-7d3ed13ce201>[0m in [0;36m<module>[0;34m()[0m
[1;32m      2[0m [0;32mfrom[0m [0mdatetime[0m [0;32mimport[0m [0mdatetime[0m[0;34m[0m[0m
[1;32m      3[0m [0;34m[0m[0m
[0;32m----> 4[0;31m [0msourceExtractor[0m [0;34m=[0m [0mudf[0m[0;34m([0m[0;32mlambda[0m [0msource[0m[0;34m:[0m [0mBeautifulSoup[0m[0;34m([0m[0msource[0m[0;34m)[0m[0;34m.[0m[0mstring[0m[0;34m.[0m[0mencode[0m[0;34m([0m[0;34m"utf8"[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m      5[0m [0mdateOnlyExtractor[0m [0;34m=[0m [0mudf[0m[0;34m([0m[0;32mlambda[0m [0mdateWithTime[0m[0;34m:[0m [0mdateWithTime[0m[0;34m.[0m[0mstrftime[0m[0;34m([0m[0;34m"%Y-%m-%d"[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m

[0;31mNameError[0m: name 'udf' is not defined
 INFO [2020-01-22 17:42:38,615] ({pool-2-thread-8} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:42:38,632] ({pool-2-thread-8} SchedulerFactory.java[jobFinished]:120) - Job 20200122-174128_1762695518 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:42:51,130] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:42:51,152] ({pool-2-thread-18} SchedulerFactory.java[jobStarted]:114) - Job 20200122-174128_1762695518 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:42:51,153] ({pool-2-thread-18} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-174128_1762695518, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 17:42:51,240] ({pool-2-thread-18} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-174128_1762695518 is finished successfully, status: FINISHED
 INFO [2020-01-22 17:42:51,260] ({pool-2-thread-18} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:42:51,275] ({pool-2-thread-18} SchedulerFactory.java[jobFinished]:120) - Job 20200122-174128_1762695518 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:43:13,265] ({qtp2107447833-57} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:43:22,233] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:43:22,257] ({pool-2-thread-1} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:43:22,262] ({pool-2-thread-1} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 17:43:22,550] ({pool-2-thread-1} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-173241_479707786 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mAnalysisException[0mTraceback (most recent call last)
[0;32m<ipython-input-48-c234a18d53fb>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdf[0m[0;34m.[0m[0mselect[0m[0;34m([0m[0msourceExtractor[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m,[0m[0mdateOnlyExtractor[0m[0;34m([0m[0;34m"created_at"[0m[0;34m)[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mgroupBy[0m[0;34m([0m[0;34m"source"[0m[0;34m,[0m[0;34m"created_at"[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mcount[0m[0;34m([0m[0;34m)[0m[0;34m.[0m[0mshow[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/group.py[0m in [0;36m_api[0;34m(self)[0m
[1;32m     30[0m     [0;32mdef[0m [0m_api[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     31[0m         [0mname[0m [0;34m=[0m [0mf[0m[0;34m.[0m[0m__name__[0m[0;34m[0m[0m
[0;32m---> 32[0;31m         [0mjdf[0m [0;34m=[0m [0mgetattr[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jgd[0m[0;34m,[0m [0mname[0m[0;34m)[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     33[0m         [0;32mreturn[0m [0mDataFrame[0m[0;34m([0m[0mjdf[0m[0;34m,[0m [0mself[0m[0;34m.[0m[0msql_ctx[0m[0;34m)[0m[0;34m[0m[0m
[1;32m     34[0m     [0m_api[0m[0;34m.[0m[0m__name__[0m [0;34m=[0m [0mf[0m[0;34m.[0m[0m__name__[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     67[0m                                              e.java_exception.getStackTrace()))
[1;32m     68[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.AnalysisException: '[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 69[0;31m                 [0;32mraise[0m [0mAnalysisException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     70[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.catalyst.analysis'[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     71[0m                 [0;32mraise[0m [0mAnalysisException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m

[0;31mAnalysisException[0m: u"cannot resolve '`source`' given input columns: [<lambda>(source), <lambda>(created_at), lang];;\n'Aggregate ['source, 'created_at, lang#25], ['source, 'created_at, lang#25, count(1) AS count#1261L]\n+- Project [<lambda>(source#37) AS <lambda>(source)#1252, <lambda>(created_at#8) AS <lambda>(created_at)#1253, lang#25]\n   +- Relation[contributors#6,coordinates#7,created_at#8,display_text_range#9,entities#10,extended_entities#11,extended_tweet#12,favorite_count#13L,favorited#14,filter_level#15,geo#16,id#17L,id_str#18,in_reply_to_screen_name#19,in_reply_to_status_id#20L,in_reply_to_status_id_str#21,in_reply_to_user_id#22L,in_reply_to_user_id_str#23,is_quote_status#24,lang#25,place#26,possibly_sensitive#27,quote_count#28L,quoted_status#29,... 12 more fields] json\n"
 INFO [2020-01-22 17:43:22,565] ({pool-2-thread-1} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:43:22,584] ({pool-2-thread-1} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:43:49,402] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:43:49,418] ({pool-2-thread-19} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:43:49,418] ({pool-2-thread-19} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 17:43:49,479] ({pool-2-thread-19} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-173241_479707786 is finished, status: ERROR, exception: null, result: %text [0;36m  File [0;32m"<ipython-input-50-4dab854abbbe>"[0;36m, line [0;32m1[0m
[0;31m    df.select(sourceExtractor("source").as("source"),dateOnlyExtractor("created_at").as("created_at"),"lang").groupBy("source","created_at","lang").count().show()[0m
[0m                                         ^[0m
[0;31mSyntaxError[0m[0;31m:[0m invalid syntax

 INFO [2020-01-22 17:43:49,503] ({pool-2-thread-19} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:43:49,518] ({pool-2-thread-19} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:43:56,791] ({qtp2107447833-57} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:43:56,812] ({pool-2-thread-10} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:43:56,813] ({pool-2-thread-10} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 17:44:01,180] ({pool-2-thread-10} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-173241_479707786 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mPy4JJavaError[0mTraceback (most recent call last)
[0;32m<ipython-input-52-e4874b15b491>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdf[0m[0;34m.[0m[0mselect[0m[0;34m([0m[0msourceExtractor[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m,[0m[0mdateOnlyExtractor[0m[0;34m([0m[0;34m"created_at"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"created_at"[0m[0;34m)[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mgroupBy[0m[0;34m([0m[0;34m"source"[0m[0;34m,[0m[0;34m"created_at"[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mcount[0m[0;34m([0m[0;34m)[0m[0;34m.[0m[0mshow[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py[0m in [0;36mshow[0;34m(self, n, truncate, vertical)[0m
[1;32m    376[0m         """
[1;32m    377[0m         [0;32mif[0m [0misinstance[0m[0;34m([0m[0mtruncate[0m[0;34m,[0m [0mbool[0m[0;34m)[0m [0;32mand[0m [0mtruncate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 378[0;31m             [0;32mprint[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mshowString[0m[0;34m([0m[0mn[0m[0;34m,[0m [0;36m20[0m[0;34m,[0m [0mvertical[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    379[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    380[0m             [0;32mprint[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mshowString[0m[0;34m([0m[0mn[0m[0;34m,[0m [0mint[0m[0;34m([0m[0mtruncate[0m[0;34m)[0m[0;34m,[0m [0mvertical[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     61[0m     [0;32mdef[0m [0mdeco[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     62[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 63[0;31m             [0;32mreturn[0m [0mf[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     64[0m         [0;32mexcept[0m [0mpy4j[0m[0;34m.[0m[0mprotocol[0m[0;34m.[0m[0mPy4JJavaError[0m [0;32mas[0m [0me[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     65[0m             [0ms[0m [0;34m=[0m [0me[0m[0;34m.[0m[0mjava_exception[0m[0;34m.[0m[0mtoString[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py[0m in [0;36mget_return_value[0;34m(answer, gateway_client, target_id, name)[0m
[1;32m    326[0m                 raise Py4JJavaError(
[1;32m    327[0m                     [0;34m"An error occurred while calling {0}{1}{2}.\n"[0m[0;34m.[0m[0;34m[0m[0m
[0;32m--> 328[0;31m                     format(target_id, ".", name), value)
[0m[1;32m    329[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    330[0m                 raise Py4JError(

[0;31mPy4JJavaError[0m: An error occurred while calling o411.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 17.0 failed 4 times, most recent failure: Lost task 0.3 in stage 17.0 (TID 175, 172.18.0.9, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 361, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 236, in read_udfs
    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type, runner_conf)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 163, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 64, in read_command
    command = serializer._read_with_length(file)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 580, in loads
    return pickle.loads(obj)
ImportError: No module named bs4

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2759)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:292)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 361, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 236, in read_udfs
    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type, runner_conf)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 163, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 64, in read_command
    command = serializer._read_with_length(file)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 580, in loads
    return pickle.loads(obj)
ImportError: No module named bs4

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

 INFO [2020-01-22 17:44:01,240] ({pool-2-thread-10} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:44:01,267] ({pool-2-thread-10} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:44:16,045] ({qtp2107447833-14} InterpreterRestApi.java[restartSetting]:180) - Restart interpreterSetting spark, msg=
 INFO [2020-01-22 17:44:16,046] ({qtp2107447833-14} InterpreterSetting.java[close]:483) - Close InterpreterSetting: spark
 INFO [2020-01-22 17:44:16,046] ({qtp2107447833-14} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: spark:shared_process
 INFO [2020-01-22 17:44:16,046] ({qtp2107447833-14} ManagedInterpreterGroup.java[close]:100) - Close Session: shared_session for interpreter setting: spark
 WARN [2020-01-22 17:44:16,046] ({qtp2107447833-14} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.SparkInterpreter
 WARN [2020-01-22 17:44:16,051] ({qtp2107447833-14} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.SparkSqlInterpreter
 WARN [2020-01-22 17:44:16,387] ({qtp2107447833-14} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.IPySparkInterpreter
 WARN [2020-01-22 17:44:16,388] ({qtp2107447833-14} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.SparkRInterpreter
 INFO [2020-01-22 17:44:16,388] ({qtp2107447833-14} ManagedInterpreterGroup.java[close]:105) - Remove this InterpreterGroup: spark:shared_process as all the sessions are closed
 INFO [2020-01-22 17:44:16,388] ({qtp2107447833-14} ManagedInterpreterGroup.java[close]:108) - Kill RemoteInterpreterProcess
 INFO [2020-01-22 17:44:16,390] ({qtp2107447833-14} RemoteInterpreterManagedProcess.java[stop]:220) - Kill interpreter process
ERROR [2020-01-22 17:44:17,166] ({Thread-46} RemoteInterpreterEventPoller.java[run]:257) - Can not get RemoteInterpreterEvent because it is shutdown.
ERROR [2020-01-22 17:44:17,166] ({pool-9-thread-1} AppendOutputRunner.java[run]:68) - Wait for OutputBuffer queue interrupted: null
 WARN [2020-01-22 17:44:18,911] ({qtp2107447833-14} RemoteInterpreterManagedProcess.java[stop]:230) - ignore the exception when shutting down
 INFO [2020-01-22 17:44:18,913] ({qtp2107447833-14} RemoteInterpreterManagedProcess.java[stop]:238) - Remote process terminated
 INFO [2020-01-22 17:44:18,917] ({Exec Default Executor} RemoteInterpreterManagedProcess.java[onProcessFailed]:250) - Interpreter process failed {}
org.apache.commons.exec.ExecuteException: Process exited with an error: 143 (Exit value: 143)
	at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:404)
	at org.apache.commons.exec.DefaultExecutor.access$200(DefaultExecutor.java:48)
	at org.apache.commons.exec.DefaultExecutor$1.run(DefaultExecutor.java:200)
	at java.lang.Thread.run(Thread.java:748)
 INFO [2020-01-22 17:44:36,657] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:44:36,675] ({qtp2107447833-11} InterpreterSetting.java[getOrCreateInterpreterGroup]:419) - Create InterpreterGroup with groupId: spark:shared_process for user: anonymous and note: 2EXQEGYMG
 INFO [2020-01-22 17:44:36,675] ({qtp2107447833-11} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.SparkInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 17:44:36,676] ({qtp2107447833-11} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.SparkSqlInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 17:44:36,676] ({qtp2107447833-11} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.DepInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 17:44:36,676] ({qtp2107447833-11} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.PySparkInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 17:44:36,676] ({qtp2107447833-11} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.IPySparkInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 17:44:36,676] ({qtp2107447833-11} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.SparkRInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 17:44:36,676] ({qtp2107447833-11} ManagedInterpreterGroup.java[getOrCreateSession]:158) - Create Session: shared_session in InterpreterGroup: spark:shared_process for user: anonymous
 INFO [2020-01-22 17:44:36,679] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200121-185651_2144996466 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:44:36,680] ({pool-2-thread-4} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-185651_2144996466, interpreter: dep, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 17:44:36,680] ({pool-2-thread-4} ManagedInterpreterGroup.java[getOrCreateInterpreterProcess]:61) - Create InterpreterProcess for InterpreterGroup: spark:shared_process
 INFO [2020-01-22 17:44:36,681] ({pool-2-thread-4} ShellScriptLauncher.java[launch]:48) - Launching Interpreter: spark
 INFO [2020-01-22 17:44:36,682] ({pool-2-thread-4} SparkInterpreterLauncher.java[buildEnvFromProperties]:108) - Run Spark under non-secure mode as no keytab and principal is specified
 INFO [2020-01-22 17:44:36,686] ({pool-2-thread-4} RemoteInterpreterManagedProcess.java[start]:115) - Thrift server for callback will start. Port: 39141
 INFO [2020-01-22 17:44:37,189] ({pool-2-thread-4} RemoteInterpreterManagedProcess.java[start]:190) - Run interpreter process [/zeppelin/bin/interpreter.sh, -d, /zeppelin/interpreter/spark, -c, 172.18.0.2, -p, 39141, -r, :, -l, /usr/local/local-repo/spark, -g, spark]
 INFO [2020-01-22 17:44:40,358] ({pool-12-thread-1} RemoteInterpreterManagedProcess.java[callback]:123) - RemoteInterpreterServer Registered: CallbackInfo(host:172.18.0.2, port:36011)
 INFO [2020-01-22 17:44:40,361] ({pool-2-thread-4} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkInterpreter
 INFO [2020-01-22 17:44:40,469] ({pool-2-thread-4} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkSqlInterpreter
 INFO [2020-01-22 17:44:40,475] ({pool-2-thread-4} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.DepInterpreter
 INFO [2020-01-22 17:44:40,477] ({pool-2-thread-4} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.PySparkInterpreter
 INFO [2020-01-22 17:44:40,482] ({pool-2-thread-4} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.IPySparkInterpreter
 INFO [2020-01-22 17:44:40,487] ({pool-2-thread-4} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkRInterpreter
 INFO [2020-01-22 17:44:40,489] ({pool-2-thread-4} RemoteInterpreter.java[call]:142) - Open RemoteInterpreter org.apache.zeppelin.spark.DepInterpreter
 INFO [2020-01-22 17:44:40,489] ({pool-2-thread-4} RemoteInterpreter.java[pushAngularObjectRegistryToRemote]:436) - Push local angular object registry from ZeppelinServer to remote interpreter group spark:shared_process
 INFO [2020-01-22 17:44:47,671] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-185651_2144996466 is finished successfully, status: FINISHED
 INFO [2020-01-22 17:44:47,685] ({pool-2-thread-4} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:44:47,728] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200121-185651_2144996466 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:45:32,075] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:45:32,095] ({pool-2-thread-24} SchedulerFactory.java[jobStarted]:114) - Job 20200121-183200_1639671760 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:45:32,096] ({pool-2-thread-24} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-183200_1639671760, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 17:45:32,097] ({pool-2-thread-24} RemoteInterpreter.java[call]:142) - Open RemoteInterpreter org.apache.zeppelin.spark.PySparkInterpreter
 INFO [2020-01-22 17:45:47,765] ({pool-2-thread-24} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-183200_1639671760 is finished successfully, status: FINISHED
 INFO [2020-01-22 17:45:47,778] ({pool-2-thread-24} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:45:47,792] ({pool-2-thread-24} SchedulerFactory.java[jobFinished]:120) - Job 20200121-183200_1639671760 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:46:38,403] ({qtp2107447833-15} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:46:38,422] ({pool-2-thread-13} SchedulerFactory.java[jobStarted]:114) - Job 20200121-183236_977276561 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:46:38,423] ({pool-2-thread-13} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-183236_977276561, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 17:46:51,545] ({pool-2-thread-13} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-183236_977276561 is finished successfully, status: FINISHED
 INFO [2020-01-22 17:46:51,559] ({pool-2-thread-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:46:51,597] ({pool-2-thread-13} SchedulerFactory.java[jobFinished]:120) - Job 20200121-183236_977276561 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:46:53,748] ({qtp2107447833-57} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:46:53,761] ({pool-2-thread-25} SchedulerFactory.java[jobStarted]:114) - Job 20200122-174128_1762695518 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:46:53,762] ({pool-2-thread-25} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-174128_1762695518, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 17:46:53,999] ({pool-2-thread-25} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-174128_1762695518 is finished successfully, status: FINISHED
 INFO [2020-01-22 17:46:54,012] ({pool-2-thread-25} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:46:54,028] ({pool-2-thread-25} SchedulerFactory.java[jobFinished]:120) - Job 20200122-174128_1762695518 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:46:57,001] ({qtp2107447833-57} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:46:57,015] ({pool-2-thread-7} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:46:57,018] ({pool-2-thread-7} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 17:47:03,015] ({pool-2-thread-7} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-173241_479707786 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mPy4JJavaError[0mTraceback (most recent call last)
[0;32m<ipython-input-10-e4874b15b491>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdf[0m[0;34m.[0m[0mselect[0m[0;34m([0m[0msourceExtractor[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m,[0m[0mdateOnlyExtractor[0m[0;34m([0m[0;34m"created_at"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"created_at"[0m[0;34m)[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mgroupBy[0m[0;34m([0m[0;34m"source"[0m[0;34m,[0m[0;34m"created_at"[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mcount[0m[0;34m([0m[0;34m)[0m[0;34m.[0m[0mshow[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py[0m in [0;36mshow[0;34m(self, n, truncate, vertical)[0m
[1;32m    376[0m         """
[1;32m    377[0m         [0;32mif[0m [0misinstance[0m[0;34m([0m[0mtruncate[0m[0;34m,[0m [0mbool[0m[0;34m)[0m [0;32mand[0m [0mtruncate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 378[0;31m             [0;32mprint[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mshowString[0m[0;34m([0m[0mn[0m[0;34m,[0m [0;36m20[0m[0;34m,[0m [0mvertical[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    379[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    380[0m             [0;32mprint[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mshowString[0m[0;34m([0m[0mn[0m[0;34m,[0m [0mint[0m[0;34m([0m[0mtruncate[0m[0;34m)[0m[0;34m,[0m [0mvertical[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     61[0m     [0;32mdef[0m [0mdeco[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     62[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 63[0;31m             [0;32mreturn[0m [0mf[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     64[0m         [0;32mexcept[0m [0mpy4j[0m[0;34m.[0m[0mprotocol[0m[0;34m.[0m[0mPy4JJavaError[0m [0;32mas[0m [0me[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     65[0m             [0ms[0m [0;34m=[0m [0me[0m[0;34m.[0m[0mjava_exception[0m[0;34m.[0m[0mtoString[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py[0m in [0;36mget_return_value[0;34m(answer, gateway_client, target_id, name)[0m
[1;32m    326[0m                 raise Py4JJavaError(
[1;32m    327[0m                     [0;34m"An error occurred while calling {0}{1}{2}.\n"[0m[0;34m.[0m[0;34m[0m[0m
[0;32m--> 328[0;31m                     format(target_id, ".", name), value)
[0m[1;32m    329[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    330[0m                 raise Py4JError(

[0;31mPy4JJavaError[0m: An error occurred while calling o109.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 4, 172.18.0.9, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 361, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 236, in read_udfs
    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type, runner_conf)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 163, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 64, in read_command
    command = serializer._read_with_length(file)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 580, in loads
    return pickle.loads(obj)
ImportError: No module named bs4

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2759)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:292)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 361, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 236, in read_udfs
    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type, runner_conf)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 163, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 64, in read_command
    command = serializer._read_with_length(file)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 580, in loads
    return pickle.loads(obj)
ImportError: No module named bs4

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

 INFO [2020-01-22 17:47:03,038] ({pool-2-thread-7} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:47:03,052] ({pool-2-thread-7} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:48:15,522] ({Thread-34} ZeppelinServer.java[run]:253) - Shutting down Zeppelin Server ... 
 INFO [2020-01-22 17:48:15,530] ({qtp2107447833-12} NotebookServer.java[onClose]:372) - Closed connection to 192.168.99.1 : 14970. (1000) null
 INFO [2020-01-22 17:48:15,530] ({qtp2107447833-11} NotebookServer.java[onClose]:372) - Closed connection to 192.168.99.1 : 14962. (1000) null
 INFO [2020-01-22 17:48:15,531] ({qtp2107447833-15} NotebookServer.java[onClose]:372) - Closed connection to 192.168.99.1 : 15052. (1000) null
 INFO [2020-01-22 17:48:15,542] ({Thread-34} AbstractConnector.java[doStop]:341) - Stopped ServerConnector@79a41e8{HTTP/1.1,[http/1.1]}{0.0.0.0:8080}
 INFO [2020-01-22 17:48:15,542] ({Thread-34} HouseKeeper.java[stopScavenging]:167) - node0 Stopped scavenging
 INFO [2020-01-22 17:48:17,084] ({Thread-34} ContextHandler.java[doStop]:1045) - Stopped o.e.j.w.WebAppContext@5af97850{zeppelin-web,/,null,UNAVAILABLE}{/zeppelin/zeppelin-web-0.8.2.war}
 INFO [2020-01-22 17:48:17,086] ({Thread-108} InterpreterSetting.java[close]:483) - Close InterpreterSetting: python
 INFO [2020-01-22 17:48:17,086] ({Thread-110} InterpreterSetting.java[close]:483) - Close InterpreterSetting: jdbc
 INFO [2020-01-22 17:48:17,086] ({Thread-107} InterpreterSetting.java[close]:483) - Close InterpreterSetting: ignite
 INFO [2020-01-22 17:48:17,087] ({Thread-112} InterpreterSetting.java[close]:483) - Close InterpreterSetting: pig
 INFO [2020-01-22 17:48:17,086] ({Thread-109} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sap
 INFO [2020-01-22 17:48:17,087] ({Thread-116} InterpreterSetting.java[close]:483) - Close InterpreterSetting: neo4j
 INFO [2020-01-22 17:48:17,087] ({Thread-114} InterpreterSetting.java[close]:483) - Close InterpreterSetting: angular
 INFO [2020-01-22 17:48:17,087] ({Thread-113} InterpreterSetting.java[close]:483) - Close InterpreterSetting: flink
 INFO [2020-01-22 17:48:17,087] ({Thread-111} InterpreterSetting.java[close]:483) - Close InterpreterSetting: lens
 INFO [2020-01-22 17:48:17,088] ({Thread-118} InterpreterSetting.java[close]:483) - Close InterpreterSetting: groovy
 INFO [2020-01-22 17:48:17,088] ({Thread-117} InterpreterSetting.java[close]:483) - Close InterpreterSetting: file
 INFO [2020-01-22 17:48:17,087] ({Thread-115} InterpreterSetting.java[close]:483) - Close InterpreterSetting: livy
 INFO [2020-01-22 17:48:17,088] ({Thread-121} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sh
 INFO [2020-01-22 17:48:17,088] ({Thread-119} InterpreterSetting.java[close]:483) - Close InterpreterSetting: elasticsearch
 INFO [2020-01-22 17:48:17,089] ({Thread-122} InterpreterSetting.java[close]:483) - Close InterpreterSetting: spark
 INFO [2020-01-22 17:48:17,089] ({Thread-120} InterpreterSetting.java[close]:483) - Close InterpreterSetting: cassandra
 INFO [2020-01-22 17:48:17,089] ({Thread-124} InterpreterSetting.java[close]:483) - Close InterpreterSetting: alluxio
 INFO [2020-01-22 17:48:17,089] ({Thread-123} InterpreterSetting.java[close]:483) - Close InterpreterSetting: md
 INFO [2020-01-22 17:48:17,089] ({Thread-122} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: spark:shared_process
 INFO [2020-01-22 17:48:17,090] ({Thread-122} ManagedInterpreterGroup.java[close]:100) - Close Session: shared_session for interpreter setting: spark
 WARN [2020-01-22 17:48:17,090] ({Thread-122} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.SparkInterpreter
 INFO [2020-01-22 17:48:17,090] ({Thread-125} InterpreterSetting.java[close]:483) - Close InterpreterSetting: bigquery
 INFO [2020-01-22 17:48:17,090] ({Thread-126} InterpreterSetting.java[close]:483) - Close InterpreterSetting: hbase
 INFO [2020-01-22 17:48:17,092] ({Thread-128} InterpreterSetting.java[close]:483) - Close InterpreterSetting: ignite
 INFO [2020-01-22 17:48:17,092] ({Thread-129} InterpreterSetting.java[close]:483) - Close InterpreterSetting: python
 INFO [2020-01-22 17:48:17,093] ({Thread-130} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sap
 INFO [2020-01-22 17:48:17,093] ({Thread-131} InterpreterSetting.java[close]:483) - Close InterpreterSetting: jdbc
 INFO [2020-01-22 17:48:17,095] ({Thread-132} InterpreterSetting.java[close]:483) - Close InterpreterSetting: lens
 INFO [2020-01-22 17:48:17,095] ({Thread-133} InterpreterSetting.java[close]:483) - Close InterpreterSetting: pig
 INFO [2020-01-22 17:48:17,095] ({Thread-134} InterpreterSetting.java[close]:483) - Close InterpreterSetting: flink
 INFO [2020-01-22 17:48:17,096] ({Thread-135} InterpreterSetting.java[close]:483) - Close InterpreterSetting: angular
 INFO [2020-01-22 17:48:17,096] ({Thread-137} InterpreterSetting.java[close]:483) - Close InterpreterSetting: neo4j
 INFO [2020-01-22 17:48:17,097] ({Thread-138} InterpreterSetting.java[close]:483) - Close InterpreterSetting: file
 INFO [2020-01-22 17:48:17,097] ({Thread-136} InterpreterSetting.java[close]:483) - Close InterpreterSetting: livy
 INFO [2020-01-22 17:48:17,098] ({Thread-140} InterpreterSetting.java[close]:483) - Close InterpreterSetting: elasticsearch
 INFO [2020-01-22 17:48:17,098] ({Thread-139} InterpreterSetting.java[close]:483) - Close InterpreterSetting: groovy
 INFO [2020-01-22 17:48:17,098] ({Thread-141} InterpreterSetting.java[close]:483) - Close InterpreterSetting: cassandra
 INFO [2020-01-22 17:48:17,099] ({Thread-143} InterpreterSetting.java[close]:483) - Close InterpreterSetting: spark
 INFO [2020-01-22 17:48:17,099] ({Thread-142} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sh
 INFO [2020-01-22 17:48:17,099] ({Thread-145} InterpreterSetting.java[close]:483) - Close InterpreterSetting: alluxio
 INFO [2020-01-22 17:48:17,099] ({Thread-144} InterpreterSetting.java[close]:483) - Close InterpreterSetting: md
 INFO [2020-01-22 17:48:17,099] ({Thread-147} InterpreterSetting.java[close]:483) - Close InterpreterSetting: hbase
 INFO [2020-01-22 17:48:17,099] ({Thread-148} InterpreterSetting.java[close]:483) - Close InterpreterSetting: kylin
 INFO [2020-01-22 17:48:17,099] ({Thread-127} InterpreterSetting.java[close]:483) - Close InterpreterSetting: kylin
 INFO [2020-01-22 17:48:17,099] ({Thread-146} InterpreterSetting.java[close]:483) - Close InterpreterSetting: bigquery
 WARN [2020-01-22 17:48:17,100] ({Thread-122} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.SparkSqlInterpreter
 WARN [2020-01-22 17:48:17,380] ({Thread-122} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.IPySparkInterpreter
 WARN [2020-01-22 17:48:17,381] ({Thread-122} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.SparkRInterpreter
 INFO [2020-01-22 17:48:17,382] ({Thread-122} ManagedInterpreterGroup.java[close]:105) - Remove this InterpreterGroup: spark:shared_process as all the sessions are closed
 INFO [2020-01-22 17:48:17,382] ({Thread-122} ManagedInterpreterGroup.java[close]:108) - Kill RemoteInterpreterProcess
 INFO [2020-01-22 17:48:17,382] ({Thread-122} RemoteInterpreterManagedProcess.java[stop]:220) - Kill interpreter process
ERROR [2020-01-22 17:48:18,092] ({Thread-90} RemoteInterpreterEventPoller.java[run]:257) - Can not get RemoteInterpreterEvent because it is shutdown.
ERROR [2020-01-22 17:48:18,092] ({pool-11-thread-1} AppendOutputRunner.java[run]:68) - Wait for OutputBuffer queue interrupted: null
 WARN [2020-01-22 17:48:19,921] ({Thread-122} RemoteInterpreterManagedProcess.java[stop]:230) - ignore the exception when shutting down
 INFO [2020-01-22 17:48:19,921] ({Thread-122} RemoteInterpreterManagedProcess.java[stop]:238) - Remote process terminated
 INFO [2020-01-22 17:48:19,922] ({Thread-143} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: spark:shared_process
 INFO [2020-01-22 17:48:19,923] ({Thread-34} NotebookRepoSync.java[close]:428) - Closing all notebook storages
 INFO [2020-01-22 17:48:19,931] ({Exec Default Executor} RemoteInterpreterManagedProcess.java[onProcessFailed]:250) - Interpreter process failed {}
org.apache.commons.exec.ExecuteException: Process exited with an error: 143 (Exit value: 143)
	at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:404)
	at org.apache.commons.exec.DefaultExecutor.access$200(DefaultExecutor.java:48)
	at org.apache.commons.exec.DefaultExecutor$1.run(DefaultExecutor.java:200)
	at java.lang.Thread.run(Thread.java:748)
 INFO [2020-01-22 17:48:22,935] ({Thread-34} ZeppelinServer.java[run]:264) - Bye
 WARN [2020-01-22 17:49:42,707] ({main} ZeppelinConfiguration.java[create]:117) - Failed to load configuration, proceeding with a default
 INFO [2020-01-22 17:49:43,026] ({main} ZeppelinConfiguration.java[create]:129) - Server Host: 0.0.0.0
 INFO [2020-01-22 17:49:43,026] ({main} ZeppelinConfiguration.java[create]:131) - Server Port: 8080
 INFO [2020-01-22 17:49:43,035] ({main} ZeppelinConfiguration.java[create]:135) - Context Path: /
 INFO [2020-01-22 17:49:43,059] ({main} ZeppelinConfiguration.java[create]:136) - Zeppelin Version: 0.8.2
 INFO [2020-01-22 17:49:43,175] ({main} Log.java[initialized]:193) - Logging initialized @2565ms to org.eclipse.jetty.util.log.Slf4jLog
 WARN [2020-01-22 17:49:44,057] ({main} ServerConnector.java[setSoLingerTime]:458) - Ignoring deprecated socket close linger time
 INFO [2020-01-22 17:49:44,483] ({main} ZeppelinServer.java[setupWebAppContext]:413) - ZeppelinServer Webapp path: /zeppelin/webapps
 INFO [2020-01-22 17:49:45,129] ({main} ZeppelinServer.java[main]:239) - Starting zeppelin server
 INFO [2020-01-22 17:49:45,143] ({main} Server.java[doStart]:370) - jetty-9.4.14.v20181114; built: 2018-11-14T21:20:31.478Z; git: c4550056e785fb5665914545889f21dc136ad9e6; jvm 1.8.0_222-8u222-b10-1ubuntu1~16.04.1-b10
 INFO [2020-01-22 17:49:57,905] ({main} StandardDescriptorProcessor.java[visitServlet]:283) - NO JSP Support for /, did not find org.eclipse.jetty.jsp.JettyJspServlet
 INFO [2020-01-22 17:49:57,962] ({main} DefaultSessionIdManager.java[doStart]:365) - DefaultSessionIdManager workerName=node0
 INFO [2020-01-22 17:49:57,971] ({main} DefaultSessionIdManager.java[doStart]:370) - No SessionScavenger set, using defaults
 INFO [2020-01-22 17:49:57,986] ({main} HouseKeeper.java[startScavenging]:149) - node0 Scavenging every 660000ms
 INFO [2020-01-22 17:50:00,078] ({main} SchedulerFactory.java[<init>]:59) - Scheduler Thread Pool Size: 100
 WARN [2020-01-22 17:50:00,152] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir
 WARN [2020-01-22 17:50:00,179] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir
 INFO [2020-01-22 17:50:00,667] ({main} InterpreterSettingManager.java[<init>]:165) - Using RecoveryStorage: org.apache.zeppelin.interpreter.recovery.NullRecoveryStorage
 INFO [2020-01-22 17:50:00,679] ({main} InterpreterSettingManager.java[<init>]:169) - Using LifecycleManager: org.apache.zeppelin.interpreter.lifecycle.NullLifecycleManager
 INFO [2020-01-22 17:50:00,962] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: cassandra
 INFO [2020-01-22 17:50:00,989] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: md
 INFO [2020-01-22 17:50:01,005] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: sap
 INFO [2020-01-22 17:50:01,025] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: sh
 INFO [2020-01-22 17:50:01,067] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: angular
 WARN [2020-01-22 17:50:01,095] ({main} InterpreterSettingManager.java[init]:331) - No interpreter-setting.json found in /zeppelin/interpreter/lib
 INFO [2020-01-22 17:50:01,126] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: hbase
 INFO [2020-01-22 17:50:01,145] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: elasticsearch
 WARN [2020-01-22 17:50:01,589] ({main} InterpreterSettingManager.java[init]:331) - No interpreter-setting.json found in /zeppelin/interpreter/scio
 INFO [2020-01-22 17:50:01,638] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: alluxio
 INFO [2020-01-22 17:50:01,683] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: jdbc
 INFO [2020-01-22 17:50:01,722] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: neo4j
 INFO [2020-01-22 17:50:01,762] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: python
 INFO [2020-01-22 17:50:01,791] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: pig
 INFO [2020-01-22 17:50:01,860] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: spark
 INFO [2020-01-22 17:50:01,877] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: lens
 INFO [2020-01-22 17:50:01,915] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: bigquery
 INFO [2020-01-22 17:50:01,954] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: file
 INFO [2020-01-22 17:50:01,978] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: ignite
 INFO [2020-01-22 17:50:01,999] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: kylin
 INFO [2020-01-22 17:50:02,043] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: livy
 INFO [2020-01-22 17:50:02,068] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: groovy
 INFO [2020-01-22 17:50:02,083] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: flink
 WARN [2020-01-22 17:50:02,094] ({main} InterpreterSettingManager.java[init]:331) - No interpreter-setting.json found in /zeppelin/interpreter/${interpreter.name}
 INFO [2020-01-22 17:50:02,095] ({main} LocalConfigStorage.java[loadInterpreterSettings]:63) - Load Interpreter Setting from file: /zeppelin/conf/interpreter.json
 INFO [2020-01-22 17:50:02,572] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting ignite from interpreter.json
 INFO [2020-01-22 17:50:02,585] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting python from interpreter.json
 INFO [2020-01-22 17:50:02,601] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting sap from interpreter.json
 INFO [2020-01-22 17:50:02,614] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting jdbc from interpreter.json
 INFO [2020-01-22 17:50:02,616] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting lens from interpreter.json
 INFO [2020-01-22 17:50:02,626] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting pig from interpreter.json
 INFO [2020-01-22 17:50:02,628] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting flink from interpreter.json
 INFO [2020-01-22 17:50:02,634] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting angular from interpreter.json
 INFO [2020-01-22 17:50:02,647] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting livy from interpreter.json
 INFO [2020-01-22 17:50:02,653] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting neo4j from interpreter.json
 INFO [2020-01-22 17:50:02,658] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting file from interpreter.json
 INFO [2020-01-22 17:50:02,658] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting groovy from interpreter.json
 INFO [2020-01-22 17:50:02,670] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting elasticsearch from interpreter.json
 INFO [2020-01-22 17:50:02,688] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting cassandra from interpreter.json
 INFO [2020-01-22 17:50:02,710] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting sh from interpreter.json
 INFO [2020-01-22 17:50:02,712] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting spark from interpreter.json
 INFO [2020-01-22 17:50:02,718] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting md from interpreter.json
 INFO [2020-01-22 17:50:02,720] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting alluxio from interpreter.json
 INFO [2020-01-22 17:50:02,726] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting bigquery from interpreter.json
 INFO [2020-01-22 17:50:02,727] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting hbase from interpreter.json
 INFO [2020-01-22 17:50:02,728] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting kylin from interpreter.json
 INFO [2020-01-22 17:50:02,756] ({main} LocalConfigStorage.java[save]:53) - Save Interpreter Setting to /zeppelin/conf/interpreter.json
 INFO [2020-01-22 17:50:03,487] ({main} VfsLog.java[info]:138) - Using "/tmp/vfs_cache" as temporary files store.
ERROR [2020-01-22 17:50:03,682] ({Thread-24} InterpreterSetting.java[run]:840) - Error while downloading repos for interpreter group : spark, go to interpreter setting page click on edit and save it again to make this interpreter work properly. : Cannot fetch dependencies for org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0
org.sonatype.aether.RepositoryException: Cannot fetch dependencies for org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0
	at org.apache.zeppelin.dep.DependencyResolver.getArtifactsWithDep(DependencyResolver.java:179)
	at org.apache.zeppelin.dep.DependencyResolver.loadFromMvn(DependencyResolver.java:128)
	at org.apache.zeppelin.dep.DependencyResolver.load(DependencyResolver.java:76)
	at org.apache.zeppelin.dep.DependencyResolver.load(DependencyResolver.java:93)
	at org.apache.zeppelin.dep.DependencyResolver.load(DependencyResolver.java:85)
	at org.apache.zeppelin.interpreter.InterpreterSetting$1.run(InterpreterSetting.java:832)
Caused by: java.lang.NullPointerException
	at org.sonatype.aether.impl.internal.DefaultRepositorySystem.resolveDependencies(DefaultRepositorySystem.java:352)
	at org.apache.zeppelin.dep.DependencyResolver.getArtifactsWithDep(DependencyResolver.java:176)
	... 5 more
 INFO [2020-01-22 17:50:03,785] ({main} GitNotebookRepo.java[<init>]:64) - Opening a git repo at '/zeppelin/notebook'
 INFO [2020-01-22 17:50:04,066] ({main} NotebookRepoSync.java[<init>]:77) - Instantiate NotebookRepo: org.apache.zeppelin.notebook.repo.GitNotebookRepo
 WARN [2020-01-22 17:50:04,389] ({main} NotebookAuthorization.java[getInstance]:86) - Notebook authorization module was called without initialization, initializing with default configuration
 WARN [2020-01-22 17:50:04,392] ({main} LocalConfigStorage.java[loadNotebookAuthorization]:77) - NotebookAuthorization file /zeppelin/conf/notebook-authorization.json is not existed
 INFO [2020-01-22 17:50:04,395] ({main} Credentials.java[loadFromFile]:121) - /zeppelin/conf/credentials.json
 INFO [2020-01-22 17:50:04,531] ({main} StdSchedulerFactory.java[instantiate]:1184) - Using default implementation for ThreadExecutor
 INFO [2020-01-22 17:50:04,536] ({main} SimpleThreadPool.java[initialize]:268) - Job execution threads will use class loader of thread: main
 INFO [2020-01-22 17:50:04,566] ({main} SchedulerSignalerImpl.java[<init>]:61) - Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
 INFO [2020-01-22 17:50:04,573] ({main} QuartzScheduler.java[<init>]:240) - Quartz Scheduler v.2.2.1 created.
 INFO [2020-01-22 17:50:04,580] ({main} RAMJobStore.java[initialize]:155) - RAMJobStore initialized.
 INFO [2020-01-22 17:50:04,581] ({main} QuartzScheduler.java[initialize]:305) - Scheduler meta-data: Quartz Scheduler (v2.2.1) 'DefaultQuartzScheduler' with instanceId 'NON_CLUSTERED'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'org.quartz.simpl.SimpleThreadPool' - with 10 threads.
  Using job-store 'org.quartz.simpl.RAMJobStore' - which does not support persistence. and is not clustered.

 INFO [2020-01-22 17:50:04,582] ({main} StdSchedulerFactory.java[instantiate]:1339) - Quartz scheduler 'DefaultQuartzScheduler' initialized from default resource file in Quartz package: 'quartz.properties'
 INFO [2020-01-22 17:50:04,582] ({main} StdSchedulerFactory.java[instantiate]:1343) - Quartz scheduler version: 2.2.1
 INFO [2020-01-22 17:50:04,584] ({main} QuartzScheduler.java[start]:575) - Scheduler DefaultQuartzScheduler_$_NON_CLUSTERED started.
 INFO [2020-01-22 17:50:05,067] ({main} FolderView.java[createFolder]:107) - Create folder /
 INFO [2020-01-22 17:50:05,072] ({main} Folder.java[setParent]:169) - Set parent of / to /
 INFO [2020-01-22 17:50:05,082] ({main} Folder.java[addNote]:185) - Add note 2EXBGWKU1 to folder /
 WARN [2020-01-22 17:50:05,086] ({main} Notebook.java[refreshCron]:981) - execution of the cron job is skipped cron is not enabled from Zeppelin server
 INFO [2020-01-22 17:50:05,199] ({main} Folder.java[addNote]:185) - Add note 2EXQEGYMG to folder /
 WARN [2020-01-22 17:50:05,200] ({main} Notebook.java[refreshCron]:981) - execution of the cron job is skipped cron is not enabled from Zeppelin server
 INFO [2020-01-22 17:50:05,204] ({main} Notebook.java[<init>]:127) - Notebook indexing started...
 INFO [2020-01-22 17:50:05,995] ({main} LuceneSearch.java[addIndexDocs]:305) - Indexing 2 notebooks took 788ms
 INFO [2020-01-22 17:50:06,002] ({main} Notebook.java[<init>]:129) - Notebook indexing finished: 2 indexed in 0s
 INFO [2020-01-22 17:50:06,008] ({main} Helium.java[loadConf]:103) - Add helium local registry /zeppelin/helium
 INFO [2020-01-22 17:50:06,012] ({main} Helium.java[loadConf]:100) - Add helium online registry https://s3.amazonaws.com/helium-package/helium.json
 WARN [2020-01-22 17:50:06,021] ({main} Helium.java[loadConf]:111) - /zeppelin/conf/helium.json does not exists
 INFO [2020-01-22 17:50:12,923] ({main} ContextHandler.java[doStart]:855) - Started o.e.j.w.WebAppContext@5af97850{zeppelin-web,/,file:///zeppelin/webapps/webapp/,AVAILABLE}{/zeppelin/zeppelin-web-0.8.2.war}
 INFO [2020-01-22 17:50:13,008] ({main} AbstractConnector.java[doStart]:292) - Started ServerConnector@58472096{HTTP/1.1,[http/1.1]}{0.0.0.0:8080}
 INFO [2020-01-22 17:50:13,009] ({main} Server.java[doStart]:407) - Started @32413ms
 INFO [2020-01-22 17:50:13,009] ({main} ZeppelinServer.java[main]:249) - Done, zeppelin server started
 WARN [2020-01-22 17:51:00,478] ({qtp2107447833-9} SecurityRestApi.java[ticket]:88) - {"status":"OK","message":"","body":{"principal":"anonymous","ticket":"anonymous","roles":"[]"}}
 INFO [2020-01-22 17:51:00,607] ({qtp2107447833-12} NotebookServer.java[onOpen]:151) - New connection from 192.168.99.1 : 15924
 WARN [2020-01-22 17:51:00,652] ({qtp2107447833-16} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:51:00,661] ({qtp2107447833-16} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:51:00,664] ({qtp2107447833-16} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:51:00,666] ({qtp2107447833-16} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:51:00,669] ({qtp2107447833-16} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:51:00,673] ({qtp2107447833-16} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:51:14,281] ({qtp2107447833-13} SecurityRestApi.java[ticket]:88) - {"status":"OK","message":"","body":{"principal":"anonymous","ticket":"anonymous","roles":"[]"}}
 INFO [2020-01-22 17:51:14,315] ({qtp2107447833-12} NotebookServer.java[onOpen]:151) - New connection from 192.168.99.1 : 15932
 INFO [2020-01-22 17:51:15,978] ({qtp2107447833-15} NotebookServer.java[sendNote]:828) - New operation from 192.168.99.1 : 15932 : anonymous : GET_NOTE : 2EXQEGYMG
 WARN [2020-01-22 17:51:16,094] ({qtp2107447833-15} GitNotebookRepo.java[revisionHistory]:158) - No Head found for 2EXQEGYMG, No HEAD exists and no explicit starting revision was specified
 WARN [2020-01-22 17:51:16,096] ({qtp2107447833-15} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:51:16,098] ({qtp2107447833-15} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:51:16,098] ({qtp2107447833-15} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:51:16,099] ({qtp2107447833-15} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:51:16,099] ({qtp2107447833-15} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 17:51:16,099] ({qtp2107447833-15} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 INFO [2020-01-22 17:51:16,124] ({qtp2107447833-12} InterpreterSetting.java[getOrCreateInterpreterGroup]:419) - Create InterpreterGroup with groupId: spark:shared_process for user: anonymous and note: 2EXQEGYMG
 INFO [2020-01-22 17:51:16,134] ({qtp2107447833-12} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.SparkInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 17:51:16,135] ({qtp2107447833-12} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.SparkSqlInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 17:51:16,135] ({qtp2107447833-12} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.DepInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 17:51:16,136] ({qtp2107447833-12} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.PySparkInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 17:51:16,136] ({qtp2107447833-12} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.IPySparkInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 17:51:16,137] ({qtp2107447833-12} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.SparkRInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 17:51:16,138] ({qtp2107447833-12} ManagedInterpreterGroup.java[getOrCreateSession]:158) - Create Session: shared_session in InterpreterGroup: spark:shared_process for user: anonymous
 INFO [2020-01-22 17:51:22,002] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:51:22,056] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200121-185651_2144996466 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:51:22,060] ({pool-2-thread-2} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-185651_2144996466, interpreter: dep, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 17:51:22,060] ({pool-2-thread-2} ManagedInterpreterGroup.java[getOrCreateInterpreterProcess]:61) - Create InterpreterProcess for InterpreterGroup: spark:shared_process
 INFO [2020-01-22 17:51:22,064] ({pool-2-thread-2} ShellScriptLauncher.java[launch]:48) - Launching Interpreter: spark
 INFO [2020-01-22 17:51:22,072] ({pool-2-thread-2} SparkInterpreterLauncher.java[buildEnvFromProperties]:108) - Run Spark under non-secure mode as no keytab and principal is specified
 INFO [2020-01-22 17:51:22,077] ({pool-2-thread-2} RemoteInterpreterManagedProcess.java[start]:115) - Thrift server for callback will start. Port: 42203
 INFO [2020-01-22 17:51:22,590] ({pool-2-thread-2} RemoteInterpreterManagedProcess.java[start]:190) - Run interpreter process [/zeppelin/bin/interpreter.sh, -d, /zeppelin/interpreter/spark, -c, 172.18.0.3, -p, 42203, -r, :, -l, /usr/local/local-repo/spark, -g, spark]
 INFO [2020-01-22 17:51:25,241] ({pool-8-thread-1} RemoteInterpreterManagedProcess.java[callback]:123) - RemoteInterpreterServer Registered: CallbackInfo(host:172.18.0.3, port:34511)
 INFO [2020-01-22 17:51:25,300] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkInterpreter
 INFO [2020-01-22 17:51:25,408] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkSqlInterpreter
 INFO [2020-01-22 17:51:25,418] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.DepInterpreter
 INFO [2020-01-22 17:51:25,425] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.PySparkInterpreter
 INFO [2020-01-22 17:51:25,432] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.IPySparkInterpreter
 INFO [2020-01-22 17:51:25,436] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkRInterpreter
 INFO [2020-01-22 17:51:25,442] ({pool-2-thread-2} RemoteInterpreter.java[call]:142) - Open RemoteInterpreter org.apache.zeppelin.spark.DepInterpreter
 INFO [2020-01-22 17:51:25,443] ({pool-2-thread-2} RemoteInterpreter.java[pushAngularObjectRegistryToRemote]:436) - Push local angular object registry from ZeppelinServer to remote interpreter group spark:shared_process
 INFO [2020-01-22 17:51:32,931] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-185651_2144996466 is finished successfully, status: FINISHED
 INFO [2020-01-22 17:51:33,048] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:51:33,091] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200121-185651_2144996466 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:51:55,061] ({qtp2107447833-60} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:51:55,086] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200121-183200_1639671760 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:51:55,086] ({pool-2-thread-2} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-183200_1639671760, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 17:51:55,088] ({pool-2-thread-2} RemoteInterpreter.java[call]:142) - Open RemoteInterpreter org.apache.zeppelin.spark.PySparkInterpreter
 INFO [2020-01-22 17:52:10,588] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-183200_1639671760 is finished successfully, status: FINISHED
 INFO [2020-01-22 17:52:10,625] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:52:10,663] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200121-183200_1639671760 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:52:48,173] ({qtp2107447833-60} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:52:48,197] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20200121-183236_977276561 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:52:48,197] ({pool-2-thread-3} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-183236_977276561, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 17:52:50,350] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:52:50,389] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200122-174128_1762695518 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:52:50,391] ({pool-2-thread-2} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-174128_1762695518, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 17:53:01,471] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-183236_977276561 is finished successfully, status: FINISHED
 INFO [2020-01-22 17:53:01,535] ({pool-2-thread-3} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:53:01,567] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20200121-183236_977276561 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:53:01,816] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-174128_1762695518 is finished successfully, status: FINISHED
 INFO [2020-01-22 17:53:01,893] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:53:01,923] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200122-174128_1762695518 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:54:32,151] ({qtp2107447833-59} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:54:32,176] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 17:54:32,177] ({pool-2-thread-4} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 17:54:37,968] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-173241_479707786 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mPy4JJavaError[0mTraceback (most recent call last)
[0;32m<ipython-input-10-e4874b15b491>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdf[0m[0;34m.[0m[0mselect[0m[0;34m([0m[0msourceExtractor[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m,[0m[0mdateOnlyExtractor[0m[0;34m([0m[0;34m"created_at"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"created_at"[0m[0;34m)[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mgroupBy[0m[0;34m([0m[0;34m"source"[0m[0;34m,[0m[0;34m"created_at"[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mcount[0m[0;34m([0m[0;34m)[0m[0;34m.[0m[0mshow[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py[0m in [0;36mshow[0;34m(self, n, truncate, vertical)[0m
[1;32m    376[0m         """
[1;32m    377[0m         [0;32mif[0m [0misinstance[0m[0;34m([0m[0mtruncate[0m[0;34m,[0m [0mbool[0m[0;34m)[0m [0;32mand[0m [0mtruncate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 378[0;31m             [0;32mprint[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mshowString[0m[0;34m([0m[0mn[0m[0;34m,[0m [0;36m20[0m[0;34m,[0m [0mvertical[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    379[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    380[0m             [0;32mprint[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mshowString[0m[0;34m([0m[0mn[0m[0;34m,[0m [0mint[0m[0;34m([0m[0mtruncate[0m[0;34m)[0m[0;34m,[0m [0mvertical[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     61[0m     [0;32mdef[0m [0mdeco[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     62[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 63[0;31m             [0;32mreturn[0m [0mf[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     64[0m         [0;32mexcept[0m [0mpy4j[0m[0;34m.[0m[0mprotocol[0m[0;34m.[0m[0mPy4JJavaError[0m [0;32mas[0m [0me[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     65[0m             [0ms[0m [0;34m=[0m [0me[0m[0;34m.[0m[0mjava_exception[0m[0;34m.[0m[0mtoString[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py[0m in [0;36mget_return_value[0;34m(answer, gateway_client, target_id, name)[0m
[1;32m    326[0m                 raise Py4JJavaError(
[1;32m    327[0m                     [0;34m"An error occurred while calling {0}{1}{2}.\n"[0m[0;34m.[0m[0;34m[0m[0m
[0;32m--> 328[0;31m                     format(target_id, ".", name), value)
[0m[1;32m    329[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    330[0m                 raise Py4JError(

[0;31mPy4JJavaError[0m: An error occurred while calling o109.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 4, 172.18.0.4, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 361, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 236, in read_udfs
    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type, runner_conf)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 163, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 64, in read_command
    command = serializer._read_with_length(file)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 580, in loads
    return pickle.loads(obj)
ImportError: No module named bs4

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2759)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:292)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 361, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 236, in read_udfs
    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type, runner_conf)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 163, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 64, in read_command
    command = serializer._read_with_length(file)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 580, in loads
    return pickle.loads(obj)
ImportError: No module named bs4

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

 INFO [2020-01-22 17:54:38,012] ({pool-2-thread-4} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 17:54:38,087] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:03:21,371] ({qtp2107447833-60} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:23:54,069] ({qtp2107447833-59} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:24:02,299] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:24:07,072] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:24:07,103] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20200121-183200_1639671760 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:24:07,104] ({pool-2-thread-5} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-183200_1639671760, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 18:24:07,318] ({pool-2-thread-5} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-183200_1639671760 is finished successfully, status: FINISHED
 INFO [2020-01-22 18:24:07,354] ({pool-2-thread-5} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:24:07,388] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20200121-183200_1639671760 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:24:10,192] ({qtp2107447833-59} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:24:10,228] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20200121-183236_977276561 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:24:10,228] ({pool-2-thread-3} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-183236_977276561, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 18:24:12,831] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-183236_977276561 is finished successfully, status: FINISHED
 INFO [2020-01-22 18:24:12,862] ({pool-2-thread-3} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:24:12,897] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20200121-183236_977276561 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:24:19,586] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:24:19,609] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200121-183200_1639671760 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:24:19,610] ({pool-2-thread-2} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-183200_1639671760, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 18:24:19,701] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-183200_1639671760 is finished successfully, status: FINISHED
 INFO [2020-01-22 18:24:19,719] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:24:19,744] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200121-183200_1639671760 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:24:21,039] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:24:21,070] ({pool-2-thread-6} SchedulerFactory.java[jobStarted]:114) - Job 20200121-183200_1639671760 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:24:21,071] ({pool-2-thread-6} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-183200_1639671760, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 18:24:21,159] ({pool-2-thread-6} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-183200_1639671760 is finished successfully, status: FINISHED
 INFO [2020-01-22 18:24:21,173] ({pool-2-thread-6} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:24:21,191] ({pool-2-thread-6} SchedulerFactory.java[jobFinished]:120) - Job 20200121-183200_1639671760 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:24:22,498] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:24:22,515] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200121-183236_977276561 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:24:22,516] ({pool-2-thread-4} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-183236_977276561, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 18:24:24,870] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-183236_977276561 is finished successfully, status: FINISHED
 INFO [2020-01-22 18:24:24,901] ({pool-2-thread-4} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:24:24,931] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200121-183236_977276561 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:24:26,041] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:24:26,061] ({pool-2-thread-12} SchedulerFactory.java[jobStarted]:114) - Job 20200122-174128_1762695518 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:24:26,062] ({pool-2-thread-12} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-174128_1762695518, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 18:24:26,141] ({pool-2-thread-12} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-174128_1762695518 is finished successfully, status: FINISHED
 INFO [2020-01-22 18:24:26,158] ({pool-2-thread-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:24:26,176] ({pool-2-thread-12} SchedulerFactory.java[jobFinished]:120) - Job 20200122-174128_1762695518 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:24:28,343] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:24:28,385] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:24:28,385] ({pool-2-thread-5} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 18:24:31,803] ({pool-2-thread-5} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-173241_479707786 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mPy4JJavaError[0mTraceback (most recent call last)
[0;32m<ipython-input-24-e4874b15b491>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdf[0m[0;34m.[0m[0mselect[0m[0;34m([0m[0msourceExtractor[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m,[0m[0mdateOnlyExtractor[0m[0;34m([0m[0;34m"created_at"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"created_at"[0m[0;34m)[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mgroupBy[0m[0;34m([0m[0;34m"source"[0m[0;34m,[0m[0;34m"created_at"[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mcount[0m[0;34m([0m[0;34m)[0m[0;34m.[0m[0mshow[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py[0m in [0;36mshow[0;34m(self, n, truncate, vertical)[0m
[1;32m    376[0m         """
[1;32m    377[0m         [0;32mif[0m [0misinstance[0m[0;34m([0m[0mtruncate[0m[0;34m,[0m [0mbool[0m[0;34m)[0m [0;32mand[0m [0mtruncate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 378[0;31m             [0;32mprint[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mshowString[0m[0;34m([0m[0mn[0m[0;34m,[0m [0;36m20[0m[0;34m,[0m [0mvertical[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    379[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    380[0m             [0;32mprint[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mshowString[0m[0;34m([0m[0mn[0m[0;34m,[0m [0mint[0m[0;34m([0m[0mtruncate[0m[0;34m)[0m[0;34m,[0m [0mvertical[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     61[0m     [0;32mdef[0m [0mdeco[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     62[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 63[0;31m             [0;32mreturn[0m [0mf[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     64[0m         [0;32mexcept[0m [0mpy4j[0m[0;34m.[0m[0mprotocol[0m[0;34m.[0m[0mPy4JJavaError[0m [0;32mas[0m [0me[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     65[0m             [0ms[0m [0;34m=[0m [0me[0m[0;34m.[0m[0mjava_exception[0m[0;34m.[0m[0mtoString[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py[0m in [0;36mget_return_value[0;34m(answer, gateway_client, target_id, name)[0m
[1;32m    326[0m                 raise Py4JJavaError(
[1;32m    327[0m                     [0;34m"An error occurred while calling {0}{1}{2}.\n"[0m[0;34m.[0m[0;34m[0m[0m
[0;32m--> 328[0;31m                     format(target_id, ".", name), value)
[0m[1;32m    329[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    330[0m                 raise Py4JError(

[0;31mPy4JJavaError[0m: An error occurred while calling o249.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 10, 172.18.0.4, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 361, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 236, in read_udfs
    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type, runner_conf)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 163, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 64, in read_command
    command = serializer._read_with_length(file)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 580, in loads
    return pickle.loads(obj)
ImportError: No module named bs4

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2759)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:292)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 361, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 236, in read_udfs
    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type, runner_conf)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 163, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 64, in read_command
    command = serializer._read_with_length(file)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 172, in _read_with_length
    return self.loads(obj)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 580, in loads
    return pickle.loads(obj)
ImportError: No module named bs4

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

 INFO [2020-01-22 18:24:31,824] ({pool-2-thread-5} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:24:31,848] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:28:33,161] ({Thread-34} ZeppelinServer.java[run]:253) - Shutting down Zeppelin Server ... 
 INFO [2020-01-22 18:28:33,167] ({qtp2107447833-15} NotebookServer.java[onClose]:372) - Closed connection to 192.168.99.1 : 15924. (1000) null
 INFO [2020-01-22 18:28:33,172] ({qtp2107447833-13} NotebookServer.java[onClose]:372) - Closed connection to 192.168.99.1 : 15932. (1000) null
 INFO [2020-01-22 18:28:33,173] ({Thread-34} AbstractConnector.java[doStop]:341) - Stopped ServerConnector@58472096{HTTP/1.1,[http/1.1]}{0.0.0.0:8080}
 INFO [2020-01-22 18:28:33,174] ({Thread-34} HouseKeeper.java[stopScavenging]:167) - node0 Stopped scavenging
 INFO [2020-01-22 18:28:34,406] ({Thread-34} ContextHandler.java[doStop]:1045) - Stopped o.e.j.w.WebAppContext@5af97850{zeppelin-web,/,null,UNAVAILABLE}{/zeppelin/zeppelin-web-0.8.2.war}
 INFO [2020-01-22 18:28:34,409] ({Thread-79} InterpreterSetting.java[close]:483) - Close InterpreterSetting: ignite
 INFO [2020-01-22 18:28:34,409] ({Thread-80} InterpreterSetting.java[close]:483) - Close InterpreterSetting: python
 INFO [2020-01-22 18:28:34,410] ({Thread-82} InterpreterSetting.java[close]:483) - Close InterpreterSetting: jdbc
 INFO [2020-01-22 18:28:34,410] ({Thread-83} InterpreterSetting.java[close]:483) - Close InterpreterSetting: lens
 INFO [2020-01-22 18:28:34,410] ({Thread-81} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sap
 INFO [2020-01-22 18:28:34,410] ({Thread-85} InterpreterSetting.java[close]:483) - Close InterpreterSetting: pig
 INFO [2020-01-22 18:28:34,412] ({Thread-88} InterpreterSetting.java[close]:483) - Close InterpreterSetting: livy
 INFO [2020-01-22 18:28:34,412] ({Thread-93} InterpreterSetting.java[close]:483) - Close InterpreterSetting: cassandra
 INFO [2020-01-22 18:28:34,412] ({Thread-92} InterpreterSetting.java[close]:483) - Close InterpreterSetting: elasticsearch
 INFO [2020-01-22 18:28:34,412] ({Thread-90} InterpreterSetting.java[close]:483) - Close InterpreterSetting: file
 INFO [2020-01-22 18:28:34,412] ({Thread-91} InterpreterSetting.java[close]:483) - Close InterpreterSetting: groovy
 INFO [2020-01-22 18:28:34,412] ({Thread-89} InterpreterSetting.java[close]:483) - Close InterpreterSetting: neo4j
 INFO [2020-01-22 18:28:34,412] ({Thread-87} InterpreterSetting.java[close]:483) - Close InterpreterSetting: angular
 INFO [2020-01-22 18:28:34,413] ({Thread-95} InterpreterSetting.java[close]:483) - Close InterpreterSetting: spark
 INFO [2020-01-22 18:28:34,414] ({Thread-95} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: spark:shared_process
 INFO [2020-01-22 18:28:34,414] ({Thread-95} ManagedInterpreterGroup.java[close]:100) - Close Session: shared_session for interpreter setting: spark
 INFO [2020-01-22 18:28:34,415] ({Thread-99} InterpreterSetting.java[close]:483) - Close InterpreterSetting: hbase
 INFO [2020-01-22 18:28:34,413] ({Thread-94} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sh
 INFO [2020-01-22 18:28:34,415] ({Thread-98} InterpreterSetting.java[close]:483) - Close InterpreterSetting: bigquery
 INFO [2020-01-22 18:28:34,415] ({Thread-100} InterpreterSetting.java[close]:483) - Close InterpreterSetting: kylin
 INFO [2020-01-22 18:28:34,413] ({Thread-86} InterpreterSetting.java[close]:483) - Close InterpreterSetting: flink
 INFO [2020-01-22 18:28:34,414] ({Thread-96} InterpreterSetting.java[close]:483) - Close InterpreterSetting: md
 INFO [2020-01-22 18:28:34,413] ({Thread-97} InterpreterSetting.java[close]:483) - Close InterpreterSetting: alluxio
 INFO [2020-01-22 18:28:34,416] ({Thread-101} InterpreterSetting.java[close]:483) - Close InterpreterSetting: python
 INFO [2020-01-22 18:28:34,415] ({Thread-84} InterpreterSetting.java[close]:483) - Close InterpreterSetting: ignite
 INFO [2020-01-22 18:28:34,416] ({Thread-103} InterpreterSetting.java[close]:483) - Close InterpreterSetting: jdbc
 INFO [2020-01-22 18:28:34,417] ({Thread-102} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sap
 INFO [2020-01-22 18:28:34,417] ({Thread-105} InterpreterSetting.java[close]:483) - Close InterpreterSetting: pig
 INFO [2020-01-22 18:28:34,417] ({Thread-104} InterpreterSetting.java[close]:483) - Close InterpreterSetting: lens
 INFO [2020-01-22 18:28:34,417] ({Thread-106} InterpreterSetting.java[close]:483) - Close InterpreterSetting: flink
 INFO [2020-01-22 18:28:34,418] ({Thread-108} InterpreterSetting.java[close]:483) - Close InterpreterSetting: livy
 INFO [2020-01-22 18:28:34,418] ({Thread-107} InterpreterSetting.java[close]:483) - Close InterpreterSetting: angular
 INFO [2020-01-22 18:28:34,418] ({Thread-110} InterpreterSetting.java[close]:483) - Close InterpreterSetting: file
 INFO [2020-01-22 18:28:34,418] ({Thread-109} InterpreterSetting.java[close]:483) - Close InterpreterSetting: neo4j
 INFO [2020-01-22 18:28:34,419] ({Thread-112} InterpreterSetting.java[close]:483) - Close InterpreterSetting: elasticsearch
 INFO [2020-01-22 18:28:34,419] ({Thread-111} InterpreterSetting.java[close]:483) - Close InterpreterSetting: groovy
 INFO [2020-01-22 18:28:34,420] ({Thread-113} InterpreterSetting.java[close]:483) - Close InterpreterSetting: cassandra
 INFO [2020-01-22 18:28:34,423] ({Thread-114} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sh
 INFO [2020-01-22 18:28:34,425] ({Thread-115} InterpreterSetting.java[close]:483) - Close InterpreterSetting: spark
 INFO [2020-01-22 18:28:34,427] ({Thread-117} InterpreterSetting.java[close]:483) - Close InterpreterSetting: alluxio
 INFO [2020-01-22 18:28:34,428] ({Thread-116} InterpreterSetting.java[close]:483) - Close InterpreterSetting: md
 INFO [2020-01-22 18:28:34,428] ({Thread-119} InterpreterSetting.java[close]:483) - Close InterpreterSetting: hbase
 INFO [2020-01-22 18:28:34,428] ({Thread-118} InterpreterSetting.java[close]:483) - Close InterpreterSetting: bigquery
 WARN [2020-01-22 18:28:34,415] ({Thread-95} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.SparkInterpreter
 WARN [2020-01-22 18:28:34,431] ({Thread-95} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.SparkSqlInterpreter
 INFO [2020-01-22 18:28:34,437] ({Thread-120} InterpreterSetting.java[close]:483) - Close InterpreterSetting: kylin
 WARN [2020-01-22 18:28:34,720] ({Thread-95} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.IPySparkInterpreter
 WARN [2020-01-22 18:28:34,722] ({Thread-95} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.SparkRInterpreter
 INFO [2020-01-22 18:28:34,723] ({Thread-95} ManagedInterpreterGroup.java[close]:105) - Remove this InterpreterGroup: spark:shared_process as all the sessions are closed
 INFO [2020-01-22 18:28:34,724] ({Thread-95} ManagedInterpreterGroup.java[close]:108) - Kill RemoteInterpreterProcess
 INFO [2020-01-22 18:28:34,724] ({Thread-95} RemoteInterpreterManagedProcess.java[stop]:220) - Kill interpreter process
ERROR [2020-01-22 18:28:35,042] ({Thread-39} RemoteInterpreterEventPoller.java[run]:257) - Can not get RemoteInterpreterEvent because it is shutdown.
ERROR [2020-01-22 18:28:35,044] ({pool-7-thread-1} AppendOutputRunner.java[run]:68) - Wait for OutputBuffer queue interrupted: null
 WARN [2020-01-22 18:28:37,251] ({Thread-95} RemoteInterpreterManagedProcess.java[stop]:230) - ignore the exception when shutting down
 INFO [2020-01-22 18:28:37,253] ({Thread-95} RemoteInterpreterManagedProcess.java[stop]:238) - Remote process terminated
 INFO [2020-01-22 18:28:37,254] ({Thread-115} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: spark:shared_process
 INFO [2020-01-22 18:28:37,254] ({Thread-34} NotebookRepoSync.java[close]:428) - Closing all notebook storages
 INFO [2020-01-22 18:28:37,258] ({Exec Default Executor} RemoteInterpreterManagedProcess.java[onProcessFailed]:250) - Interpreter process failed {}
org.apache.commons.exec.ExecuteException: Process exited with an error: 143 (Exit value: 143)
	at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:404)
	at org.apache.commons.exec.DefaultExecutor.access$200(DefaultExecutor.java:48)
	at org.apache.commons.exec.DefaultExecutor$1.run(DefaultExecutor.java:200)
	at java.lang.Thread.run(Thread.java:748)
 INFO [2020-01-22 18:28:40,266] ({Thread-34} ZeppelinServer.java[run]:264) - Bye
 WARN [2020-01-22 18:31:51,879] ({main} ZeppelinConfiguration.java[create]:117) - Failed to load configuration, proceeding with a default
 INFO [2020-01-22 18:31:52,085] ({main} ZeppelinConfiguration.java[create]:129) - Server Host: 0.0.0.0
 INFO [2020-01-22 18:31:52,091] ({main} ZeppelinConfiguration.java[create]:131) - Server Port: 8080
 INFO [2020-01-22 18:31:52,091] ({main} ZeppelinConfiguration.java[create]:135) - Context Path: /
 INFO [2020-01-22 18:31:52,114] ({main} ZeppelinConfiguration.java[create]:136) - Zeppelin Version: 0.8.2
 INFO [2020-01-22 18:31:52,311] ({main} Log.java[initialized]:193) - Logging initialized @2724ms to org.eclipse.jetty.util.log.Slf4jLog
 WARN [2020-01-22 18:31:53,174] ({main} ServerConnector.java[setSoLingerTime]:458) - Ignoring deprecated socket close linger time
 INFO [2020-01-22 18:31:53,571] ({main} ZeppelinServer.java[setupWebAppContext]:413) - ZeppelinServer Webapp path: /zeppelin/webapps
 INFO [2020-01-22 18:31:54,369] ({main} ZeppelinServer.java[main]:239) - Starting zeppelin server
 INFO [2020-01-22 18:31:54,381] ({main} Server.java[doStart]:370) - jetty-9.4.14.v20181114; built: 2018-11-14T21:20:31.478Z; git: c4550056e785fb5665914545889f21dc136ad9e6; jvm 1.8.0_222-8u222-b10-1ubuntu1~16.04.1-b10
 INFO [2020-01-22 18:32:08,117] ({main} StandardDescriptorProcessor.java[visitServlet]:283) - NO JSP Support for /, did not find org.eclipse.jetty.jsp.JettyJspServlet
 INFO [2020-01-22 18:32:08,255] ({main} DefaultSessionIdManager.java[doStart]:365) - DefaultSessionIdManager workerName=node0
 INFO [2020-01-22 18:32:08,256] ({main} DefaultSessionIdManager.java[doStart]:370) - No SessionScavenger set, using defaults
 INFO [2020-01-22 18:32:08,281] ({main} HouseKeeper.java[startScavenging]:149) - node0 Scavenging every 600000ms
 INFO [2020-01-22 18:32:10,694] ({main} SchedulerFactory.java[<init>]:59) - Scheduler Thread Pool Size: 100
 WARN [2020-01-22 18:32:10,756] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir
 WARN [2020-01-22 18:32:10,770] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir
 INFO [2020-01-22 18:32:11,185] ({main} InterpreterSettingManager.java[<init>]:165) - Using RecoveryStorage: org.apache.zeppelin.interpreter.recovery.NullRecoveryStorage
 INFO [2020-01-22 18:32:11,198] ({main} InterpreterSettingManager.java[<init>]:169) - Using LifecycleManager: org.apache.zeppelin.interpreter.lifecycle.NullLifecycleManager
 INFO [2020-01-22 18:32:11,629] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: cassandra
 INFO [2020-01-22 18:32:11,709] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: md
 INFO [2020-01-22 18:32:11,755] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: sap
 INFO [2020-01-22 18:32:11,819] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: sh
 INFO [2020-01-22 18:32:11,854] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: angular
 WARN [2020-01-22 18:32:11,918] ({main} InterpreterSettingManager.java[init]:331) - No interpreter-setting.json found in /zeppelin/interpreter/lib
 INFO [2020-01-22 18:32:11,964] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: hbase
 INFO [2020-01-22 18:32:12,001] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: elasticsearch
 WARN [2020-01-22 18:32:12,476] ({main} InterpreterSettingManager.java[init]:331) - No interpreter-setting.json found in /zeppelin/interpreter/scio
 INFO [2020-01-22 18:32:12,535] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: alluxio
 INFO [2020-01-22 18:32:12,576] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: jdbc
 INFO [2020-01-22 18:32:12,609] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: neo4j
 INFO [2020-01-22 18:32:12,637] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: python
 INFO [2020-01-22 18:32:12,663] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: pig
 INFO [2020-01-22 18:32:12,696] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: spark
 INFO [2020-01-22 18:32:12,717] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: lens
 INFO [2020-01-22 18:32:12,744] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: bigquery
 INFO [2020-01-22 18:32:12,765] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: file
 INFO [2020-01-22 18:32:12,790] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: ignite
 INFO [2020-01-22 18:32:12,818] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: kylin
 INFO [2020-01-22 18:32:12,845] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: livy
 INFO [2020-01-22 18:32:12,868] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: groovy
 INFO [2020-01-22 18:32:12,893] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: flink
 WARN [2020-01-22 18:32:12,904] ({main} InterpreterSettingManager.java[init]:331) - No interpreter-setting.json found in /zeppelin/interpreter/${interpreter.name}
 WARN [2020-01-22 18:32:12,910] ({main} LocalConfigStorage.java[loadInterpreterSettings]:60) - Interpreter Setting file /zeppelin/conf/interpreter.json is not existed
 INFO [2020-01-22 18:32:13,177] ({main} LocalConfigStorage.java[save]:53) - Save Interpreter Setting to /zeppelin/conf/interpreter.json
 INFO [2020-01-22 18:32:13,930] ({main} VfsLog.java[info]:138) - Using "/tmp/vfs_cache" as temporary files store.
 INFO [2020-01-22 18:32:14,093] ({main} GitNotebookRepo.java[<init>]:64) - Opening a git repo at '/zeppelin/notebook'
 INFO [2020-01-22 18:32:14,494] ({main} NotebookRepoSync.java[<init>]:77) - Instantiate NotebookRepo: org.apache.zeppelin.notebook.repo.GitNotebookRepo
 WARN [2020-01-22 18:32:14,708] ({main} NotebookAuthorization.java[getInstance]:86) - Notebook authorization module was called without initialization, initializing with default configuration
 WARN [2020-01-22 18:32:14,715] ({main} LocalConfigStorage.java[loadNotebookAuthorization]:77) - NotebookAuthorization file /zeppelin/conf/notebook-authorization.json is not existed
 INFO [2020-01-22 18:32:14,717] ({main} Credentials.java[loadFromFile]:121) - /zeppelin/conf/credentials.json
 INFO [2020-01-22 18:32:14,793] ({main} StdSchedulerFactory.java[instantiate]:1184) - Using default implementation for ThreadExecutor
 INFO [2020-01-22 18:32:14,799] ({main} SimpleThreadPool.java[initialize]:268) - Job execution threads will use class loader of thread: main
 INFO [2020-01-22 18:32:14,868] ({main} SchedulerSignalerImpl.java[<init>]:61) - Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
 INFO [2020-01-22 18:32:14,890] ({main} QuartzScheduler.java[<init>]:240) - Quartz Scheduler v.2.2.1 created.
 INFO [2020-01-22 18:32:14,892] ({main} RAMJobStore.java[initialize]:155) - RAMJobStore initialized.
 INFO [2020-01-22 18:32:14,893] ({main} QuartzScheduler.java[initialize]:305) - Scheduler meta-data: Quartz Scheduler (v2.2.1) 'DefaultQuartzScheduler' with instanceId 'NON_CLUSTERED'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'org.quartz.simpl.SimpleThreadPool' - with 10 threads.
  Using job-store 'org.quartz.simpl.RAMJobStore' - which does not support persistence. and is not clustered.

 INFO [2020-01-22 18:32:14,893] ({main} StdSchedulerFactory.java[instantiate]:1339) - Quartz scheduler 'DefaultQuartzScheduler' initialized from default resource file in Quartz package: 'quartz.properties'
 INFO [2020-01-22 18:32:14,894] ({main} StdSchedulerFactory.java[instantiate]:1343) - Quartz scheduler version: 2.2.1
 INFO [2020-01-22 18:32:14,895] ({main} QuartzScheduler.java[start]:575) - Scheduler DefaultQuartzScheduler_$_NON_CLUSTERED started.
 INFO [2020-01-22 18:32:15,394] ({main} FolderView.java[createFolder]:107) - Create folder /
 INFO [2020-01-22 18:32:15,398] ({main} Folder.java[setParent]:169) - Set parent of / to /
 INFO [2020-01-22 18:32:15,403] ({main} Folder.java[addNote]:185) - Add note 2EXBGWKU1 to folder /
 WARN [2020-01-22 18:32:15,406] ({main} Notebook.java[refreshCron]:981) - execution of the cron job is skipped cron is not enabled from Zeppelin server
 INFO [2020-01-22 18:32:15,519] ({main} Folder.java[addNote]:185) - Add note 2EXQEGYMG to folder /
 WARN [2020-01-22 18:32:15,526] ({main} Notebook.java[refreshCron]:981) - execution of the cron job is skipped cron is not enabled from Zeppelin server
 INFO [2020-01-22 18:32:15,527] ({main} Notebook.java[<init>]:127) - Notebook indexing started...
 INFO [2020-01-22 18:32:16,472] ({main} LuceneSearch.java[addIndexDocs]:305) - Indexing 2 notebooks took 937ms
 INFO [2020-01-22 18:32:16,480] ({main} Notebook.java[<init>]:129) - Notebook indexing finished: 2 indexed in 0s
 INFO [2020-01-22 18:32:16,486] ({main} Helium.java[loadConf]:103) - Add helium local registry /zeppelin/helium
 INFO [2020-01-22 18:32:16,493] ({main} Helium.java[loadConf]:100) - Add helium online registry https://s3.amazonaws.com/helium-package/helium.json
 WARN [2020-01-22 18:32:16,506] ({main} Helium.java[loadConf]:111) - /zeppelin/conf/helium.json does not exists
 INFO [2020-01-22 18:32:24,077] ({main} ContextHandler.java[doStart]:855) - Started o.e.j.w.WebAppContext@5af97850{zeppelin-web,/,file:///zeppelin/webapps/webapp/,AVAILABLE}{/zeppelin/zeppelin-web-0.8.2.war}
 INFO [2020-01-22 18:32:24,136] ({main} AbstractConnector.java[doStart]:292) - Started ServerConnector@49faf066{HTTP/1.1,[http/1.1]}{0.0.0.0:8080}
 INFO [2020-01-22 18:32:24,138] ({main} Server.java[doStart]:407) - Started @34581ms
 INFO [2020-01-22 18:32:24,140] ({main} ZeppelinServer.java[main]:249) - Done, zeppelin server started
 WARN [2020-01-22 18:35:16,038] ({qtp2107447833-9} SecurityRestApi.java[ticket]:88) - {"status":"OK","message":"","body":{"principal":"anonymous","ticket":"anonymous","roles":"[]"}}
 INFO [2020-01-22 18:35:16,250] ({qtp2107447833-14} NotebookServer.java[onOpen]:151) - New connection from 192.168.99.1 : 16261
 INFO [2020-01-22 18:35:18,042] ({qtp2107447833-13} NotebookServer.java[sendNote]:828) - New operation from 192.168.99.1 : 16261 : anonymous : GET_NOTE : 2EXQEGYMG
 WARN [2020-01-22 18:35:18,159] ({qtp2107447833-13} GitNotebookRepo.java[revisionHistory]:158) - No Head found for 2EXQEGYMG, No HEAD exists and no explicit starting revision was specified
 WARN [2020-01-22 18:35:18,163] ({qtp2107447833-13} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 18:35:18,164] ({qtp2107447833-13} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 18:35:18,165] ({qtp2107447833-13} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 18:35:18,166] ({qtp2107447833-13} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 18:35:18,166] ({qtp2107447833-13} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 18:35:18,167] ({qtp2107447833-13} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 18:35:18,359] ({qtp2107447833-12} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: spark
 WARN [2020-01-22 18:35:18,366] ({qtp2107447833-9} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: spark
 WARN [2020-01-22 18:35:18,628] ({qtp2107447833-12} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: dep
 WARN [2020-01-22 18:35:18,629] ({qtp2107447833-12} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: spark.pyspark
 WARN [2020-01-22 18:35:18,631] ({qtp2107447833-12} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: spark.pyspark
 WARN [2020-01-22 18:35:18,632] ({qtp2107447833-12} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: spark.pyspark
 WARN [2020-01-22 18:35:18,633] ({qtp2107447833-12} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: spark.pyspark
 WARN [2020-01-22 18:35:18,634] ({qtp2107447833-12} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: spark.pyspark
 WARN [2020-01-22 18:35:18,635] ({qtp2107447833-12} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: spark.pyspark
 WARN [2020-01-22 18:35:18,637] ({qtp2107447833-12} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: spark.pyspark
 INFO [2020-01-22 18:35:20,400] ({qtp2107447833-9} LocalConfigStorage.java[save]:53) - Save Interpreter Setting to /zeppelin/conf/interpreter.json
 WARN [2020-01-22 18:35:20,404] ({qtp2107447833-9} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 18:35:20,404] ({qtp2107447833-9} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 18:35:20,411] ({qtp2107447833-9} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 18:35:20,411] ({qtp2107447833-9} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 18:35:20,412] ({qtp2107447833-9} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 18:35:20,412] ({qtp2107447833-9} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 18:35:24,122] ({qtp2107447833-9} SecurityRestApi.java[ticket]:88) - {"status":"OK","message":"","body":{"principal":"anonymous","ticket":"anonymous","roles":"[]"}}
 INFO [2020-01-22 18:35:24,151] ({qtp2107447833-12} NotebookServer.java[onOpen]:151) - New connection from 192.168.99.1 : 16263
 WARN [2020-01-22 18:35:24,227] ({qtp2107447833-12} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 18:35:24,228] ({qtp2107447833-12} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 18:35:24,229] ({qtp2107447833-12} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 18:35:24,230] ({qtp2107447833-12} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 18:35:24,232] ({qtp2107447833-12} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 18:35:24,232] ({qtp2107447833-12} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 INFO [2020-01-22 18:35:43,378] ({qtp2107447833-11} InterpreterRestApi.java[updateSetting]:137) - Update interpreterSetting spark
 INFO [2020-01-22 18:35:43,380] ({qtp2107447833-11} InterpreterSetting.java[close]:483) - Close InterpreterSetting: spark
 INFO [2020-01-22 18:35:43,382] ({qtp2107447833-11} LocalConfigStorage.java[save]:53) - Save Interpreter Setting to /zeppelin/conf/interpreter.json
 INFO [2020-01-22 18:35:45,391] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:35:45,423] ({qtp2107447833-11} InterpreterSetting.java[getOrCreateInterpreterGroup]:419) - Create InterpreterGroup with groupId: spark:shared_process for user: anonymous and note: 2EXQEGYMG
 INFO [2020-01-22 18:35:45,425] ({qtp2107447833-11} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.SparkInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 18:35:45,426] ({qtp2107447833-11} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.SparkSqlInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 18:35:45,426] ({qtp2107447833-11} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.DepInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 18:35:45,427] ({qtp2107447833-11} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.PySparkInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 18:35:45,427] ({qtp2107447833-11} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.IPySparkInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 18:35:45,428] ({qtp2107447833-11} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.SparkRInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 18:35:45,429] ({qtp2107447833-11} ManagedInterpreterGroup.java[getOrCreateSession]:158) - Create Session: shared_session in InterpreterGroup: spark:shared_process for user: anonymous
 INFO [2020-01-22 18:35:45,445] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200121-185651_2144996466 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:35:45,449] ({pool-2-thread-2} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-185651_2144996466, interpreter: dep, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 18:35:45,450] ({pool-2-thread-2} ManagedInterpreterGroup.java[getOrCreateInterpreterProcess]:61) - Create InterpreterProcess for InterpreterGroup: spark:shared_process
 INFO [2020-01-22 18:35:45,451] ({pool-2-thread-2} ShellScriptLauncher.java[launch]:48) - Launching Interpreter: spark
 INFO [2020-01-22 18:35:45,459] ({pool-2-thread-2} SparkInterpreterLauncher.java[buildEnvFromProperties]:108) - Run Spark under non-secure mode as no keytab and principal is specified
 INFO [2020-01-22 18:35:45,465] ({pool-2-thread-2} RemoteInterpreterManagedProcess.java[start]:115) - Thrift server for callback will start. Port: 45433
 INFO [2020-01-22 18:35:45,997] ({pool-2-thread-2} RemoteInterpreterManagedProcess.java[start]:190) - Run interpreter process [/zeppelin/bin/interpreter.sh, -d, /zeppelin/interpreter/spark, -c, 172.19.0.3, -p, 45433, -r, :, -l, /usr/local/local-repo/spark, -g, spark]
 INFO [2020-01-22 18:35:48,845] ({pool-7-thread-1} RemoteInterpreterManagedProcess.java[callback]:123) - RemoteInterpreterServer Registered: CallbackInfo(host:172.19.0.3, port:33465)
 INFO [2020-01-22 18:35:48,901] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkInterpreter
 INFO [2020-01-22 18:35:49,003] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkSqlInterpreter
 INFO [2020-01-22 18:35:49,008] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.DepInterpreter
 INFO [2020-01-22 18:35:49,011] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.PySparkInterpreter
 INFO [2020-01-22 18:35:49,016] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.IPySparkInterpreter
 INFO [2020-01-22 18:35:49,024] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkRInterpreter
 INFO [2020-01-22 18:35:49,027] ({pool-2-thread-2} RemoteInterpreter.java[call]:142) - Open RemoteInterpreter org.apache.zeppelin.spark.DepInterpreter
 INFO [2020-01-22 18:35:49,028] ({pool-2-thread-2} RemoteInterpreter.java[pushAngularObjectRegistryToRemote]:436) - Push local angular object registry from ZeppelinServer to remote interpreter group spark:shared_process
 INFO [2020-01-22 18:39:17,768] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-185651_2144996466 is finished successfully, status: FINISHED
 INFO [2020-01-22 18:39:17,858] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:39:17,895] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200121-185651_2144996466 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:45:40,922] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:45:40,944] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200121-183200_1639671760 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:45:40,946] ({pool-2-thread-2} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-183200_1639671760, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 18:45:40,946] ({pool-2-thread-2} RemoteInterpreter.java[call]:142) - Open RemoteInterpreter org.apache.zeppelin.spark.PySparkInterpreter
 WARN [2020-01-22 18:45:55,291] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2316) - Job 20200121-183200_1639671760 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mPy4JJavaError[0mTraceback (most recent call last)
[0;32m<ipython-input-4-24a72a9f2183>[0m in [0;36m<module>[0;34m()[0m
[1;32m      5[0m [0mconfig[0m [0;34m=[0m [0mpyspark[0m[0;34m.[0m[0mSparkConf[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[1;32m      6[0m [0mconfig[0m[0;34m.[0m[0msetMaster[0m[0;34m([0m[0;34m"spark://spark-master:7077"[0m[0;34m)[0m[0;34m[0m[0m
[0;32m----> 7[0;31m [0msc[0m[0;34m.[0m[0maddPyFile[0m[0;34m([0m[0;34m"/zeppelin/dependencies.zip"[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m      8[0m [0mspark[0m [0;34m=[0m [0mSparkSession[0m[0;34m.[0m[0mbuilder[0m             [0;34m.[0m[0mappName[0m[0;34m([0m[0;34m"J2SchemaTeest"[0m[0;34m)[0m             [0;34m.[0m[0mconfig[0m[0;34m([0m[0;34m"fs.s3a.access.key"[0m[0;34m,[0m[0;34m"AKIA4CC66JB65LVFCDR2"[0m[0;34m)[0m             [0;34m.[0m[0mconfig[0m[0;34m([0m[0;34m"fs.s3a.secret.key"[0m[0;34m,[0m[0;34m"wswPivszEZz0J7MQGj2i9lXCCelGTrim6tSsNH4t"[0m[0;34m)[0m             [0;34m.[0m[0mgetOrCreate[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/context.py[0m in [0;36maddPyFile[0;34m(self, path)[0m
[1;32m    907[0m         [0;34m.[0m[0;34m.[0m [0mnote[0m[0;34m:[0m[0;34m:[0m [0mA[0m [0mpath[0m [0mcan[0m [0mbe[0m [0madded[0m [0monly[0m [0monce[0m[0;34m.[0m [0mSubsequent[0m [0madditions[0m [0mof[0m [0mthe[0m [0msame[0m [0mpath[0m [0mare[0m [0mignored[0m[0;34m.[0m[0;34m[0m[0m
[1;32m    908[0m         """
[0;32m--> 909[0;31m         [0mself[0m[0;34m.[0m[0maddFile[0m[0;34m([0m[0mpath[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    910[0m         [0;34m([0m[0mdirname[0m[0;34m,[0m [0mfilename[0m[0;34m)[0m [0;34m=[0m [0mos[0m[0;34m.[0m[0mpath[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0mpath[0m[0;34m)[0m  [0;31m# dirname may be directory or HDFS/S3 prefix[0m[0;34m[0m[0m
[1;32m    911[0m         [0;32mif[0m [0mfilename[0m[0;34m[[0m[0;34m-[0m[0;36m4[0m[0;34m:[0m[0;34m][0m[0;34m.[0m[0mlower[0m[0;34m([0m[0;34m)[0m [0;32min[0m [0mself[0m[0;34m.[0m[0mPACKAGE_EXTENSIONS[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/context.py[0m in [0;36maddFile[0;34m(self, path, recursive)[0m
[1;32m    896[0m         [0;34m[[0m[0;36m100[0m[0;34m,[0m [0;36m200[0m[0;34m,[0m [0;36m300[0m[0;34m,[0m [0;36m400[0m[0;34m][0m[0;34m[0m[0m
[1;32m    897[0m         """
[0;32m--> 898[0;31m         [0mself[0m[0;34m.[0m[0m_jsc[0m[0;34m.[0m[0msc[0m[0;34m([0m[0;34m)[0m[0;34m.[0m[0maddFile[0m[0;34m([0m[0mpath[0m[0;34m,[0m [0mrecursive[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    899[0m [0;34m[0m[0m
[1;32m    900[0m     [0;32mdef[0m [0maddPyFile[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mpath[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     61[0m     [0;32mdef[0m [0mdeco[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     62[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 63[0;31m             [0;32mreturn[0m [0mf[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     64[0m         [0;32mexcept[0m [0mpy4j[0m[0;34m.[0m[0mprotocol[0m[0;34m.[0m[0mPy4JJavaError[0m [0;32mas[0m [0me[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     65[0m             [0ms[0m [0;34m=[0m [0me[0m[0;34m.[0m[0mjava_exception[0m[0;34m.[0m[0mtoString[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py[0m in [0;36mget_return_value[0;34m(answer, gateway_client, target_id, name)[0m
[1;32m    326[0m                 raise Py4JJavaError(
[1;32m    327[0m                     [0;34m"An error occurred while calling {0}{1}{2}.\n"[0m[0;34m.[0m[0;34m[0m[0m
[0;32m--> 328[0;31m                     format(target_id, ".", name), value)
[0m[1;32m    329[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    330[0m                 raise Py4JError(

[0;31mPy4JJavaError[0m: An error occurred while calling o46.addFile.
: java.io.FileNotFoundException: File file:/zeppelin/dependencies.zip does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.spark.SparkContext.addFile(SparkContext.scala:1544)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)

 INFO [2020-01-22 18:45:55,338] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:45:55,367] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200121-183200_1639671760 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:46:15,120] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:46:15,141] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20200121-183200_1639671760 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:46:15,142] ({pool-2-thread-3} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-183200_1639671760, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 18:46:16,024] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-183200_1639671760 is finished successfully, status: FINISHED
 INFO [2020-01-22 18:46:16,051] ({pool-2-thread-3} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:46:16,070] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20200121-183200_1639671760 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:46:31,145] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:46:31,165] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200121-183236_977276561 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:46:31,166] ({pool-2-thread-2} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-183236_977276561, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 18:46:42,260] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-183236_977276561 is finished successfully, status: FINISHED
 INFO [2020-01-22 18:46:42,282] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:46:42,301] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200121-183236_977276561 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:46:43,614] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:46:43,634] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200122-174128_1762695518 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:46:43,634] ({pool-2-thread-4} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-174128_1762695518, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 18:46:43,858] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-174128_1762695518 is finished successfully, status: FINISHED
 INFO [2020-01-22 18:46:43,895] ({pool-2-thread-4} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:46:43,940] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200122-174128_1762695518 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:46:45,815] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:46:45,835] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:46:45,836] ({pool-2-thread-3} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 18:46:52,135] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-173241_479707786 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mPy4JJavaError[0mTraceback (most recent call last)
[0;32m<ipython-input-12-e4874b15b491>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdf[0m[0;34m.[0m[0mselect[0m[0;34m([0m[0msourceExtractor[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m,[0m[0mdateOnlyExtractor[0m[0;34m([0m[0;34m"created_at"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"created_at"[0m[0;34m)[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mgroupBy[0m[0;34m([0m[0;34m"source"[0m[0;34m,[0m[0;34m"created_at"[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mcount[0m[0;34m([0m[0;34m)[0m[0;34m.[0m[0mshow[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py[0m in [0;36mshow[0;34m(self, n, truncate, vertical)[0m
[1;32m    376[0m         """
[1;32m    377[0m         [0;32mif[0m [0misinstance[0m[0;34m([0m[0mtruncate[0m[0;34m,[0m [0mbool[0m[0;34m)[0m [0;32mand[0m [0mtruncate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 378[0;31m             [0;32mprint[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mshowString[0m[0;34m([0m[0mn[0m[0;34m,[0m [0;36m20[0m[0;34m,[0m [0mvertical[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    379[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    380[0m             [0;32mprint[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mshowString[0m[0;34m([0m[0mn[0m[0;34m,[0m [0mint[0m[0;34m([0m[0mtruncate[0m[0;34m)[0m[0;34m,[0m [0mvertical[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     61[0m     [0;32mdef[0m [0mdeco[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     62[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 63[0;31m             [0;32mreturn[0m [0mf[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     64[0m         [0;32mexcept[0m [0mpy4j[0m[0;34m.[0m[0mprotocol[0m[0;34m.[0m[0mPy4JJavaError[0m [0;32mas[0m [0me[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     65[0m             [0ms[0m [0;34m=[0m [0me[0m[0;34m.[0m[0mjava_exception[0m[0;34m.[0m[0mtoString[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py[0m in [0;36mget_return_value[0;34m(answer, gateway_client, target_id, name)[0m
[1;32m    326[0m                 raise Py4JJavaError(
[1;32m    327[0m                     [0;34m"An error occurred while calling {0}{1}{2}.\n"[0m[0;34m.[0m[0;34m[0m[0m
[0;32m--> 328[0;31m                     format(target_id, ".", name), value)
[0m[1;32m    329[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    330[0m                 raise Py4JError(

[0;31mPy4JJavaError[0m: An error occurred while calling o130.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 4, 172.19.0.2, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in main
    process()
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 367, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 243, in <lambda>
    func = lambda _, it: map(mapper, it)
  File "<string>", line 1, in <lambda>
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 80, in <lambda>
    return lambda *a: f(*a)
  File "/spark/python/lib/pyspark.zip/pyspark/util.py", line 99, in wrapper
    return f(*args, **kwargs)
  File "<ipython-input-10-820a517f400f>", line 6, in <lambda>
AttributeError: 'unicode' object has no attribute 'strftime'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2759)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:292)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in main
    process()
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 367, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 243, in <lambda>
    func = lambda _, it: map(mapper, it)
  File "<string>", line 1, in <lambda>
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 80, in <lambda>
    return lambda *a: f(*a)
  File "/spark/python/lib/pyspark.zip/pyspark/util.py", line 99, in wrapper
    return f(*args, **kwargs)
  File "<ipython-input-10-820a517f400f>", line 6, in <lambda>
AttributeError: 'unicode' object has no attribute 'strftime'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

 INFO [2020-01-22 18:46:52,155] ({pool-2-thread-3} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:46:52,189] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:47:48,336] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:47:48,358] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20200122-174128_1762695518 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:47:48,364] ({pool-2-thread-5} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-174128_1762695518, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 18:47:48,437] ({pool-2-thread-5} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-174128_1762695518 is finished successfully, status: FINISHED
 INFO [2020-01-22 18:47:48,468] ({pool-2-thread-5} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:47:48,485] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20200122-174128_1762695518 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:47:50,104] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:47:50,131] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:47:50,133] ({pool-2-thread-2} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 18:47:54,751] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-173241_479707786 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mPy4JJavaError[0mTraceback (most recent call last)
[0;32m<ipython-input-16-e4874b15b491>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdf[0m[0;34m.[0m[0mselect[0m[0;34m([0m[0msourceExtractor[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m,[0m[0mdateOnlyExtractor[0m[0;34m([0m[0;34m"created_at"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"created_at"[0m[0;34m)[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mgroupBy[0m[0;34m([0m[0;34m"source"[0m[0;34m,[0m[0;34m"created_at"[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mcount[0m[0;34m([0m[0;34m)[0m[0;34m.[0m[0mshow[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py[0m in [0;36mshow[0;34m(self, n, truncate, vertical)[0m
[1;32m    376[0m         """
[1;32m    377[0m         [0;32mif[0m [0misinstance[0m[0;34m([0m[0mtruncate[0m[0;34m,[0m [0mbool[0m[0;34m)[0m [0;32mand[0m [0mtruncate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 378[0;31m             [0;32mprint[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mshowString[0m[0;34m([0m[0mn[0m[0;34m,[0m [0;36m20[0m[0;34m,[0m [0mvertical[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    379[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    380[0m             [0;32mprint[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mshowString[0m[0;34m([0m[0mn[0m[0;34m,[0m [0mint[0m[0;34m([0m[0mtruncate[0m[0;34m)[0m[0;34m,[0m [0mvertical[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     61[0m     [0;32mdef[0m [0mdeco[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     62[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 63[0;31m             [0;32mreturn[0m [0mf[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     64[0m         [0;32mexcept[0m [0mpy4j[0m[0;34m.[0m[0mprotocol[0m[0;34m.[0m[0mPy4JJavaError[0m [0;32mas[0m [0me[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     65[0m             [0ms[0m [0;34m=[0m [0me[0m[0;34m.[0m[0mjava_exception[0m[0;34m.[0m[0mtoString[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py[0m in [0;36mget_return_value[0;34m(answer, gateway_client, target_id, name)[0m
[1;32m    326[0m                 raise Py4JJavaError(
[1;32m    327[0m                     [0;34m"An error occurred while calling {0}{1}{2}.\n"[0m[0;34m.[0m[0;34m[0m[0m
[0;32m--> 328[0;31m                     format(target_id, ".", name), value)
[0m[1;32m    329[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    330[0m                 raise Py4JError(

[0;31mPy4JJavaError[0m: An error occurred while calling o226.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 8, 172.19.0.2, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in main
    process()
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 367, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 243, in <lambda>
    func = lambda _, it: map(mapper, it)
  File "<string>", line 1, in <lambda>
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 80, in <lambda>
    return lambda *a: f(*a)
  File "/spark/python/lib/pyspark.zip/pyspark/util.py", line 99, in wrapper
    return f(*args, **kwargs)
  File "<ipython-input-14-8a2ef419d4de>", line 6, in <lambda>
TypeError: descriptor 'strftime' requires a 'datetime.date' object but received a 'unicode'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2759)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:292)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in main
    process()
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 367, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 243, in <lambda>
    func = lambda _, it: map(mapper, it)
  File "<string>", line 1, in <lambda>
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 80, in <lambda>
    return lambda *a: f(*a)
  File "/spark/python/lib/pyspark.zip/pyspark/util.py", line 99, in wrapper
    return f(*args, **kwargs)
  File "<ipython-input-14-8a2ef419d4de>", line 6, in <lambda>
TypeError: descriptor 'strftime' requires a 'datetime.date' object but received a 'unicode'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

 INFO [2020-01-22 18:47:54,783] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:47:54,816] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:54:35,007] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:54:35,028] ({pool-2-thread-6} SchedulerFactory.java[jobStarted]:114) - Job 20200122-174128_1762695518 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:54:35,029] ({pool-2-thread-6} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-174128_1762695518, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 18:54:35,116] ({pool-2-thread-6} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-174128_1762695518 is finished successfully, status: FINISHED
 INFO [2020-01-22 18:54:35,129] ({pool-2-thread-6} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:54:35,145] ({pool-2-thread-6} SchedulerFactory.java[jobFinished]:120) - Job 20200122-174128_1762695518 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:54:36,972] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:54:37,001] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:54:37,003] ({pool-2-thread-4} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 18:54:41,182] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-173241_479707786 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mPy4JJavaError[0mTraceback (most recent call last)
[0;32m<ipython-input-20-e4874b15b491>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdf[0m[0;34m.[0m[0mselect[0m[0;34m([0m[0msourceExtractor[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m,[0m[0mdateOnlyExtractor[0m[0;34m([0m[0;34m"created_at"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"created_at"[0m[0;34m)[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mgroupBy[0m[0;34m([0m[0;34m"source"[0m[0;34m,[0m[0;34m"created_at"[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mcount[0m[0;34m([0m[0;34m)[0m[0;34m.[0m[0mshow[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py[0m in [0;36mshow[0;34m(self, n, truncate, vertical)[0m
[1;32m    376[0m         """
[1;32m    377[0m         [0;32mif[0m [0misinstance[0m[0;34m([0m[0mtruncate[0m[0;34m,[0m [0mbool[0m[0;34m)[0m [0;32mand[0m [0mtruncate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 378[0;31m             [0;32mprint[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mshowString[0m[0;34m([0m[0mn[0m[0;34m,[0m [0;36m20[0m[0;34m,[0m [0mvertical[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    379[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    380[0m             [0;32mprint[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mshowString[0m[0;34m([0m[0mn[0m[0;34m,[0m [0mint[0m[0;34m([0m[0mtruncate[0m[0;34m)[0m[0;34m,[0m [0mvertical[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     61[0m     [0;32mdef[0m [0mdeco[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     62[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 63[0;31m             [0;32mreturn[0m [0mf[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     64[0m         [0;32mexcept[0m [0mpy4j[0m[0;34m.[0m[0mprotocol[0m[0;34m.[0m[0mPy4JJavaError[0m [0;32mas[0m [0me[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     65[0m             [0ms[0m [0;34m=[0m [0me[0m[0;34m.[0m[0mjava_exception[0m[0;34m.[0m[0mtoString[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py[0m in [0;36mget_return_value[0;34m(answer, gateway_client, target_id, name)[0m
[1;32m    326[0m                 raise Py4JJavaError(
[1;32m    327[0m                     [0;34m"An error occurred while calling {0}{1}{2}.\n"[0m[0;34m.[0m[0;34m[0m[0m
[0;32m--> 328[0;31m                     format(target_id, ".", name), value)
[0m[1;32m    329[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    330[0m                 raise Py4JError(

[0;31mPy4JJavaError[0m: An error occurred while calling o322.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 12, 172.19.0.2, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in main
    process()
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 367, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 243, in <lambda>
    func = lambda _, it: map(mapper, it)
  File "<string>", line 1, in <lambda>
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 80, in <lambda>
    return lambda *a: f(*a)
  File "/spark/python/lib/pyspark.zip/pyspark/util.py", line 99, in wrapper
    return f(*args, **kwargs)
  File "<ipython-input-18-b19771fa94ad>", line 6, in <lambda>
  File "/usr/lib/python2.7/_strptime.py", line 332, in _strptime
    (data_string, format))
ValueError: time data 'Tue Jan 21 03:52:41 +0000 2020' does not match format '%Y-%m-%d'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2759)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:292)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in main
    process()
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 367, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 243, in <lambda>
    func = lambda _, it: map(mapper, it)
  File "<string>", line 1, in <lambda>
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 80, in <lambda>
    return lambda *a: f(*a)
  File "/spark/python/lib/pyspark.zip/pyspark/util.py", line 99, in wrapper
    return f(*args, **kwargs)
  File "<ipython-input-18-b19771fa94ad>", line 6, in <lambda>
  File "/usr/lib/python2.7/_strptime.py", line 332, in _strptime
    (data_string, format))
ValueError: time data 'Tue Jan 21 03:52:41 +0000 2020' does not match format '%Y-%m-%d'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

 INFO [2020-01-22 18:54:41,212] ({pool-2-thread-4} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:54:41,233] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:55:50,379] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:57:24,656] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:57:59,328] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:57:59,347] ({pool-2-thread-7} SchedulerFactory.java[jobStarted]:114) - Job 20200122-174128_1762695518 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:57:59,348] ({pool-2-thread-7} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-174128_1762695518, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 18:57:59,428] ({pool-2-thread-7} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-174128_1762695518 is finished successfully, status: FINISHED
 INFO [2020-01-22 18:57:59,441] ({pool-2-thread-7} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:57:59,455] ({pool-2-thread-7} SchedulerFactory.java[jobFinished]:120) - Job 20200122-174128_1762695518 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:58:01,298] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:58:01,317] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:58:01,317] ({pool-2-thread-3} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 18:58:04,828] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-173241_479707786 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mPy4JJavaError[0mTraceback (most recent call last)
[0;32m<ipython-input-24-e4874b15b491>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdf[0m[0;34m.[0m[0mselect[0m[0;34m([0m[0msourceExtractor[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m,[0m[0mdateOnlyExtractor[0m[0;34m([0m[0;34m"created_at"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"created_at"[0m[0;34m)[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mgroupBy[0m[0;34m([0m[0;34m"source"[0m[0;34m,[0m[0;34m"created_at"[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mcount[0m[0;34m([0m[0;34m)[0m[0;34m.[0m[0mshow[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py[0m in [0;36mshow[0;34m(self, n, truncate, vertical)[0m
[1;32m    376[0m         """
[1;32m    377[0m         [0;32mif[0m [0misinstance[0m[0;34m([0m[0mtruncate[0m[0;34m,[0m [0mbool[0m[0;34m)[0m [0;32mand[0m [0mtruncate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 378[0;31m             [0;32mprint[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mshowString[0m[0;34m([0m[0mn[0m[0;34m,[0m [0;36m20[0m[0;34m,[0m [0mvertical[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    379[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    380[0m             [0;32mprint[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mshowString[0m[0;34m([0m[0mn[0m[0;34m,[0m [0mint[0m[0;34m([0m[0mtruncate[0m[0;34m)[0m[0;34m,[0m [0mvertical[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     61[0m     [0;32mdef[0m [0mdeco[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     62[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 63[0;31m             [0;32mreturn[0m [0mf[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     64[0m         [0;32mexcept[0m [0mpy4j[0m[0;34m.[0m[0mprotocol[0m[0;34m.[0m[0mPy4JJavaError[0m [0;32mas[0m [0me[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     65[0m             [0ms[0m [0;34m=[0m [0me[0m[0;34m.[0m[0mjava_exception[0m[0;34m.[0m[0mtoString[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py[0m in [0;36mget_return_value[0;34m(answer, gateway_client, target_id, name)[0m
[1;32m    326[0m                 raise Py4JJavaError(
[1;32m    327[0m                     [0;34m"An error occurred while calling {0}{1}{2}.\n"[0m[0;34m.[0m[0;34m[0m[0m
[0;32m--> 328[0;31m                     format(target_id, ".", name), value)
[0m[1;32m    329[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    330[0m                 raise Py4JError(

[0;31mPy4JJavaError[0m: An error occurred while calling o418.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 4 times, most recent failure: Lost task 0.3 in stage 7.0 (TID 16, 172.19.0.2, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in main
    process()
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 367, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 243, in <lambda>
    func = lambda _, it: map(mapper, it)
  File "<string>", line 1, in <lambda>
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 80, in <lambda>
    return lambda *a: f(*a)
  File "/spark/python/lib/pyspark.zip/pyspark/util.py", line 99, in wrapper
    return f(*args, **kwargs)
  File "<ipython-input-22-e2703c1386ec>", line 6, in <lambda>
  File "/usr/lib/python2.7/_strptime.py", line 324, in _strptime
    (bad_directive, format))
ValueError: 'z' is a bad directive in format '%a %b %d %H:%M:%S %z %Y'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2759)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:292)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in main
    process()
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 367, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 243, in <lambda>
    func = lambda _, it: map(mapper, it)
  File "<string>", line 1, in <lambda>
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 80, in <lambda>
    return lambda *a: f(*a)
  File "/spark/python/lib/pyspark.zip/pyspark/util.py", line 99, in wrapper
    return f(*args, **kwargs)
  File "<ipython-input-22-e2703c1386ec>", line 6, in <lambda>
  File "/usr/lib/python2.7/_strptime.py", line 324, in _strptime
    (bad_directive, format))
ValueError: 'z' is a bad directive in format '%a %b %d %H:%M:%S %z %Y'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

 INFO [2020-01-22 18:58:04,849] ({pool-2-thread-3} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:58:04,869] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:58:37,710] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:58:37,789] ({pool-2-thread-8} SchedulerFactory.java[jobStarted]:114) - Job 20200122-174128_1762695518 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:58:37,790] ({pool-2-thread-8} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-174128_1762695518, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 18:58:37,900] ({pool-2-thread-8} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-174128_1762695518 is finished successfully, status: FINISHED
 INFO [2020-01-22 18:58:37,942] ({pool-2-thread-8} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:58:37,959] ({pool-2-thread-8} SchedulerFactory.java[jobFinished]:120) - Job 20200122-174128_1762695518 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:58:39,257] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:58:39,282] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:58:39,283] ({pool-2-thread-5} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 18:58:42,668] ({pool-2-thread-5} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-173241_479707786 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mPy4JJavaError[0mTraceback (most recent call last)
[0;32m<ipython-input-28-e4874b15b491>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdf[0m[0;34m.[0m[0mselect[0m[0;34m([0m[0msourceExtractor[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m,[0m[0mdateOnlyExtractor[0m[0;34m([0m[0;34m"created_at"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"created_at"[0m[0;34m)[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mgroupBy[0m[0;34m([0m[0;34m"source"[0m[0;34m,[0m[0;34m"created_at"[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mcount[0m[0;34m([0m[0;34m)[0m[0;34m.[0m[0mshow[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py[0m in [0;36mshow[0;34m(self, n, truncate, vertical)[0m
[1;32m    376[0m         """
[1;32m    377[0m         [0;32mif[0m [0misinstance[0m[0;34m([0m[0mtruncate[0m[0;34m,[0m [0mbool[0m[0;34m)[0m [0;32mand[0m [0mtruncate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 378[0;31m             [0;32mprint[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mshowString[0m[0;34m([0m[0mn[0m[0;34m,[0m [0;36m20[0m[0;34m,[0m [0mvertical[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    379[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    380[0m             [0;32mprint[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mshowString[0m[0;34m([0m[0mn[0m[0;34m,[0m [0mint[0m[0;34m([0m[0mtruncate[0m[0;34m)[0m[0;34m,[0m [0mvertical[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     61[0m     [0;32mdef[0m [0mdeco[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     62[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 63[0;31m             [0;32mreturn[0m [0mf[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     64[0m         [0;32mexcept[0m [0mpy4j[0m[0;34m.[0m[0mprotocol[0m[0;34m.[0m[0mPy4JJavaError[0m [0;32mas[0m [0me[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     65[0m             [0ms[0m [0;34m=[0m [0me[0m[0;34m.[0m[0mjava_exception[0m[0;34m.[0m[0mtoString[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py[0m in [0;36mget_return_value[0;34m(answer, gateway_client, target_id, name)[0m
[1;32m    326[0m                 raise Py4JJavaError(
[1;32m    327[0m                     [0;34m"An error occurred while calling {0}{1}{2}.\n"[0m[0;34m.[0m[0;34m[0m[0m
[0;32m--> 328[0;31m                     format(target_id, ".", name), value)
[0m[1;32m    329[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    330[0m                 raise Py4JError(

[0;31mPy4JJavaError[0m: An error occurred while calling o514.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 4 times, most recent failure: Lost task 0.3 in stage 9.0 (TID 20, 172.19.0.2, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in main
    process()
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 367, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 243, in <lambda>
    func = lambda _, it: map(mapper, it)
  File "<string>", line 1, in <lambda>
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 80, in <lambda>
    return lambda *a: f(*a)
  File "/spark/python/lib/pyspark.zip/pyspark/util.py", line 99, in wrapper
    return f(*args, **kwargs)
  File "<ipython-input-26-663f226af5ae>", line 6, in <lambda>
  File "/usr/lib/python2.7/_strptime.py", line 332, in _strptime
    (data_string, format))
ValueError: time data 'Tue Jan 21 03:52:41 +0000 2020' does not match format '%a %b %d %H:%M:%S %Z %Y'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2759)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:292)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in main
    process()
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 367, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 243, in <lambda>
    func = lambda _, it: map(mapper, it)
  File "<string>", line 1, in <lambda>
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 80, in <lambda>
    return lambda *a: f(*a)
  File "/spark/python/lib/pyspark.zip/pyspark/util.py", line 99, in wrapper
    return f(*args, **kwargs)
  File "<ipython-input-26-663f226af5ae>", line 6, in <lambda>
  File "/usr/lib/python2.7/_strptime.py", line 332, in _strptime
    (data_string, format))
ValueError: time data 'Tue Jan 21 03:52:41 +0000 2020' does not match format '%a %b %d %H:%M:%S %Z %Y'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

 INFO [2020-01-22 18:58:42,680] ({pool-2-thread-5} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 18:58:42,695] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 18:59:51,965] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:00:14,824] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:00:14,841] ({pool-2-thread-9} SchedulerFactory.java[jobStarted]:114) - Job 20200122-174128_1762695518 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:00:14,844] ({pool-2-thread-9} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-174128_1762695518, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:00:14,950] ({pool-2-thread-9} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-174128_1762695518 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:00:14,993] ({pool-2-thread-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:00:15,030] ({pool-2-thread-9} SchedulerFactory.java[jobFinished]:120) - Job 20200122-174128_1762695518 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:00:16,671] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:00:16,687] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:00:16,688] ({pool-2-thread-2} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 19:00:19,890] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-173241_479707786 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mPy4JJavaError[0mTraceback (most recent call last)
[0;32m<ipython-input-32-e4874b15b491>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdf[0m[0;34m.[0m[0mselect[0m[0;34m([0m[0msourceExtractor[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m,[0m[0mdateOnlyExtractor[0m[0;34m([0m[0;34m"created_at"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"created_at"[0m[0;34m)[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mgroupBy[0m[0;34m([0m[0;34m"source"[0m[0;34m,[0m[0;34m"created_at"[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mcount[0m[0;34m([0m[0;34m)[0m[0;34m.[0m[0mshow[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py[0m in [0;36mshow[0;34m(self, n, truncate, vertical)[0m
[1;32m    376[0m         """
[1;32m    377[0m         [0;32mif[0m [0misinstance[0m[0;34m([0m[0mtruncate[0m[0;34m,[0m [0mbool[0m[0;34m)[0m [0;32mand[0m [0mtruncate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 378[0;31m             [0;32mprint[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mshowString[0m[0;34m([0m[0mn[0m[0;34m,[0m [0;36m20[0m[0;34m,[0m [0mvertical[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    379[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    380[0m             [0;32mprint[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mshowString[0m[0;34m([0m[0mn[0m[0;34m,[0m [0mint[0m[0;34m([0m[0mtruncate[0m[0;34m)[0m[0;34m,[0m [0mvertical[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     61[0m     [0;32mdef[0m [0mdeco[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     62[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 63[0;31m             [0;32mreturn[0m [0mf[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     64[0m         [0;32mexcept[0m [0mpy4j[0m[0;34m.[0m[0mprotocol[0m[0;34m.[0m[0mPy4JJavaError[0m [0;32mas[0m [0me[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     65[0m             [0ms[0m [0;34m=[0m [0me[0m[0;34m.[0m[0mjava_exception[0m[0;34m.[0m[0mtoString[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py[0m in [0;36mget_return_value[0;34m(answer, gateway_client, target_id, name)[0m
[1;32m    326[0m                 raise Py4JJavaError(
[1;32m    327[0m                     [0;34m"An error occurred while calling {0}{1}{2}.\n"[0m[0;34m.[0m[0;34m[0m[0m
[0;32m--> 328[0;31m                     format(target_id, ".", name), value)
[0m[1;32m    329[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    330[0m                 raise Py4JError(

[0;31mPy4JJavaError[0m: An error occurred while calling o610.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 4 times, most recent failure: Lost task 0.3 in stage 11.0 (TID 24, 172.19.0.2, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in main
    process()
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 367, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 243, in <lambda>
    func = lambda _, it: map(mapper, it)
  File "<string>", line 1, in <lambda>
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 80, in <lambda>
    return lambda *a: f(*a)
  File "/spark/python/lib/pyspark.zip/pyspark/util.py", line 99, in wrapper
    return f(*args, **kwargs)
  File "<ipython-input-30-e2703c1386ec>", line 6, in <lambda>
  File "/usr/lib/python2.7/_strptime.py", line 324, in _strptime
    (bad_directive, format))
ValueError: 'z' is a bad directive in format '%a %b %d %H:%M:%S %z %Y'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2759)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:292)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in main
    process()
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 367, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 243, in <lambda>
    func = lambda _, it: map(mapper, it)
  File "<string>", line 1, in <lambda>
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 80, in <lambda>
    return lambda *a: f(*a)
  File "/spark/python/lib/pyspark.zip/pyspark/util.py", line 99, in wrapper
    return f(*args, **kwargs)
  File "<ipython-input-30-e2703c1386ec>", line 6, in <lambda>
  File "/usr/lib/python2.7/_strptime.py", line 324, in _strptime
    (bad_directive, format))
ValueError: 'z' is a bad directive in format '%a %b %d %H:%M:%S %z %Y'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

 INFO [2020-01-22 19:00:19,911] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:00:19,941] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:01:34,430] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:01:34,447] ({pool-2-thread-10} SchedulerFactory.java[jobStarted]:114) - Job 20200122-174128_1762695518 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:01:34,451] ({pool-2-thread-10} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-174128_1762695518, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:01:34,518] ({pool-2-thread-10} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-174128_1762695518 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:01:34,528] ({pool-2-thread-10} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:01:34,541] ({pool-2-thread-10} SchedulerFactory.java[jobFinished]:120) - Job 20200122-174128_1762695518 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:01:36,795] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:01:36,812] ({pool-2-thread-6} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:01:36,815] ({pool-2-thread-6} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 19:01:40,444] ({pool-2-thread-6} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-173241_479707786 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mPy4JJavaError[0mTraceback (most recent call last)
[0;32m<ipython-input-36-e4874b15b491>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdf[0m[0;34m.[0m[0mselect[0m[0;34m([0m[0msourceExtractor[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m,[0m[0mdateOnlyExtractor[0m[0;34m([0m[0;34m"created_at"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"created_at"[0m[0;34m)[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mgroupBy[0m[0;34m([0m[0;34m"source"[0m[0;34m,[0m[0;34m"created_at"[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mcount[0m[0;34m([0m[0;34m)[0m[0;34m.[0m[0mshow[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py[0m in [0;36mshow[0;34m(self, n, truncate, vertical)[0m
[1;32m    376[0m         """
[1;32m    377[0m         [0;32mif[0m [0misinstance[0m[0;34m([0m[0mtruncate[0m[0;34m,[0m [0mbool[0m[0;34m)[0m [0;32mand[0m [0mtruncate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 378[0;31m             [0;32mprint[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mshowString[0m[0;34m([0m[0mn[0m[0;34m,[0m [0;36m20[0m[0;34m,[0m [0mvertical[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    379[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    380[0m             [0;32mprint[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mshowString[0m[0;34m([0m[0mn[0m[0;34m,[0m [0mint[0m[0;34m([0m[0mtruncate[0m[0;34m)[0m[0;34m,[0m [0mvertical[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     61[0m     [0;32mdef[0m [0mdeco[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     62[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 63[0;31m             [0;32mreturn[0m [0mf[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     64[0m         [0;32mexcept[0m [0mpy4j[0m[0;34m.[0m[0mprotocol[0m[0;34m.[0m[0mPy4JJavaError[0m [0;32mas[0m [0me[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     65[0m             [0ms[0m [0;34m=[0m [0me[0m[0;34m.[0m[0mjava_exception[0m[0;34m.[0m[0mtoString[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py[0m in [0;36mget_return_value[0;34m(answer, gateway_client, target_id, name)[0m
[1;32m    326[0m                 raise Py4JJavaError(
[1;32m    327[0m                     [0;34m"An error occurred while calling {0}{1}{2}.\n"[0m[0;34m.[0m[0;34m[0m[0m
[0;32m--> 328[0;31m                     format(target_id, ".", name), value)
[0m[1;32m    329[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    330[0m                 raise Py4JError(

[0;31mPy4JJavaError[0m: An error occurred while calling o706.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 13.0 failed 4 times, most recent failure: Lost task 0.3 in stage 13.0 (TID 28, 172.19.0.2, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in main
    process()
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 367, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 243, in <lambda>
    func = lambda _, it: map(mapper, it)
  File "<string>", line 1, in <lambda>
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 80, in <lambda>
    return lambda *a: f(*a)
  File "/spark/python/lib/pyspark.zip/pyspark/util.py", line 99, in wrapper
    return f(*args, **kwargs)
  File "<ipython-input-34-ff6d3aae1e8f>", line 6, in <lambda>
  File "/usr/lib/python2.7/_strptime.py", line 332, in _strptime
    (data_string, format))
ValueError: time data 'Tue Jan 21 03:52:41 +0000 2020' does not match format '%a %b %d %H:%M:%S %Y'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2759)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:292)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in main
    process()
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 367, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 243, in <lambda>
    func = lambda _, it: map(mapper, it)
  File "<string>", line 1, in <lambda>
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 80, in <lambda>
    return lambda *a: f(*a)
  File "/spark/python/lib/pyspark.zip/pyspark/util.py", line 99, in wrapper
    return f(*args, **kwargs)
  File "<ipython-input-34-ff6d3aae1e8f>", line 6, in <lambda>
  File "/usr/lib/python2.7/_strptime.py", line 332, in _strptime
    (data_string, format))
ValueError: time data 'Tue Jan 21 03:52:41 +0000 2020' does not match format '%a %b %d %H:%M:%S %Y'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

 INFO [2020-01-22 19:01:40,457] ({pool-2-thread-6} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:01:40,473] ({pool-2-thread-6} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:05:52,134] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:05:57,840] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:05:57,871] ({pool-2-thread-11} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:05:57,872] ({pool-2-thread-11} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:06:02,981] ({pool-2-thread-11} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-173241_479707786 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:06:03,000] ({pool-2-thread-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:06:03,031] ({pool-2-thread-11} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:12:03,775] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:12:03,792] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200122-174128_1762695518 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:12:03,793] ({pool-2-thread-4} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-174128_1762695518, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:12:03,859] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-174128_1762695518 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:12:03,868] ({pool-2-thread-4} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:12:03,883] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200122-174128_1762695518 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:12:06,562] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:12:06,578] ({pool-2-thread-12} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:12:06,579] ({pool-2-thread-12} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:12:10,421] ({pool-2-thread-12} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-173241_479707786 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:12:10,472] ({pool-2-thread-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:12:10,490] ({pool-2-thread-12} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:14:11,377] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:14:30,538] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:14:42,808] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:15:53,590] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:15:53,610] ({pool-2-thread-7} SchedulerFactory.java[jobStarted]:114) - Job 20200121-185449_2110594246 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:15:53,612] ({pool-2-thread-7} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-185449_2110594246, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 19:15:55,450] ({pool-2-thread-7} NotebookServer.java[afterStatusChange]:2316) - Job 20200121-185449_2110594246 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mAnalysisException[0mTraceback (most recent call last)
[0;32m<ipython-input-44-c1181422e7aa>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdataDf[0m [0;34m=[0m [0mspark[0m[0;34m.[0m[0mreadStream[0m[0;34m.[0m[0mjson[0m[0;34m([0m[0;34m"s3a://j2training/data"[0m[0;34m,[0m[0mdf[0m[0;34m.[0m[0mschema[0m[0;34m)[0m[0;34m.[0m[0mselect[0m[0;34m([0m[0msourceExtractor[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m,[0m[0;34m"created_at"[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mgroupBy[0m[0;34m([0m[0;34m"source"[0m[0;34m,[0m[0;34m"created_at"[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mcount[0m[0;34m([0m[0;34m)[0m[0;34m.[0m[0mwriteStream[0m     [0;34m.[0m[0moutputMode[0m[0;34m([0m[0;34m"complete"[0m[0;34m)[0m     [0;34m.[0m[0moption[0m[0;34m([0m[0;34m"checkpointLocation"[0m[0;34m,[0m [0;34m"s3a://j2training/checkpoint/"[0m[0;34m)[0m     [0;34m.[0m[0mformat[0m[0;34m([0m[0;34m"memory"[0m[0;34m)[0m     [0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py[0m in [0;36mstart[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)[0m
[1;32m   1103[0m             [0mself[0m[0;34m.[0m[0mqueryName[0m[0;34m([0m[0mqueryName[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1104[0m         [0;32mif[0m [0mpath[0m [0;32mis[0m [0mNone[0m[0;34m:[0m[0;34m[0m[0m
[0;32m-> 1105[0;31m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_sq[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jwrite[0m[0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1106[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m   1107[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_sq[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jwrite[0m[0;34m.[0m[0mstart[0m[0;34m([0m[0mpath[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     67[0m                                              e.java_exception.getStackTrace()))
[1;32m     68[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.AnalysisException: '[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 69[0;31m                 [0;32mraise[0m [0mAnalysisException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     70[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.catalyst.analysis'[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     71[0m                 [0;32mraise[0m [0mAnalysisException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m

[0;31mAnalysisException[0m: u'queryName must be specified for memory sink;'
 INFO [2020-01-22 19:15:55,466] ({pool-2-thread-7} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:15:55,492] ({pool-2-thread-7} SchedulerFactory.java[jobFinished]:120) - Job 20200121-185449_2110594246 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:16:15,124] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:16:15,187] ({pool-2-thread-13} SchedulerFactory.java[jobStarted]:114) - Job 20200121-185449_2110594246 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:16:15,188] ({pool-2-thread-13} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-185449_2110594246, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 19:16:16,715] ({pool-2-thread-13} NotebookServer.java[afterStatusChange]:2316) - Job 20200121-185449_2110594246 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mAnalysisException[0mTraceback (most recent call last)
[0;32m<ipython-input-46-1e031551bec9>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdataDf[0m [0;34m=[0m [0mspark[0m[0;34m.[0m[0mreadStream[0m[0;34m.[0m[0mjson[0m[0;34m([0m[0;34m"s3a://j2training/data"[0m[0;34m,[0m[0mdf[0m[0;34m.[0m[0mschema[0m[0;34m)[0m[0;34m.[0m[0mselect[0m[0;34m([0m[0msourceExtractor[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m,[0m[0;34m"created_at"[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mgroupBy[0m[0;34m([0m[0;34m"source"[0m[0;34m,[0m[0;34m"created_at"[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mcount[0m[0;34m([0m[0;34m)[0m[0;34m.[0m[0mwriteStream[0m     [0;34m.[0m[0moutputMode[0m[0;34m([0m[0;34m"append"[0m[0;34m)[0m     [0;34m.[0m[0moption[0m[0;34m([0m[0;34m"checkpointLocation"[0m[0;34m,[0m [0;34m"s3a://j2training/checkpoint/"[0m[0;34m)[0m     [0;34m.[0m[0mformat[0m[0;34m([0m[0;34m"console"[0m[0;34m)[0m     [0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py[0m in [0;36mstart[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)[0m
[1;32m   1103[0m             [0mself[0m[0;34m.[0m[0mqueryName[0m[0;34m([0m[0mqueryName[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1104[0m         [0;32mif[0m [0mpath[0m [0;32mis[0m [0mNone[0m[0;34m:[0m[0;34m[0m[0m
[0;32m-> 1105[0;31m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_sq[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jwrite[0m[0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1106[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m   1107[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_sq[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jwrite[0m[0;34m.[0m[0mstart[0m[0;34m([0m[0mpath[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     67[0m                                              e.java_exception.getStackTrace()))
[1;32m     68[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.AnalysisException: '[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 69[0;31m                 [0;32mraise[0m [0mAnalysisException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     70[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.catalyst.analysis'[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     71[0m                 [0;32mraise[0m [0mAnalysisException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m

[0;31mAnalysisException[0m: u'Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;;\nAggregate [source#559, created_at#488, lang#505], [source#559, created_at#488, lang#505, count(1) AS count#567L]\n+- Project [<lambda>(source#517) AS source#559, created_at#488, lang#505]\n   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@6c5dbac8,json,List(),Some(StructType(StructField(contributors,StringType,true), StructField(coordinates,StructType(StructField(coordinates,ArrayType(DoubleType,true),true), StructField(type,StringType,true)),true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StructType(StructField(indices,ArrayType(LongType,true),true), StructField(text,StringType,true)),true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StructType(StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(name,StringType,true), StructField(screen_name,StringType,true)),true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(extended_tweet,StructType(StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StringType,true),true), StructField(user_mentions,ArrayType(StructType(StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(name,StringType,true), StructField(screen_name,StringType,true)),true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true)),true), StructField(full_text,StringType,true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StructType(StructField(coordinates,ArrayType(DoubleType,true),true), StructField(type,StringType,true)),true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,LongType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,LongType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StructType(StructField(bounding_box,StructType(StructField(coordinates,ArrayType(ArrayType(ArrayType(DoubleType,true),true),true),true), StructField(type,StringType,true)),true), StructField(country,StringType,true), StructField(country_code,StringType,true), StructField(full_name,StringType,true), StructField(id,StringType,true), StructField(name,StringType,true), StructField(place_type,StringType,true), StructField(url,StringType,true)),true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(quoted_status,StructType(StructField(contributors,StringType,true), StructField(coordinates,StringType,true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StringType,true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,StringType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,StringType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StringType,true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true)),true), StructField(quoted_status_id,LongType,true), StructField(quoted_status_id_str,StringType,true), StructField(quoted_status_permalink,StructType(StructField(display,StringType,true), StructField(expanded,StringType,true), StructField(url,StringType,true)),true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(retweeted_status,StructType(StructField(contributors,StringType,true), StructField(coordinates,StringType,true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StructType(StructField(indices,ArrayType(LongType,true),true), StructField(text,StringType,true)),true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StructType(StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(name,StringType,true), StructField(screen_name,StringType,true)),true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(extended_tweet,StructType(StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StringType,true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true)),true), StructField(full_text,StringType,true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,LongType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,LongType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StructType(StructField(bounding_box,StructType(StructField(coordinates,ArrayType(ArrayType(ArrayType(DoubleType,true),true),true),true), StructField(type,StringType,true)),true), StructField(country,StringType,true), StructField(country_code,StringType,true), StructField(full_name,StringType,true), StructField(id,StringType,true), StructField(name,StringType,true), StructField(place_type,StringType,true), StructField(url,StringType,true)),true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(quoted_status,StructType(StructField(contributors,StringType,true), StructField(coordinates,StringType,true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StringType,true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,StringType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,StringType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StringType,true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true)),true), StructField(quoted_status_id,LongType,true), StructField(quoted_status_id_str,StringType,true), StructField(quoted_status_permalink,StructType(StructField(display,StringType,true), StructField(expanded,StringType,true), StructField(url,StringType,true)),true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true)),true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(timestamp_ms,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true))),List(),None,Map(path -> s3a://j2training/data),None), FileSource[s3a://j2training/data], [contributors#486, coordinates#487, created_at#488, display_text_range#489, entities#490, extended_entities#491, extended_tweet#492, favorite_count#493L, favorited#494, filter_level#495, geo#496, id#497L, id_str#498, in_reply_to_screen_name#499, in_reply_to_status_id#500L, in_reply_to_status_id_str#501, in_reply_to_user_id#502L, in_reply_to_user_id_str#503, is_quote_status#504, lang#505, place#506, possibly_sensitive#507, quote_count#508L, quoted_status#509, ... 12 more fields]\n'
 INFO [2020-01-22 19:16:16,735] ({pool-2-thread-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:16:16,755] ({pool-2-thread-13} SchedulerFactory.java[jobFinished]:120) - Job 20200121-185449_2110594246 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:19:19,532] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:19:19,547] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20200121-185449_2110594246 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:19:19,548] ({pool-2-thread-3} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-185449_2110594246, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:19:25,023] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-185449_2110594246 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:19:25,053] ({pool-2-thread-3} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:19:25,070] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20200121-185449_2110594246 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:19:53,797] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:19:53,823] ({pool-2-thread-14} SchedulerFactory.java[jobStarted]:114) - Job 20200121-185449_2110594246 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:19:53,824] ({pool-2-thread-14} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-185449_2110594246, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 19:19:56,282] ({pool-2-thread-14} NotebookServer.java[afterStatusChange]:2316) - Job 20200121-185449_2110594246 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mPy4JJavaError[0mTraceback (most recent call last)
[0;32m<ipython-input-50-ea0944c40499>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdataDf[0m [0;34m=[0m [0mspark[0m[0;34m.[0m[0mreadStream[0m[0;34m.[0m[0mjson[0m[0;34m([0m[0;34m"s3a://j2training/data"[0m[0;34m,[0m[0mdf[0m[0;34m.[0m[0mschema[0m[0;34m)[0m[0;34m.[0m[0mselect[0m[0;34m([0m[0msourceExtractor[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m,[0m[0;34m"created_at"[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mgroupBy[0m[0;34m([0m[0;34m"source"[0m[0;34m,[0m[0;34m"created_at"[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mcount[0m[0;34m([0m[0;34m)[0m[0;34m.[0m[0mwriteStream[0m     [0;34m.[0m[0moutputMode[0m[0;34m([0m[0;34m"complete"[0m[0;34m)[0m     [0;34m.[0m[0moption[0m[0;34m([0m[0;34m"checkpointLocation"[0m[0;34m,[0m [0;34m"s3a://j2training/checkpoint/"[0m[0;34m)[0m     [0;34m.[0m[0mformat[0m[0;34m([0m[0;34m"console"[0m[0;34m)[0m     [0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py[0m in [0;36mstart[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)[0m
[1;32m   1103[0m             [0mself[0m[0;34m.[0m[0mqueryName[0m[0;34m([0m[0mqueryName[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1104[0m         [0;32mif[0m [0mpath[0m [0;32mis[0m [0mNone[0m[0;34m:[0m[0;34m[0m[0m
[0;32m-> 1105[0;31m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_sq[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jwrite[0m[0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1106[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m   1107[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_sq[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jwrite[0m[0;34m.[0m[0mstart[0m[0;34m([0m[0mpath[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     61[0m     [0;32mdef[0m [0mdeco[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     62[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 63[0;31m             [0;32mreturn[0m [0mf[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     64[0m         [0;32mexcept[0m [0mpy4j[0m[0;34m.[0m[0mprotocol[0m[0;34m.[0m[0mPy4JJavaError[0m [0;32mas[0m [0me[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     65[0m             [0ms[0m [0;34m=[0m [0me[0m[0;34m.[0m[0mjava_exception[0m[0;34m.[0m[0mtoString[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py[0m in [0;36mget_return_value[0;34m(answer, gateway_client, target_id, name)[0m
[1;32m    326[0m                 raise Py4JJavaError(
[1;32m    327[0m                     [0;34m"An error occurred while calling {0}{1}{2}.\n"[0m[0;34m.[0m[0;34m[0m[0m
[0;32m--> 328[0;31m                     format(target_id, ".", name), value)
[0m[1;32m    329[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    330[0m                 raise Py4JError(

[0;31mPy4JJavaError[0m: An error occurred while calling o970.start.
: java.lang.IllegalStateException: Cannot start query with id 4d59d541-a18a-4cf5-95bd-9be77ccb83dd as another query with same id is already active. Perhaps you are attempting to restart a query from checkpoint that is already active.
	at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:339)
	at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:325)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)

 INFO [2020-01-22 19:19:56,320] ({pool-2-thread-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:19:56,342] ({pool-2-thread-14} SchedulerFactory.java[jobFinished]:120) - Job 20200121-185449_2110594246 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:20:25,909] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:20:37,406] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:20:37,423] ({pool-2-thread-8} SchedulerFactory.java[jobStarted]:114) - Job 20200121-185449_2110594246 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:20:37,423] ({pool-2-thread-8} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-185449_2110594246, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 19:27:12,482] ({pool-2-thread-8} NotebookServer.java[afterStatusChange]:2316) - Job 20200121-185449_2110594246 is finished, status: ABORT, exception: null, result: %text [0;31m[0m
[0;31mKeyboardInterrupt[0mTraceback (most recent call last)
[0;32m<ipython-input-52-bfdd6aa38623>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdataDf[0m [0;34m=[0m [0mspark[0m[0;34m.[0m[0mreadStream[0m[0;34m.[0m[0mjson[0m[0;34m([0m[0;34m"s3a://j2training/data"[0m[0;34m,[0m[0mdf[0m[0;34m.[0m[0mschema[0m[0;34m)[0m[0;34m.[0m[0mselect[0m[0;34m([0m[0msourceExtractor[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m,[0m[0;34m"created_at"[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mgroupBy[0m[0;34m([0m[0;34m"source"[0m[0;34m,[0m[0;34m"created_at"[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mcount[0m[0;34m([0m[0;34m)[0m[0;34m.[0m[0mwriteStream[0m     [0;34m.[0m[0moutputMode[0m[0;34m([0m[0;34m"complete"[0m[0;34m)[0m     [0;34m.[0m[0moption[0m[0;34m([0m[0;34m"checkpointLocation"[0m[0;34m,[0m [0;34m"s3a://j2training/checkpoint/"[0m[0;34m)[0m     [0;34m.[0m[0mformat[0m[0;34m([0m[0;34m"console"[0m[0;34m)[0m     [0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m.[0m[0mawaitTermination[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py[0m in [0;36mawaitTermination[0;34m(self, timeout)[0m
[1;32m    101[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_jsq[0m[0;34m.[0m[0mawaitTermination[0m[0;34m([0m[0mint[0m[0;34m([0m[0mtimeout[0m [0;34m*[0m [0;36m1000[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[1;32m    102[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 103[0;31m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_jsq[0m[0;34m.[0m[0mawaitTermination[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    104[0m [0;34m[0m[0m
[1;32m    105[0m     [0;34m@[0m[0mproperty[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1253[0m             [0mproto[0m[0;34m.[0m[0mEND_COMMAND_PART[0m[0;34m[0m[0m
[1;32m   1254[0m [0;34m[0m[0m
[0;32m-> 1255[0;31m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1256[0m         return_value = get_return_value(
[1;32m   1257[0m             answer, self.gateway_client, self.target_id, self.name)

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36msend_command[0;34m(self, command, retry, binary)[0m
[1;32m    983[0m         [0mconnection[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_get_connection[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[1;32m    984[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 985[0;31m             [0mresponse[0m [0;34m=[0m [0mconnection[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    986[0m             [0;32mif[0m [0mbinary[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    987[0m                 [0;32mreturn[0m [0mresponse[0m[0;34m,[0m [0mself[0m[0;34m.[0m[0m_create_connection_guard[0m[0;34m([0m[0mconnection[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36msend_command[0;34m(self, command)[0m
[1;32m   1150[0m [0;34m[0m[0m
[1;32m   1151[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m-> 1152[0;31m             [0manswer[0m [0;34m=[0m [0msmart_decode[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mstream[0m[0;34m.[0m[0mreadline[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0;34m:[0m[0;34m-[0m[0;36m1[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1153[0m             [0mlogger[0m[0;34m.[0m[0mdebug[0m[0;34m([0m[0;34m"Answer received: {0}"[0m[0;34m.[0m[0mformat[0m[0;34m([0m[0manswer[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1154[0m             [0;32mif[0m [0manswer[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0mproto[0m[0;34m.[0m[0mRETURN_MESSAGE[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/opt/conda/lib/python2.7/socket.pyc[0m in [0;36mreadline[0;34m(self, size)[0m
[1;32m    449[0m             [0;32mwhile[0m [0mTrue[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    450[0m                 [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 451[0;31m                     [0mdata[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_sock[0m[0;34m.[0m[0mrecv[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_rbufsize[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    452[0m                 [0;32mexcept[0m [0merror[0m[0;34m,[0m [0me[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    453[0m                     [0;32mif[0m [0me[0m[0;34m.[0m[0margs[0m[0;34m[[0m[0;36m0[0m[0;34m][0m [0;34m==[0m [0mEINTR[0m[0;34m:[0m[0;34m[0m[0m

[0;31mKeyboardInterrupt[0m: 
 INFO [2020-01-22 19:27:12,537] ({pool-2-thread-8} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:27:12,561] ({pool-2-thread-8} SchedulerFactory.java[jobFinished]:120) - Job 20200121-185449_2110594246 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:27:34,236] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:27:34,269] ({pool-2-thread-15} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:27:34,270] ({pool-2-thread-15} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:27:35,105] ({pool-2-thread-15} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:27:35,122] ({pool-2-thread-15} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:27:35,182] ({pool-2-thread-15} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:27:51,521] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:28:04,862] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:30:54,288] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:30:54,306] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:30:54,306] ({pool-2-thread-5} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:30:54,399] ({pool-2-thread-5} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:30:54,419] ({pool-2-thread-5} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:30:54,432] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:30:59,663] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:30:59,682] ({pool-2-thread-16} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:30:59,683] ({pool-2-thread-16} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:30:59,789] ({pool-2-thread-16} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:30:59,800] ({pool-2-thread-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:30:59,813] ({pool-2-thread-16} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:31:13,643] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:31:13,656] ({pool-2-thread-9} SchedulerFactory.java[jobStarted]:114) - Job 20200121-185449_2110594246 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:31:13,656] ({pool-2-thread-9} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-185449_2110594246, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:31:15,089] ({pool-2-thread-9} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-185449_2110594246 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:31:15,104] ({pool-2-thread-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:31:15,125] ({pool-2-thread-9} SchedulerFactory.java[jobFinished]:120) - Job 20200121-185449_2110594246 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:31:17,201] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:31:17,218] ({pool-2-thread-17} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:31:17,218] ({pool-2-thread-17} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:31:17,308] ({pool-2-thread-17} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:31:17,324] ({pool-2-thread-17} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:31:17,338] ({pool-2-thread-17} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:31:25,808] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:31:31,369] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:31:31,451] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200122-174128_1762695518 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:31:31,452] ({pool-2-thread-2} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-174128_1762695518, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:31:31,554] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-174128_1762695518 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:31:31,579] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:31:31,595] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200122-174128_1762695518 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:31:37,199] ({qtp2107447833-11} NotebookServer.java[sendNote]:828) - New operation from 192.168.99.1 : 16261 : anonymous : GET_NOTE : 2EXQEGYMG
 WARN [2020-01-22 19:31:37,212] ({qtp2107447833-11} GitNotebookRepo.java[revisionHistory]:158) - No Head found for 2EXQEGYMG, No HEAD exists and no explicit starting revision was specified
 WARN [2020-01-22 19:31:37,229] ({qtp2107447833-13} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 19:31:37,230] ({qtp2107447833-13} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 19:31:37,231] ({qtp2107447833-13} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 19:31:37,231] ({qtp2107447833-13} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 19:31:37,231] ({qtp2107447833-13} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 19:31:37,232] ({qtp2107447833-13} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 INFO [2020-01-22 19:32:11,113] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:32:36,556] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:32:36,572] ({pool-2-thread-18} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:32:36,572] ({pool-2-thread-18} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:32:36,659] ({pool-2-thread-18} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:32:36,671] ({pool-2-thread-18} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:32:36,687] ({pool-2-thread-18} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:33:16,197] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:34:07,538] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:34:10,671] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:34:10,693] ({pool-2-thread-10} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:34:10,698] ({pool-2-thread-10} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:34:10,867] ({pool-2-thread-10} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:34:10,875] ({pool-2-thread-10} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:34:10,888] ({pool-2-thread-10} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:34:21,714] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:34:21,733] ({pool-2-thread-19} SchedulerFactory.java[jobStarted]:114) - Job 20200121-185449_2110594246 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:34:21,734] ({pool-2-thread-19} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-185449_2110594246, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:34:23,214] ({pool-2-thread-19} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-185449_2110594246 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:34:23,232] ({pool-2-thread-19} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:34:23,247] ({pool-2-thread-19} SchedulerFactory.java[jobFinished]:120) - Job 20200121-185449_2110594246 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:34:24,334] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:34:24,362] ({pool-2-thread-6} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:34:24,363] ({pool-2-thread-6} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:34:24,499] ({pool-2-thread-6} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:34:24,530] ({pool-2-thread-6} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:34:24,560] ({pool-2-thread-6} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:34:54,522] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:35:43,109] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:36:12,504] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:36:12,943] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:36:12,960] ({pool-2-thread-20} SchedulerFactory.java[jobStarted]:114) - Job 20200121-185449_2110594246 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:36:12,965] ({pool-2-thread-20} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-185449_2110594246, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:36:14,347] ({pool-2-thread-20} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-185449_2110594246 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:36:14,365] ({pool-2-thread-20} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:36:14,379] ({pool-2-thread-20} SchedulerFactory.java[jobFinished]:120) - Job 20200121-185449_2110594246 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:36:14,810] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:36:14,830] ({pool-2-thread-11} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:36:14,840] ({pool-2-thread-11} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:36:14,940] ({pool-2-thread-11} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:36:14,947] ({pool-2-thread-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:36:14,960] ({pool-2-thread-11} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:36:41,321] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:37:19,428] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:37:30,945] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:37:50,366] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:38:02,065] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:38:34,208] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:39:24,517] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:39:40,295] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:42:09,226] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:42:09,243] ({pool-2-thread-21} SchedulerFactory.java[jobStarted]:114) - Job 20200121-185449_2110594246 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:42:09,244] ({pool-2-thread-21} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-185449_2110594246, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:42:10,482] ({pool-2-thread-21} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-185449_2110594246 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:42:10,494] ({pool-2-thread-21} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:42:10,506] ({pool-2-thread-21} SchedulerFactory.java[jobFinished]:120) - Job 20200121-185449_2110594246 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:42:43,876] ({qtp2107447833-12} NotebookServer.java[broadcastNewParagraph]:688) - Broadcasting paragraph on run call instead of note.
 INFO [2020-01-22 19:42:43,887] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:42:43,909] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200122-172305_1342549393 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:42:43,909] ({pool-2-thread-4} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-172305_1342549393, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 19:42:43,990] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-172305_1342549393 is finished, status: ERROR, exception: null, result: %text [0;36m  File [0;32m"<ipython-input-80-554b6a9bfde4>"[0;36m, line [0;32m7[0m
[0;31m    while !terminate:[0m
[0m          ^[0m
[0;31mSyntaxError[0m[0;31m:[0m invalid syntax

 INFO [2020-01-22 19:42:44,012] ({pool-2-thread-4} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:42:44,029] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200122-172305_1342549393 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:43:36,677] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:43:36,690] ({pool-2-thread-22} SchedulerFactory.java[jobStarted]:114) - Job 20200122-172305_1342549393 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:43:36,691] ({pool-2-thread-22} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-172305_1342549393, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 19:43:36,860] ({pool-2-thread-22} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-172305_1342549393 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mAnalysisException[0mTraceback (most recent call last)
[0;32m<ipython-input-82-c07ec637c963>[0m in [0;36m<module>[0;34m()[0m
[1;32m      1[0m [0;32mimport[0m [0mtime[0m[0;34m[0m[0m
[1;32m      2[0m [0;34m[0m[0m
[0;32m----> 3[0;31m [0mquery[0m [0;34m=[0m [0mdf[0m[0;34m.[0m[0mwriteStream[0m[0;34m.[0m[0moption[0m[0;34m([0m[0;34m"checkpointLocation"[0m[0;34m,[0m [0;34m"s3a://j2training/checkpoint/"[0m[0;34m)[0m             [0;34m.[0m[0mforeachBatch[0m[0;34m([0m[0mforeach_batch_function2[0m[0;34m)[0m             [0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m      4[0m [0;34m[0m[0m
[1;32m      5[0m [0mterminate[0m [0;34m=[0m [0mfalse[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py[0m in [0;36mwriteStream[0;34m(self)[0m
[1;32m    239[0m         [0;34m:[0m[0;32mreturn[0m[0;34m:[0m [0;34m:[0m[0;32mclass[0m[0;34m:[0m[0;34m`[0m[0mDataStreamWriter[0m[0;34m`[0m[0;34m[0m[0m
[1;32m    240[0m         """
[0;32m--> 241[0;31m         [0;32mreturn[0m [0mDataStreamWriter[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    242[0m [0;34m[0m[0m
[1;32m    243[0m     [0;34m@[0m[0mproperty[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py[0m in [0;36m__init__[0;34m(self, df)[0m
[1;32m    699[0m         [0mself[0m[0;34m.[0m[0m_df[0m [0;34m=[0m [0mdf[0m[0;34m[0m[0m
[1;32m    700[0m         [0mself[0m[0;34m.[0m[0m_spark[0m [0;34m=[0m [0mdf[0m[0;34m.[0m[0msql_ctx[0m[0;34m[0m[0m
[0;32m--> 701[0;31m         [0mself[0m[0;34m.[0m[0m_jwrite[0m [0;34m=[0m [0mdf[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mwriteStream[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    702[0m [0;34m[0m[0m
[1;32m    703[0m     [0;32mdef[0m [0m_sq[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mjsq[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     67[0m                                              e.java_exception.getStackTrace()))
[1;32m     68[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.AnalysisException: '[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 69[0;31m                 [0;32mraise[0m [0mAnalysisException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     70[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.catalyst.analysis'[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     71[0m                 [0;32mraise[0m [0mAnalysisException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m

[0;31mAnalysisException[0m: u"'writeStream' can be called only on streaming Dataset/DataFrame;"
 INFO [2020-01-22 19:43:36,879] ({pool-2-thread-22} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:43:36,898] ({pool-2-thread-22} SchedulerFactory.java[jobFinished]:120) - Job 20200122-172305_1342549393 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:43:53,017] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:43:53,438] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:43:53,449] ({pool-2-thread-12} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:43:53,454] ({pool-2-thread-12} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:43:53,545] ({pool-2-thread-12} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:43:53,561] ({pool-2-thread-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:43:53,572] ({pool-2-thread-12} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:43:54,960] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:43:54,978] ({pool-2-thread-23} SchedulerFactory.java[jobStarted]:114) - Job 20200122-172305_1342549393 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:43:54,979] ({pool-2-thread-23} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-172305_1342549393, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 19:43:55,205] ({pool-2-thread-23} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-172305_1342549393 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mAnalysisException[0mTraceback (most recent call last)
[0;32m<ipython-input-86-5d3340016f6a>[0m in [0;36m<module>[0;34m()[0m
[1;32m      1[0m [0;32mimport[0m [0mtime[0m[0;34m[0m[0m
[1;32m      2[0m [0;34m[0m[0m
[0;32m----> 3[0;31m [0mquery[0m [0;34m=[0m [0mdataDf[0m[0;34m.[0m[0mwriteStream[0m[0;34m.[0m[0moption[0m[0;34m([0m[0;34m"checkpointLocation"[0m[0;34m,[0m [0;34m"s3a://j2training/checkpoint/"[0m[0;34m)[0m             [0;34m.[0m[0mforeachBatch[0m[0;34m([0m[0mforeach_batch_function2[0m[0;34m)[0m             [0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m      4[0m [0;34m[0m[0m
[1;32m      5[0m [0mterminate[0m [0;34m=[0m [0mfalse[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py[0m in [0;36mstart[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)[0m
[1;32m   1103[0m             [0mself[0m[0;34m.[0m[0mqueryName[0m[0;34m([0m[0mqueryName[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1104[0m         [0;32mif[0m [0mpath[0m [0;32mis[0m [0mNone[0m[0;34m:[0m[0;34m[0m[0m
[0;32m-> 1105[0;31m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_sq[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jwrite[0m[0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1106[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m   1107[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_sq[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jwrite[0m[0;34m.[0m[0mstart[0m[0;34m([0m[0mpath[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     67[0m                                              e.java_exception.getStackTrace()))
[1;32m     68[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.AnalysisException: '[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 69[0;31m                 [0;32mraise[0m [0mAnalysisException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     70[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.catalyst.analysis'[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     71[0m                 [0;32mraise[0m [0mAnalysisException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m

[0;31mAnalysisException[0m: u'Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;;\nAggregate [source#1356, created_at#1285, lang#1302], [source#1356, created_at#1285, lang#1302, count(1) AS count#1364L]\n+- Project [<lambda>(source#1314) AS source#1356, created_at#1285, lang#1302]\n   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@6c5dbac8,json,List(),Some(StructType(StructField(contributors,StringType,true), StructField(coordinates,StructType(StructField(coordinates,ArrayType(DoubleType,true),true), StructField(type,StringType,true)),true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StructType(StructField(indices,ArrayType(LongType,true),true), StructField(text,StringType,true)),true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StructType(StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(name,StringType,true), StructField(screen_name,StringType,true)),true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(extended_tweet,StructType(StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StringType,true),true), StructField(user_mentions,ArrayType(StructType(StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(name,StringType,true), StructField(screen_name,StringType,true)),true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true)),true), StructField(full_text,StringType,true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StructType(StructField(coordinates,ArrayType(DoubleType,true),true), StructField(type,StringType,true)),true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,LongType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,LongType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StructType(StructField(bounding_box,StructType(StructField(coordinates,ArrayType(ArrayType(ArrayType(DoubleType,true),true),true),true), StructField(type,StringType,true)),true), StructField(country,StringType,true), StructField(country_code,StringType,true), StructField(full_name,StringType,true), StructField(id,StringType,true), StructField(name,StringType,true), StructField(place_type,StringType,true), StructField(url,StringType,true)),true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(quoted_status,StructType(StructField(contributors,StringType,true), StructField(coordinates,StringType,true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StringType,true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,StringType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,StringType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StringType,true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true)),true), StructField(quoted_status_id,LongType,true), StructField(quoted_status_id_str,StringType,true), StructField(quoted_status_permalink,StructType(StructField(display,StringType,true), StructField(expanded,StringType,true), StructField(url,StringType,true)),true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(retweeted_status,StructType(StructField(contributors,StringType,true), StructField(coordinates,StringType,true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StructType(StructField(indices,ArrayType(LongType,true),true), StructField(text,StringType,true)),true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StructType(StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(name,StringType,true), StructField(screen_name,StringType,true)),true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(extended_tweet,StructType(StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StringType,true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true)),true), StructField(full_text,StringType,true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,LongType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,LongType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StructType(StructField(bounding_box,StructType(StructField(coordinates,ArrayType(ArrayType(ArrayType(DoubleType,true),true),true),true), StructField(type,StringType,true)),true), StructField(country,StringType,true), StructField(country_code,StringType,true), StructField(full_name,StringType,true), StructField(id,StringType,true), StructField(name,StringType,true), StructField(place_type,StringType,true), StructField(url,StringType,true)),true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(quoted_status,StructType(StructField(contributors,StringType,true), StructField(coordinates,StringType,true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StringType,true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,StringType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,StringType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StringType,true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true)),true), StructField(quoted_status_id,LongType,true), StructField(quoted_status_id_str,StringType,true), StructField(quoted_status_permalink,StructType(StructField(display,StringType,true), StructField(expanded,StringType,true), StructField(url,StringType,true)),true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true)),true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(timestamp_ms,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true))),List(),None,Map(path -> s3a://j2training/data),None), FileSource[s3a://j2training/data], [contributors#1283, coordinates#1284, created_at#1285, display_text_range#1286, entities#1287, extended_entities#1288, extended_tweet#1289, favorite_count#1290L, favorited#1291, filter_level#1292, geo#1293, id#1294L, id_str#1295, in_reply_to_screen_name#1296, in_reply_to_status_id#1297L, in_reply_to_status_id_str#1298, in_reply_to_user_id#1299L, in_reply_to_user_id_str#1300, is_quote_status#1301, lang#1302, place#1303, possibly_sensitive#1304, quote_count#1305L, quoted_status#1306, ... 12 more fields]\n'
 INFO [2020-01-22 19:43:55,229] ({pool-2-thread-23} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:43:55,245] ({pool-2-thread-23} SchedulerFactory.java[jobFinished]:120) - Job 20200122-172305_1342549393 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:44:17,517] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:44:17,535] ({pool-2-thread-7} SchedulerFactory.java[jobStarted]:114) - Job 20200122-172305_1342549393 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:44:17,536] ({pool-2-thread-7} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-172305_1342549393, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 19:44:17,771] ({pool-2-thread-7} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-172305_1342549393 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mAnalysisException[0mTraceback (most recent call last)
[0;32m<ipython-input-88-5d3340016f6a>[0m in [0;36m<module>[0;34m()[0m
[1;32m      1[0m [0;32mimport[0m [0mtime[0m[0;34m[0m[0m
[1;32m      2[0m [0;34m[0m[0m
[0;32m----> 3[0;31m [0mquery[0m [0;34m=[0m [0mdataDf[0m[0;34m.[0m[0mwriteStream[0m[0;34m.[0m[0moption[0m[0;34m([0m[0;34m"checkpointLocation"[0m[0;34m,[0m [0;34m"s3a://j2training/checkpoint/"[0m[0;34m)[0m             [0;34m.[0m[0mforeachBatch[0m[0;34m([0m[0mforeach_batch_function2[0m[0;34m)[0m             [0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m      4[0m [0;34m[0m[0m
[1;32m      5[0m [0mterminate[0m [0;34m=[0m [0mfalse[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py[0m in [0;36mstart[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)[0m
[1;32m   1103[0m             [0mself[0m[0;34m.[0m[0mqueryName[0m[0;34m([0m[0mqueryName[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1104[0m         [0;32mif[0m [0mpath[0m [0;32mis[0m [0mNone[0m[0;34m:[0m[0;34m[0m[0m
[0;32m-> 1105[0;31m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_sq[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jwrite[0m[0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1106[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m   1107[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_sq[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jwrite[0m[0;34m.[0m[0mstart[0m[0;34m([0m[0mpath[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     67[0m                                              e.java_exception.getStackTrace()))
[1;32m     68[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.AnalysisException: '[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 69[0;31m                 [0;32mraise[0m [0mAnalysisException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     70[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.catalyst.analysis'[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     71[0m                 [0;32mraise[0m [0mAnalysisException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m

[0;31mAnalysisException[0m: u'Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;;\nAggregate [source#1356, created_at#1285, lang#1302], [source#1356, created_at#1285, lang#1302, count(1) AS count#1364L]\n+- Project [<lambda>(source#1314) AS source#1356, created_at#1285, lang#1302]\n   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@6c5dbac8,json,List(),Some(StructType(StructField(contributors,StringType,true), StructField(coordinates,StructType(StructField(coordinates,ArrayType(DoubleType,true),true), StructField(type,StringType,true)),true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StructType(StructField(indices,ArrayType(LongType,true),true), StructField(text,StringType,true)),true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StructType(StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(name,StringType,true), StructField(screen_name,StringType,true)),true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(extended_tweet,StructType(StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StringType,true),true), StructField(user_mentions,ArrayType(StructType(StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(name,StringType,true), StructField(screen_name,StringType,true)),true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true)),true), StructField(full_text,StringType,true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StructType(StructField(coordinates,ArrayType(DoubleType,true),true), StructField(type,StringType,true)),true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,LongType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,LongType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StructType(StructField(bounding_box,StructType(StructField(coordinates,ArrayType(ArrayType(ArrayType(DoubleType,true),true),true),true), StructField(type,StringType,true)),true), StructField(country,StringType,true), StructField(country_code,StringType,true), StructField(full_name,StringType,true), StructField(id,StringType,true), StructField(name,StringType,true), StructField(place_type,StringType,true), StructField(url,StringType,true)),true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(quoted_status,StructType(StructField(contributors,StringType,true), StructField(coordinates,StringType,true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StringType,true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,StringType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,StringType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StringType,true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true)),true), StructField(quoted_status_id,LongType,true), StructField(quoted_status_id_str,StringType,true), StructField(quoted_status_permalink,StructType(StructField(display,StringType,true), StructField(expanded,StringType,true), StructField(url,StringType,true)),true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(retweeted_status,StructType(StructField(contributors,StringType,true), StructField(coordinates,StringType,true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StructType(StructField(indices,ArrayType(LongType,true),true), StructField(text,StringType,true)),true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StructType(StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(name,StringType,true), StructField(screen_name,StringType,true)),true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(extended_tweet,StructType(StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StringType,true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true)),true), StructField(full_text,StringType,true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,LongType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,LongType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StructType(StructField(bounding_box,StructType(StructField(coordinates,ArrayType(ArrayType(ArrayType(DoubleType,true),true),true),true), StructField(type,StringType,true)),true), StructField(country,StringType,true), StructField(country_code,StringType,true), StructField(full_name,StringType,true), StructField(id,StringType,true), StructField(name,StringType,true), StructField(place_type,StringType,true), StructField(url,StringType,true)),true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(quoted_status,StructType(StructField(contributors,StringType,true), StructField(coordinates,StringType,true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StringType,true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,StringType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,StringType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StringType,true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true)),true), StructField(quoted_status_id,LongType,true), StructField(quoted_status_id_str,StringType,true), StructField(quoted_status_permalink,StructType(StructField(display,StringType,true), StructField(expanded,StringType,true), StructField(url,StringType,true)),true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true)),true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(timestamp_ms,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true))),List(),None,Map(path -> s3a://j2training/data),None), FileSource[s3a://j2training/data], [contributors#1283, coordinates#1284, created_at#1285, display_text_range#1286, entities#1287, extended_entities#1288, extended_tweet#1289, favorite_count#1290L, favorited#1291, filter_level#1292, geo#1293, id#1294L, id_str#1295, in_reply_to_screen_name#1296, in_reply_to_status_id#1297L, in_reply_to_status_id_str#1298, in_reply_to_user_id#1299L, in_reply_to_user_id_str#1300, is_quote_status#1301, lang#1302, place#1303, possibly_sensitive#1304, quote_count#1305L, quoted_status#1306, ... 12 more fields]\n'
 INFO [2020-01-22 19:44:17,782] ({pool-2-thread-7} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:44:17,800] ({pool-2-thread-7} SchedulerFactory.java[jobFinished]:120) - Job 20200122-172305_1342549393 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:44:30,579] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:44:33,217] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:44:35,215] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:44:35,226] ({qtp2107447833-11} NotebookServer.java[broadcastNewParagraph]:688) - Broadcasting paragraph on run call instead of note.
 INFO [2020-01-22 19:44:40,382] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:44:40,397] ({pool-2-thread-24} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:44:40,402] ({pool-2-thread-24} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 19:44:40,583] ({pool-2-thread-24} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mAnalysisException[0mTraceback (most recent call last)
[0;32m<ipython-input-90-5d3340016f6a>[0m in [0;36m<module>[0;34m()[0m
[1;32m      1[0m [0;32mimport[0m [0mtime[0m[0;34m[0m[0m
[1;32m      2[0m [0;34m[0m[0m
[0;32m----> 3[0;31m [0mquery[0m [0;34m=[0m [0mdataDf[0m[0;34m.[0m[0mwriteStream[0m[0;34m.[0m[0moption[0m[0;34m([0m[0;34m"checkpointLocation"[0m[0;34m,[0m [0;34m"s3a://j2training/checkpoint/"[0m[0;34m)[0m             [0;34m.[0m[0mforeachBatch[0m[0;34m([0m[0mforeach_batch_function2[0m[0;34m)[0m             [0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m      4[0m [0;34m[0m[0m
[1;32m      5[0m [0mterminate[0m [0;34m=[0m [0mfalse[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py[0m in [0;36mstart[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)[0m
[1;32m   1103[0m             [0mself[0m[0;34m.[0m[0mqueryName[0m[0;34m([0m[0mqueryName[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1104[0m         [0;32mif[0m [0mpath[0m [0;32mis[0m [0mNone[0m[0;34m:[0m[0;34m[0m[0m
[0;32m-> 1105[0;31m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_sq[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jwrite[0m[0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1106[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m   1107[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_sq[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jwrite[0m[0;34m.[0m[0mstart[0m[0;34m([0m[0mpath[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     67[0m                                              e.java_exception.getStackTrace()))
[1;32m     68[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.AnalysisException: '[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 69[0;31m                 [0;32mraise[0m [0mAnalysisException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     70[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.catalyst.analysis'[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     71[0m                 [0;32mraise[0m [0mAnalysisException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m

[0;31mAnalysisException[0m: u'Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;;\nAggregate [source#1356, created_at#1285, lang#1302], [source#1356, created_at#1285, lang#1302, count(1) AS count#1364L]\n+- Project [<lambda>(source#1314) AS source#1356, created_at#1285, lang#1302]\n   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@6c5dbac8,json,List(),Some(StructType(StructField(contributors,StringType,true), StructField(coordinates,StructType(StructField(coordinates,ArrayType(DoubleType,true),true), StructField(type,StringType,true)),true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StructType(StructField(indices,ArrayType(LongType,true),true), StructField(text,StringType,true)),true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StructType(StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(name,StringType,true), StructField(screen_name,StringType,true)),true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(extended_tweet,StructType(StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StringType,true),true), StructField(user_mentions,ArrayType(StructType(StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(name,StringType,true), StructField(screen_name,StringType,true)),true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true)),true), StructField(full_text,StringType,true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StructType(StructField(coordinates,ArrayType(DoubleType,true),true), StructField(type,StringType,true)),true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,LongType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,LongType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StructType(StructField(bounding_box,StructType(StructField(coordinates,ArrayType(ArrayType(ArrayType(DoubleType,true),true),true),true), StructField(type,StringType,true)),true), StructField(country,StringType,true), StructField(country_code,StringType,true), StructField(full_name,StringType,true), StructField(id,StringType,true), StructField(name,StringType,true), StructField(place_type,StringType,true), StructField(url,StringType,true)),true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(quoted_status,StructType(StructField(contributors,StringType,true), StructField(coordinates,StringType,true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StringType,true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,StringType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,StringType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StringType,true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true)),true), StructField(quoted_status_id,LongType,true), StructField(quoted_status_id_str,StringType,true), StructField(quoted_status_permalink,StructType(StructField(display,StringType,true), StructField(expanded,StringType,true), StructField(url,StringType,true)),true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(retweeted_status,StructType(StructField(contributors,StringType,true), StructField(coordinates,StringType,true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StructType(StructField(indices,ArrayType(LongType,true),true), StructField(text,StringType,true)),true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StructType(StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(name,StringType,true), StructField(screen_name,StringType,true)),true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(extended_tweet,StructType(StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StringType,true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true)),true), StructField(full_text,StringType,true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,LongType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,LongType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StructType(StructField(bounding_box,StructType(StructField(coordinates,ArrayType(ArrayType(ArrayType(DoubleType,true),true),true),true), StructField(type,StringType,true)),true), StructField(country,StringType,true), StructField(country_code,StringType,true), StructField(full_name,StringType,true), StructField(id,StringType,true), StructField(name,StringType,true), StructField(place_type,StringType,true), StructField(url,StringType,true)),true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(quoted_status,StructType(StructField(contributors,StringType,true), StructField(coordinates,StringType,true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StringType,true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,StringType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,StringType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StringType,true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true)),true), StructField(quoted_status_id,LongType,true), StructField(quoted_status_id_str,StringType,true), StructField(quoted_status_permalink,StructType(StructField(display,StringType,true), StructField(expanded,StringType,true), StructField(url,StringType,true)),true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true)),true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(timestamp_ms,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true))),List(),None,Map(path -> s3a://j2training/data),None), FileSource[s3a://j2training/data], [contributors#1283, coordinates#1284, created_at#1285, display_text_range#1286, entities#1287, extended_entities#1288, extended_tweet#1289, favorite_count#1290L, favorited#1291, filter_level#1292, geo#1293, id#1294L, id_str#1295, in_reply_to_screen_name#1296, in_reply_to_status_id#1297L, in_reply_to_status_id_str#1298, in_reply_to_user_id#1299L, in_reply_to_user_id_str#1300, is_quote_status#1301, lang#1302, place#1303, possibly_sensitive#1304, quote_count#1305L, quoted_status#1306, ... 12 more fields]\n'
 INFO [2020-01-22 19:44:40,604] ({pool-2-thread-24} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:44:40,619] ({pool-2-thread-24} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:45:00,725] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:45:00,744] ({pool-2-thread-13} SchedulerFactory.java[jobStarted]:114) - Job 20200121-185449_2110594246 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:45:00,744] ({pool-2-thread-13} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-185449_2110594246, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:45:01,966] ({pool-2-thread-13} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-185449_2110594246 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:45:01,984] ({pool-2-thread-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:45:01,996] ({pool-2-thread-13} SchedulerFactory.java[jobFinished]:120) - Job 20200121-185449_2110594246 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:45:03,377] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:45:03,391] ({pool-2-thread-25} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:45:03,392] ({pool-2-thread-25} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:45:03,502] ({pool-2-thread-25} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:45:03,525] ({pool-2-thread-25} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:45:03,541] ({pool-2-thread-25} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:45:06,045] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:45:06,060] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:45:06,060] ({pool-2-thread-3} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 19:45:06,250] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mAnalysisException[0mTraceback (most recent call last)
[0;32m<ipython-input-96-5d3340016f6a>[0m in [0;36m<module>[0;34m()[0m
[1;32m      1[0m [0;32mimport[0m [0mtime[0m[0;34m[0m[0m
[1;32m      2[0m [0;34m[0m[0m
[0;32m----> 3[0;31m [0mquery[0m [0;34m=[0m [0mdataDf[0m[0;34m.[0m[0mwriteStream[0m[0;34m.[0m[0moption[0m[0;34m([0m[0;34m"checkpointLocation"[0m[0;34m,[0m [0;34m"s3a://j2training/checkpoint/"[0m[0;34m)[0m             [0;34m.[0m[0mforeachBatch[0m[0;34m([0m[0mforeach_batch_function2[0m[0;34m)[0m             [0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m      4[0m [0;34m[0m[0m
[1;32m      5[0m [0mterminate[0m [0;34m=[0m [0mfalse[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py[0m in [0;36mstart[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)[0m
[1;32m   1103[0m             [0mself[0m[0;34m.[0m[0mqueryName[0m[0;34m([0m[0mqueryName[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1104[0m         [0;32mif[0m [0mpath[0m [0;32mis[0m [0mNone[0m[0;34m:[0m[0;34m[0m[0m
[0;32m-> 1105[0;31m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_sq[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jwrite[0m[0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1106[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m   1107[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_sq[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jwrite[0m[0;34m.[0m[0mstart[0m[0;34m([0m[0mpath[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     67[0m                                              e.java_exception.getStackTrace()))
[1;32m     68[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.AnalysisException: '[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 69[0;31m                 [0;32mraise[0m [0mAnalysisException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     70[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.catalyst.analysis'[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     71[0m                 [0;32mraise[0m [0mAnalysisException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m

[0;31mAnalysisException[0m: u'Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;;\nAggregate [source#1454, created_at#1383, lang#1400], [source#1454, created_at#1383, lang#1400, count(1) AS count#1462L]\n+- Project [<lambda>(source#1412) AS source#1454, created_at#1383, lang#1400]\n   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@6c5dbac8,json,List(),Some(StructType(StructField(contributors,StringType,true), StructField(coordinates,StructType(StructField(coordinates,ArrayType(DoubleType,true),true), StructField(type,StringType,true)),true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StructType(StructField(indices,ArrayType(LongType,true),true), StructField(text,StringType,true)),true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StructType(StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(name,StringType,true), StructField(screen_name,StringType,true)),true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(extended_tweet,StructType(StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StringType,true),true), StructField(user_mentions,ArrayType(StructType(StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(name,StringType,true), StructField(screen_name,StringType,true)),true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true)),true), StructField(full_text,StringType,true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StructType(StructField(coordinates,ArrayType(DoubleType,true),true), StructField(type,StringType,true)),true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,LongType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,LongType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StructType(StructField(bounding_box,StructType(StructField(coordinates,ArrayType(ArrayType(ArrayType(DoubleType,true),true),true),true), StructField(type,StringType,true)),true), StructField(country,StringType,true), StructField(country_code,StringType,true), StructField(full_name,StringType,true), StructField(id,StringType,true), StructField(name,StringType,true), StructField(place_type,StringType,true), StructField(url,StringType,true)),true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(quoted_status,StructType(StructField(contributors,StringType,true), StructField(coordinates,StringType,true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StringType,true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,StringType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,StringType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StringType,true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true)),true), StructField(quoted_status_id,LongType,true), StructField(quoted_status_id_str,StringType,true), StructField(quoted_status_permalink,StructType(StructField(display,StringType,true), StructField(expanded,StringType,true), StructField(url,StringType,true)),true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(retweeted_status,StructType(StructField(contributors,StringType,true), StructField(coordinates,StringType,true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StructType(StructField(indices,ArrayType(LongType,true),true), StructField(text,StringType,true)),true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StructType(StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(name,StringType,true), StructField(screen_name,StringType,true)),true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(extended_tweet,StructType(StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StringType,true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true)),true), StructField(full_text,StringType,true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,LongType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,LongType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StructType(StructField(bounding_box,StructType(StructField(coordinates,ArrayType(ArrayType(ArrayType(DoubleType,true),true),true),true), StructField(type,StringType,true)),true), StructField(country,StringType,true), StructField(country_code,StringType,true), StructField(full_name,StringType,true), StructField(id,StringType,true), StructField(name,StringType,true), StructField(place_type,StringType,true), StructField(url,StringType,true)),true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(quoted_status,StructType(StructField(contributors,StringType,true), StructField(coordinates,StringType,true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StringType,true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,StringType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,StringType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StringType,true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true)),true), StructField(quoted_status_id,LongType,true), StructField(quoted_status_id_str,StringType,true), StructField(quoted_status_permalink,StructType(StructField(display,StringType,true), StructField(expanded,StringType,true), StructField(url,StringType,true)),true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true)),true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(timestamp_ms,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true))),List(),None,Map(path -> s3a://j2training/data),None), FileSource[s3a://j2training/data], [contributors#1381, coordinates#1382, created_at#1383, display_text_range#1384, entities#1385, extended_entities#1386, extended_tweet#1387, favorite_count#1388L, favorited#1389, filter_level#1390, geo#1391, id#1392L, id_str#1393, in_reply_to_screen_name#1394, in_reply_to_status_id#1395L, in_reply_to_status_id_str#1396, in_reply_to_user_id#1397L, in_reply_to_user_id_str#1398, is_quote_status#1399, lang#1400, place#1401, possibly_sensitive#1402, quote_count#1403L, quoted_status#1404, ... 12 more fields]\n'
 INFO [2020-01-22 19:45:06,264] ({pool-2-thread-3} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:45:06,277] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:46:02,600] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:46:02,626] ({pool-2-thread-26} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:46:02,626] ({pool-2-thread-26} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 19:46:02,818] ({pool-2-thread-26} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mAnalysisException[0mTraceback (most recent call last)
[0;32m<ipython-input-98-5d3340016f6a>[0m in [0;36m<module>[0;34m()[0m
[1;32m      1[0m [0;32mimport[0m [0mtime[0m[0;34m[0m[0m
[1;32m      2[0m [0;34m[0m[0m
[0;32m----> 3[0;31m [0mquery[0m [0;34m=[0m [0mdataDf[0m[0;34m.[0m[0mwriteStream[0m[0;34m.[0m[0moption[0m[0;34m([0m[0;34m"checkpointLocation"[0m[0;34m,[0m [0;34m"s3a://j2training/checkpoint/"[0m[0;34m)[0m             [0;34m.[0m[0mforeachBatch[0m[0;34m([0m[0mforeach_batch_function2[0m[0;34m)[0m             [0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m      4[0m [0;34m[0m[0m
[1;32m      5[0m [0mterminate[0m [0;34m=[0m [0mfalse[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py[0m in [0;36mstart[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)[0m
[1;32m   1103[0m             [0mself[0m[0;34m.[0m[0mqueryName[0m[0;34m([0m[0mqueryName[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1104[0m         [0;32mif[0m [0mpath[0m [0;32mis[0m [0mNone[0m[0;34m:[0m[0;34m[0m[0m
[0;32m-> 1105[0;31m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_sq[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jwrite[0m[0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1106[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m   1107[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_sq[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jwrite[0m[0;34m.[0m[0mstart[0m[0;34m([0m[0mpath[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     67[0m                                              e.java_exception.getStackTrace()))
[1;32m     68[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.AnalysisException: '[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 69[0;31m                 [0;32mraise[0m [0mAnalysisException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     70[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.catalyst.analysis'[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     71[0m                 [0;32mraise[0m [0mAnalysisException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m

[0;31mAnalysisException[0m: u'Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;;\nAggregate [source#1454, created_at#1383, lang#1400], [source#1454, created_at#1383, lang#1400, count(1) AS count#1462L]\n+- Project [<lambda>(source#1412) AS source#1454, created_at#1383, lang#1400]\n   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@6c5dbac8,json,List(),Some(StructType(StructField(contributors,StringType,true), StructField(coordinates,StructType(StructField(coordinates,ArrayType(DoubleType,true),true), StructField(type,StringType,true)),true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StructType(StructField(indices,ArrayType(LongType,true),true), StructField(text,StringType,true)),true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StructType(StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(name,StringType,true), StructField(screen_name,StringType,true)),true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(extended_tweet,StructType(StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StringType,true),true), StructField(user_mentions,ArrayType(StructType(StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(name,StringType,true), StructField(screen_name,StringType,true)),true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true)),true), StructField(full_text,StringType,true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StructType(StructField(coordinates,ArrayType(DoubleType,true),true), StructField(type,StringType,true)),true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,LongType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,LongType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StructType(StructField(bounding_box,StructType(StructField(coordinates,ArrayType(ArrayType(ArrayType(DoubleType,true),true),true),true), StructField(type,StringType,true)),true), StructField(country,StringType,true), StructField(country_code,StringType,true), StructField(full_name,StringType,true), StructField(id,StringType,true), StructField(name,StringType,true), StructField(place_type,StringType,true), StructField(url,StringType,true)),true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(quoted_status,StructType(StructField(contributors,StringType,true), StructField(coordinates,StringType,true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StringType,true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,StringType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,StringType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StringType,true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true)),true), StructField(quoted_status_id,LongType,true), StructField(quoted_status_id_str,StringType,true), StructField(quoted_status_permalink,StructType(StructField(display,StringType,true), StructField(expanded,StringType,true), StructField(url,StringType,true)),true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(retweeted_status,StructType(StructField(contributors,StringType,true), StructField(coordinates,StringType,true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StructType(StructField(indices,ArrayType(LongType,true),true), StructField(text,StringType,true)),true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StructType(StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(name,StringType,true), StructField(screen_name,StringType,true)),true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(extended_tweet,StructType(StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StringType,true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true)),true), StructField(full_text,StringType,true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,LongType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,LongType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StructType(StructField(bounding_box,StructType(StructField(coordinates,ArrayType(ArrayType(ArrayType(DoubleType,true),true),true),true), StructField(type,StringType,true)),true), StructField(country,StringType,true), StructField(country_code,StringType,true), StructField(full_name,StringType,true), StructField(id,StringType,true), StructField(name,StringType,true), StructField(place_type,StringType,true), StructField(url,StringType,true)),true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(quoted_status,StructType(StructField(contributors,StringType,true), StructField(coordinates,StringType,true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StringType,true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,StringType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,StringType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StringType,true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true)),true), StructField(quoted_status_id,LongType,true), StructField(quoted_status_id_str,StringType,true), StructField(quoted_status_permalink,StructType(StructField(display,StringType,true), StructField(expanded,StringType,true), StructField(url,StringType,true)),true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true)),true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(timestamp_ms,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true))),List(),None,Map(path -> s3a://j2training/data),None), FileSource[s3a://j2training/data], [contributors#1381, coordinates#1382, created_at#1383, display_text_range#1384, entities#1385, extended_entities#1386, extended_tweet#1387, favorite_count#1388L, favorited#1389, filter_level#1390, geo#1391, id#1392L, id_str#1393, in_reply_to_screen_name#1394, in_reply_to_status_id#1395L, in_reply_to_status_id_str#1396, in_reply_to_user_id#1397L, in_reply_to_user_id_str#1398, is_quote_status#1399, lang#1400, place#1401, possibly_sensitive#1402, quote_count#1403L, quoted_status#1404, ... 12 more fields]\n'
 INFO [2020-01-22 19:46:02,835] ({pool-2-thread-26} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:46:02,852] ({pool-2-thread-26} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:47:12,074] ({qtp2107447833-13} InterpreterRestApi.java[restartSetting]:180) - Restart interpreterSetting spark, msg=
 INFO [2020-01-22 19:47:12,076] ({qtp2107447833-13} InterpreterSetting.java[close]:483) - Close InterpreterSetting: spark
 INFO [2020-01-22 19:47:12,076] ({qtp2107447833-13} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: spark:shared_process
 INFO [2020-01-22 19:47:12,076] ({qtp2107447833-13} ManagedInterpreterGroup.java[close]:100) - Close Session: shared_session for interpreter setting: spark
 WARN [2020-01-22 19:47:12,077] ({qtp2107447833-13} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.SparkInterpreter
 WARN [2020-01-22 19:47:12,077] ({qtp2107447833-13} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.SparkSqlInterpreter
 WARN [2020-01-22 19:47:12,503] ({qtp2107447833-13} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.IPySparkInterpreter
 WARN [2020-01-22 19:47:12,504] ({qtp2107447833-13} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.SparkRInterpreter
 INFO [2020-01-22 19:47:12,506] ({qtp2107447833-13} ManagedInterpreterGroup.java[close]:105) - Remove this InterpreterGroup: spark:shared_process as all the sessions are closed
 INFO [2020-01-22 19:47:12,507] ({qtp2107447833-13} ManagedInterpreterGroup.java[close]:108) - Kill RemoteInterpreterProcess
 INFO [2020-01-22 19:47:12,511] ({qtp2107447833-13} RemoteInterpreterManagedProcess.java[stop]:220) - Kill interpreter process
ERROR [2020-01-22 19:47:12,877] ({Thread-18} RemoteInterpreterEventPoller.java[run]:257) - Can not get RemoteInterpreterEvent because it is shutdown.
ERROR [2020-01-22 19:47:12,878] ({pool-6-thread-1} AppendOutputRunner.java[run]:68) - Wait for OutputBuffer queue interrupted: null
 WARN [2020-01-22 19:47:15,043] ({qtp2107447833-13} RemoteInterpreterManagedProcess.java[stop]:230) - ignore the exception when shutting down
 INFO [2020-01-22 19:47:15,044] ({qtp2107447833-13} RemoteInterpreterManagedProcess.java[stop]:238) - Remote process terminated
 INFO [2020-01-22 19:47:15,051] ({Exec Default Executor} RemoteInterpreterManagedProcess.java[onProcessFailed]:250) - Interpreter process failed {}
org.apache.commons.exec.ExecuteException: Process exited with an error: 143 (Exit value: 143)
	at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:404)
	at org.apache.commons.exec.DefaultExecutor.access$200(DefaultExecutor.java:48)
	at org.apache.commons.exec.DefaultExecutor$1.run(DefaultExecutor.java:200)
	at java.lang.Thread.run(Thread.java:748)
 INFO [2020-01-22 19:47:18,958] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:47:18,989] ({qtp2107447833-12} InterpreterSetting.java[getOrCreateInterpreterGroup]:419) - Create InterpreterGroup with groupId: spark:shared_process for user: anonymous and note: 2EXQEGYMG
 INFO [2020-01-22 19:47:18,989] ({qtp2107447833-12} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.SparkInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 19:47:18,990] ({qtp2107447833-12} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.SparkSqlInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 19:47:18,990] ({qtp2107447833-12} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.DepInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 19:47:18,990] ({qtp2107447833-12} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.PySparkInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 19:47:18,990] ({qtp2107447833-12} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.IPySparkInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 19:47:18,990] ({qtp2107447833-12} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.SparkRInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 19:47:18,991] ({qtp2107447833-12} ManagedInterpreterGroup.java[getOrCreateSession]:158) - Create Session: shared_session in InterpreterGroup: spark:shared_process for user: anonymous
 INFO [2020-01-22 19:47:18,993] ({pool-2-thread-30} SchedulerFactory.java[jobStarted]:114) - Job 20200121-185651_2144996466 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:47:18,994] ({pool-2-thread-30} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-185651_2144996466, interpreter: dep, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:47:18,995] ({pool-2-thread-30} ManagedInterpreterGroup.java[getOrCreateInterpreterProcess]:61) - Create InterpreterProcess for InterpreterGroup: spark:shared_process
 INFO [2020-01-22 19:47:18,995] ({pool-2-thread-30} ShellScriptLauncher.java[launch]:48) - Launching Interpreter: spark
 INFO [2020-01-22 19:47:18,996] ({pool-2-thread-30} SparkInterpreterLauncher.java[buildEnvFromProperties]:108) - Run Spark under non-secure mode as no keytab and principal is specified
 INFO [2020-01-22 19:47:18,997] ({pool-2-thread-30} RemoteInterpreterManagedProcess.java[start]:115) - Thrift server for callback will start. Port: 38775
 INFO [2020-01-22 19:47:19,498] ({pool-2-thread-30} RemoteInterpreterManagedProcess.java[start]:190) - Run interpreter process [/zeppelin/bin/interpreter.sh, -d, /zeppelin/interpreter/spark, -c, 172.19.0.3, -p, 38775, -r, :, -l, /usr/local/local-repo/spark, -g, spark]
 INFO [2020-01-22 19:47:22,348] ({pool-9-thread-1} RemoteInterpreterManagedProcess.java[callback]:123) - RemoteInterpreterServer Registered: CallbackInfo(host:172.19.0.3, port:37309)
 INFO [2020-01-22 19:47:22,356] ({pool-2-thread-30} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkInterpreter
 INFO [2020-01-22 19:47:22,432] ({pool-2-thread-30} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkSqlInterpreter
 INFO [2020-01-22 19:47:22,434] ({pool-2-thread-30} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.DepInterpreter
 INFO [2020-01-22 19:47:22,436] ({pool-2-thread-30} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.PySparkInterpreter
 INFO [2020-01-22 19:47:22,439] ({pool-2-thread-30} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.IPySparkInterpreter
 INFO [2020-01-22 19:47:22,443] ({pool-2-thread-30} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkRInterpreter
 INFO [2020-01-22 19:47:22,445] ({pool-2-thread-30} RemoteInterpreter.java[call]:142) - Open RemoteInterpreter org.apache.zeppelin.spark.DepInterpreter
 INFO [2020-01-22 19:47:22,445] ({pool-2-thread-30} RemoteInterpreter.java[pushAngularObjectRegistryToRemote]:436) - Push local angular object registry from ZeppelinServer to remote interpreter group spark:shared_process
 INFO [2020-01-22 19:47:30,747] ({pool-2-thread-30} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-185651_2144996466 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:47:30,768] ({pool-2-thread-30} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:47:30,802] ({pool-2-thread-30} SchedulerFactory.java[jobFinished]:120) - Job 20200121-185651_2144996466 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:47:45,456] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:47:45,505] ({pool-2-thread-16} SchedulerFactory.java[jobStarted]:114) - Job 20200121-183200_1639671760 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:47:45,510] ({pool-2-thread-16} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-183200_1639671760, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:47:45,526] ({pool-2-thread-16} RemoteInterpreter.java[call]:142) - Open RemoteInterpreter org.apache.zeppelin.spark.PySparkInterpreter
 INFO [2020-01-22 19:47:48,514] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:47:48,618] ({pool-2-thread-31} SchedulerFactory.java[jobStarted]:114) - Job 20200121-183236_977276561 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:47:48,633] ({pool-2-thread-31} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-183236_977276561, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:47:50,736] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:47:54,909] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:47:58,217] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:48:00,519] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:48:00,758] ({pool-2-thread-16} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-183200_1639671760 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:48:00,779] ({pool-2-thread-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:48:00,795] ({pool-2-thread-16} SchedulerFactory.java[jobFinished]:120) - Job 20200121-183200_1639671760 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:48:01,311] ({pool-2-thread-9} SchedulerFactory.java[jobStarted]:114) - Job 20200122-174128_1762695518 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:48:01,324] ({pool-2-thread-9} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-174128_1762695518, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:48:13,728] ({pool-2-thread-31} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-183236_977276561 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:48:13,775] ({pool-2-thread-31} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:48:13,810] ({pool-2-thread-31} SchedulerFactory.java[jobFinished]:120) - Job 20200121-183236_977276561 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:48:14,219] ({pool-2-thread-9} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-174128_1762695518 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:48:14,238] ({pool-2-thread-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:48:14,274] ({pool-2-thread-9} SchedulerFactory.java[jobFinished]:120) - Job 20200122-174128_1762695518 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:48:14,292] ({pool-2-thread-32} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:48:14,300] ({pool-2-thread-32} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:48:14,795] ({pool-2-thread-17} SchedulerFactory.java[jobStarted]:114) - Job 20200121-185449_2110594246 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:48:14,799] ({pool-2-thread-17} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-185449_2110594246, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:48:24,253] ({pool-2-thread-32} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-173241_479707786 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:48:24,321] ({pool-2-thread-32} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:48:24,369] ({pool-2-thread-32} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:48:24,906] ({pool-2-thread-33} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:48:24,915] ({pool-2-thread-33} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:48:26,383] ({pool-2-thread-17} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-185449_2110594246 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:48:26,396] ({pool-2-thread-17} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:48:26,411] ({pool-2-thread-17} SchedulerFactory.java[jobFinished]:120) - Job 20200121-185449_2110594246 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:48:26,963] ({pool-2-thread-33} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:48:26,977] ({pool-2-thread-33} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:48:26,991] ({pool-2-thread-33} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:48:46,466] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:48:46,487] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:48:46,489] ({pool-2-thread-2} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 19:48:46,879] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mAnalysisException[0mTraceback (most recent call last)
[0;32m<ipython-input-16-5d3340016f6a>[0m in [0;36m<module>[0;34m()[0m
[1;32m      1[0m [0;32mimport[0m [0mtime[0m[0;34m[0m[0m
[1;32m      2[0m [0;34m[0m[0m
[0;32m----> 3[0;31m [0mquery[0m [0;34m=[0m [0mdataDf[0m[0;34m.[0m[0mwriteStream[0m[0;34m.[0m[0moption[0m[0;34m([0m[0;34m"checkpointLocation"[0m[0;34m,[0m [0;34m"s3a://j2training/checkpoint/"[0m[0;34m)[0m             [0;34m.[0m[0mforeachBatch[0m[0;34m([0m[0mforeach_batch_function2[0m[0;34m)[0m             [0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m      4[0m [0;34m[0m[0m
[1;32m      5[0m [0mterminate[0m [0;34m=[0m [0mfalse[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py[0m in [0;36mstart[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)[0m
[1;32m   1103[0m             [0mself[0m[0;34m.[0m[0mqueryName[0m[0;34m([0m[0mqueryName[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1104[0m         [0;32mif[0m [0mpath[0m [0;32mis[0m [0mNone[0m[0;34m:[0m[0;34m[0m[0m
[0;32m-> 1105[0;31m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_sq[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jwrite[0m[0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1106[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m   1107[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_sq[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jwrite[0m[0;34m.[0m[0mstart[0m[0;34m([0m[0mpath[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     67[0m                                              e.java_exception.getStackTrace()))
[1;32m     68[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.AnalysisException: '[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 69[0;31m                 [0;32mraise[0m [0mAnalysisException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     70[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.catalyst.analysis'[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     71[0m                 [0;32mraise[0m [0mAnalysisException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m

[0;31mAnalysisException[0m: u'Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;;\nAggregate [source#184, created_at#113, lang#130], [source#184, created_at#113, lang#130, count(1) AS count#192L]\n+- Project [<lambda>(source#142) AS source#184, created_at#113, lang#130]\n   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@69fce4cf,json,List(),Some(StructType(StructField(contributors,StringType,true), StructField(coordinates,StructType(StructField(coordinates,ArrayType(DoubleType,true),true), StructField(type,StringType,true)),true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StructType(StructField(indices,ArrayType(LongType,true),true), StructField(text,StringType,true)),true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StructType(StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(name,StringType,true), StructField(screen_name,StringType,true)),true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(extended_tweet,StructType(StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StringType,true),true), StructField(user_mentions,ArrayType(StructType(StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(name,StringType,true), StructField(screen_name,StringType,true)),true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true)),true), StructField(full_text,StringType,true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StructType(StructField(coordinates,ArrayType(DoubleType,true),true), StructField(type,StringType,true)),true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,LongType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,LongType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StructType(StructField(bounding_box,StructType(StructField(coordinates,ArrayType(ArrayType(ArrayType(DoubleType,true),true),true),true), StructField(type,StringType,true)),true), StructField(country,StringType,true), StructField(country_code,StringType,true), StructField(full_name,StringType,true), StructField(id,StringType,true), StructField(name,StringType,true), StructField(place_type,StringType,true), StructField(url,StringType,true)),true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(quoted_status,StructType(StructField(contributors,StringType,true), StructField(coordinates,StringType,true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StringType,true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,StringType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,StringType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StringType,true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true)),true), StructField(quoted_status_id,LongType,true), StructField(quoted_status_id_str,StringType,true), StructField(quoted_status_permalink,StructType(StructField(display,StringType,true), StructField(expanded,StringType,true), StructField(url,StringType,true)),true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(retweeted_status,StructType(StructField(contributors,StringType,true), StructField(coordinates,StringType,true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StructType(StructField(indices,ArrayType(LongType,true),true), StructField(text,StringType,true)),true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StructType(StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(name,StringType,true), StructField(screen_name,StringType,true)),true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(extended_tweet,StructType(StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StringType,true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true)),true), StructField(full_text,StringType,true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,LongType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,LongType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StructType(StructField(bounding_box,StructType(StructField(coordinates,ArrayType(ArrayType(ArrayType(DoubleType,true),true),true),true), StructField(type,StringType,true)),true), StructField(country,StringType,true), StructField(country_code,StringType,true), StructField(full_name,StringType,true), StructField(id,StringType,true), StructField(name,StringType,true), StructField(place_type,StringType,true), StructField(url,StringType,true)),true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(quoted_status,StructType(StructField(contributors,StringType,true), StructField(coordinates,StringType,true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StringType,true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,StringType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,StringType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StringType,true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true)),true), StructField(quoted_status_id,LongType,true), StructField(quoted_status_id_str,StringType,true), StructField(quoted_status_permalink,StructType(StructField(display,StringType,true), StructField(expanded,StringType,true), StructField(url,StringType,true)),true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true)),true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(timestamp_ms,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true))),List(),None,Map(path -> s3a://j2training/data),None), FileSource[s3a://j2training/data], [contributors#111, coordinates#112, created_at#113, display_text_range#114, entities#115, extended_entities#116, extended_tweet#117, favorite_count#118L, favorited#119, filter_level#120, geo#121, id#122L, id_str#123, in_reply_to_screen_name#124, in_reply_to_status_id#125L, in_reply_to_status_id_str#126, in_reply_to_user_id#127L, in_reply_to_user_id_str#128, is_quote_status#129, lang#130, place#131, possibly_sensitive#132, quote_count#133L, quoted_status#134, ... 12 more fields]\n'
 INFO [2020-01-22 19:48:46,892] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:48:46,912] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:49:39,869] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:50:16,371] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:50:16,387] ({pool-2-thread-34} SchedulerFactory.java[jobStarted]:114) - Job 20200121-185449_2110594246 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:50:16,388] ({pool-2-thread-34} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-185449_2110594246, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 19:50:18,087] ({pool-2-thread-34} NotebookServer.java[afterStatusChange]:2316) - Job 20200121-185449_2110594246 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mNameError[0mTraceback (most recent call last)
[0;32m<ipython-input-18-03249b0cbfed>[0m in [0;36m<module>[0;34m()[0m
[1;32m      1[0m [0;32mfrom[0m [0mdatetime[0m [0;32mimport[0m [0mdatetime[0m[0;34m[0m[0m
[0;32m----> 2[0;31m [0mdataDf[0m [0;34m=[0m [0mspark[0m[0;34m.[0m[0mreadStream[0m[0;34m.[0m[0mjson[0m[0;34m([0m[0;34m"s3a://j2training/data"[0m[0;34m,[0m[0mdf[0m[0;34m.[0m[0mschema[0m[0;34m)[0m[0;34m.[0m[0mselect[0m[0;34m([0m[0msourceExtractor[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m,[0m[0;34m"created_at"[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mgroupBy[0m[0;34m([0m[0;34m"source"[0m[0;34m,[0m[0;34m"created_at"[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mcount[0m[0;34m([0m[0;34m)[0m[0;34m.[0m[0mwithColumn[0m[0;34m([0m[0;34m"timestamp"[0m[0;34m,[0m [0mlit[0m[0;34m([0m[0mdatetime[0m[0;34m.[0m[0mnow[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;31mNameError[0m: name 'lit' is not defined
 INFO [2020-01-22 19:50:18,108] ({pool-2-thread-34} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:50:18,124] ({pool-2-thread-34} SchedulerFactory.java[jobFinished]:120) - Job 20200121-185449_2110594246 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:50:29,815] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:50:29,828] ({pool-2-thread-18} SchedulerFactory.java[jobStarted]:114) - Job 20200121-185449_2110594246 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:50:29,830] ({pool-2-thread-18} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-185449_2110594246, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 19:50:31,281] ({pool-2-thread-18} NotebookServer.java[afterStatusChange]:2316) - Job 20200121-185449_2110594246 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mNameError[0mTraceback (most recent call last)
[0;32m<ipython-input-20-cd4b433291e5>[0m in [0;36m<module>[0;34m()[0m
[1;32m      2[0m [0;32mfrom[0m [0mpyspark[0m[0;34m.[0m[0msql[0m[0;34m.[0m[0mfunctions[0m [0;32mimport[0m [0mudf[0m[0;34m[0m[0m
[1;32m      3[0m [0;34m[0m[0m
[0;32m----> 4[0;31m [0mdataDf[0m [0;34m=[0m [0mspark[0m[0;34m.[0m[0mreadStream[0m[0;34m.[0m[0mjson[0m[0;34m([0m[0;34m"s3a://j2training/data"[0m[0;34m,[0m[0mdf[0m[0;34m.[0m[0mschema[0m[0;34m)[0m[0;34m.[0m[0mselect[0m[0;34m([0m[0msourceExtractor[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m,[0m[0;34m"created_at"[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mgroupBy[0m[0;34m([0m[0;34m"source"[0m[0;34m,[0m[0;34m"created_at"[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mcount[0m[0;34m([0m[0;34m)[0m[0;34m.[0m[0mwithColumn[0m[0;34m([0m[0;34m"timestamp"[0m[0;34m,[0m [0mlit[0m[0;34m([0m[0mdatetime[0m[0;34m.[0m[0mnow[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;31mNameError[0m: name 'lit' is not defined
 INFO [2020-01-22 19:50:31,300] ({pool-2-thread-18} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:50:31,316] ({pool-2-thread-18} SchedulerFactory.java[jobFinished]:120) - Job 20200121-185449_2110594246 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:51:02,789] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:51:02,803] ({pool-2-thread-35} SchedulerFactory.java[jobStarted]:114) - Job 20200121-185449_2110594246 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:51:02,803] ({pool-2-thread-35} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-185449_2110594246, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:51:04,457] ({pool-2-thread-35} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-185449_2110594246 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:51:04,480] ({pool-2-thread-35} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:51:04,496] ({pool-2-thread-35} SchedulerFactory.java[jobFinished]:120) - Job 20200121-185449_2110594246 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:51:34,817] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:51:34,832] ({pool-2-thread-10} SchedulerFactory.java[jobStarted]:114) - Job 20200121-185449_2110594246 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:51:34,837] ({pool-2-thread-10} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-185449_2110594246, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:51:36,434] ({pool-2-thread-10} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-185449_2110594246 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:51:36,442] ({pool-2-thread-10} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:51:36,458] ({pool-2-thread-10} SchedulerFactory.java[jobFinished]:120) - Job 20200121-185449_2110594246 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:51:37,975] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:51:37,989] ({pool-2-thread-36} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:51:37,989] ({pool-2-thread-36} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:51:38,089] ({pool-2-thread-36} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:51:38,101] ({pool-2-thread-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:51:38,116] ({pool-2-thread-36} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:51:39,213] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:51:39,226] ({pool-2-thread-19} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:51:39,227] ({pool-2-thread-19} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 19:51:39,444] ({pool-2-thread-19} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mAnalysisException[0mTraceback (most recent call last)
[0;32m<ipython-input-28-5d3340016f6a>[0m in [0;36m<module>[0;34m()[0m
[1;32m      1[0m [0;32mimport[0m [0mtime[0m[0;34m[0m[0m
[1;32m      2[0m [0;34m[0m[0m
[0;32m----> 3[0;31m [0mquery[0m [0;34m=[0m [0mdataDf[0m[0;34m.[0m[0mwriteStream[0m[0;34m.[0m[0moption[0m[0;34m([0m[0;34m"checkpointLocation"[0m[0;34m,[0m [0;34m"s3a://j2training/checkpoint/"[0m[0;34m)[0m             [0;34m.[0m[0mforeachBatch[0m[0;34m([0m[0mforeach_batch_function2[0m[0;34m)[0m             [0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m      4[0m [0;34m[0m[0m
[1;32m      5[0m [0mterminate[0m [0;34m=[0m [0mfalse[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py[0m in [0;36mstart[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)[0m
[1;32m   1103[0m             [0mself[0m[0;34m.[0m[0mqueryName[0m[0;34m([0m[0mqueryName[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1104[0m         [0;32mif[0m [0mpath[0m [0;32mis[0m [0mNone[0m[0;34m:[0m[0;34m[0m[0m
[0;32m-> 1105[0;31m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_sq[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jwrite[0m[0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1106[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m   1107[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_sq[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jwrite[0m[0;34m.[0m[0mstart[0m[0;34m([0m[0mpath[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     67[0m                                              e.java_exception.getStackTrace()))
[1;32m     68[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.AnalysisException: '[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 69[0;31m                 [0;32mraise[0m [0mAnalysisException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     70[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.catalyst.analysis'[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     71[0m                 [0;32mraise[0m [0mAnalysisException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m

[0;31mAnalysisException[0m: u'Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;;\nEventTimeWatermark timestamp#551: timestamp, interval 10 minutes\n+- Project [source#538, created_at#467, lang#484, count#546L, 1579722696381844 AS timestamp#551]\n   +- Aggregate [source#538, created_at#467, lang#484], [source#538, created_at#467, lang#484, count(1) AS count#546L]\n      +- Project [<lambda>(source#496) AS source#538, created_at#467, lang#484]\n         +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@69fce4cf,json,List(),Some(StructType(StructField(contributors,StringType,true), StructField(coordinates,StructType(StructField(coordinates,ArrayType(DoubleType,true),true), StructField(type,StringType,true)),true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StructType(StructField(indices,ArrayType(LongType,true),true), StructField(text,StringType,true)),true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StructType(StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(name,StringType,true), StructField(screen_name,StringType,true)),true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(extended_tweet,StructType(StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StringType,true),true), StructField(user_mentions,ArrayType(StructType(StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(name,StringType,true), StructField(screen_name,StringType,true)),true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true)),true), StructField(full_text,StringType,true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StructType(StructField(coordinates,ArrayType(DoubleType,true),true), StructField(type,StringType,true)),true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,LongType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,LongType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StructType(StructField(bounding_box,StructType(StructField(coordinates,ArrayType(ArrayType(ArrayType(DoubleType,true),true),true),true), StructField(type,StringType,true)),true), StructField(country,StringType,true), StructField(country_code,StringType,true), StructField(full_name,StringType,true), StructField(id,StringType,true), StructField(name,StringType,true), StructField(place_type,StringType,true), StructField(url,StringType,true)),true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(quoted_status,StructType(StructField(contributors,StringType,true), StructField(coordinates,StringType,true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StringType,true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,StringType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,StringType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StringType,true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true)),true), StructField(quoted_status_id,LongType,true), StructField(quoted_status_id_str,StringType,true), StructField(quoted_status_permalink,StructType(StructField(display,StringType,true), StructField(expanded,StringType,true), StructField(url,StringType,true)),true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(retweeted_status,StructType(StructField(contributors,StringType,true), StructField(coordinates,StringType,true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StructType(StructField(indices,ArrayType(LongType,true),true), StructField(text,StringType,true)),true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StructType(StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(name,StringType,true), StructField(screen_name,StringType,true)),true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(monetizable,BooleanType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(extended_tweet,StructType(StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StringType,true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true)),true), StructField(full_text,StringType,true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,LongType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,LongType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StructType(StructField(bounding_box,StructType(StructField(coordinates,ArrayType(ArrayType(ArrayType(DoubleType,true),true),true),true), StructField(type,StringType,true)),true), StructField(country,StringType,true), StructField(country_code,StringType,true), StructField(full_name,StringType,true), StructField(id,StringType,true), StructField(name,StringType,true), StructField(place_type,StringType,true), StructField(url,StringType,true)),true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(quoted_status,StructType(StructField(contributors,StringType,true), StructField(coordinates,StringType,true), StructField(created_at,StringType,true), StructField(display_text_range,ArrayType(LongType,true),true), StructField(entities,StructType(StructField(hashtags,ArrayType(StringType,true),true), StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true)),true),true), StructField(symbols,ArrayType(StringType,true),true), StructField(urls,ArrayType(StructType(StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(url,StringType,true)),true),true), StructField(user_mentions,ArrayType(StringType,true),true)),true), StructField(extended_entities,StructType(StructField(media,ArrayType(StructType(StructField(additional_media_info,StructType(StructField(description,StringType,true), StructField(embeddable,BooleanType,true), StructField(monetizable,BooleanType,true), StructField(title,StringType,true)),true), StructField(display_url,StringType,true), StructField(expanded_url,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(indices,ArrayType(LongType,true),true), StructField(media_url,StringType,true), StructField(media_url_https,StringType,true), StructField(sizes,StructType(StructField(large,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(medium,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(small,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true), StructField(thumb,StructType(StructField(h,LongType,true), StructField(resize,StringType,true), StructField(w,LongType,true)),true)),true), StructField(source_status_id,LongType,true), StructField(source_status_id_str,StringType,true), StructField(source_user_id,LongType,true), StructField(source_user_id_str,StringType,true), StructField(type,StringType,true), StructField(url,StringType,true), StructField(video_info,StructType(StructField(aspect_ratio,ArrayType(LongType,true),true), StructField(duration_millis,LongType,true), StructField(variants,ArrayType(StructType(StructField(bitrate,LongType,true), StructField(content_type,StringType,true), StructField(url,StringType,true)),true),true)),true)),true),true)),true), StructField(favorite_count,LongType,true), StructField(favorited,BooleanType,true), StructField(filter_level,StringType,true), StructField(geo,StringType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(in_reply_to_screen_name,StringType,true), StructField(in_reply_to_status_id,StringType,true), StructField(in_reply_to_status_id_str,StringType,true), StructField(in_reply_to_user_id,StringType,true), StructField(in_reply_to_user_id_str,StringType,true), StructField(is_quote_status,BooleanType,true), StructField(lang,StringType,true), StructField(place,StringType,true), StructField(possibly_sensitive,BooleanType,true), StructField(quote_count,LongType,true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true)),true), StructField(quoted_status_id,LongType,true), StructField(quoted_status_id_str,StringType,true), StructField(quoted_status_permalink,StructType(StructField(display,StringType,true), StructField(expanded,StringType,true), StructField(url,StringType,true)),true), StructField(reply_count,LongType,true), StructField(retweet_count,LongType,true), StructField(retweeted,BooleanType,true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true)),true), StructField(source,StringType,true), StructField(text,StringType,true), StructField(timestamp_ms,StringType,true), StructField(truncated,BooleanType,true), StructField(user,StructType(StructField(contributors_enabled,BooleanType,true), StructField(created_at,StringType,true), StructField(default_profile,BooleanType,true), StructField(default_profile_image,BooleanType,true), StructField(description,StringType,true), StructField(favourites_count,LongType,true), StructField(follow_request_sent,StringType,true), StructField(followers_count,LongType,true), StructField(following,StringType,true), StructField(friends_count,LongType,true), StructField(geo_enabled,BooleanType,true), StructField(id,LongType,true), StructField(id_str,StringType,true), StructField(is_translator,BooleanType,true), StructField(lang,StringType,true), StructField(listed_count,LongType,true), StructField(location,StringType,true), StructField(name,StringType,true), StructField(notifications,StringType,true), StructField(profile_background_color,StringType,true), StructField(profile_background_image_url,StringType,true), StructField(profile_background_image_url_https,StringType,true), StructField(profile_background_tile,BooleanType,true), StructField(profile_banner_url,StringType,true), StructField(profile_image_url,StringType,true), StructField(profile_image_url_https,StringType,true), StructField(profile_link_color,StringType,true), StructField(profile_sidebar_border_color,StringType,true), StructField(profile_sidebar_fill_color,StringType,true), StructField(profile_text_color,StringType,true), StructField(profile_use_background_image,BooleanType,true), StructField(protected,BooleanType,true), StructField(screen_name,StringType,true), StructField(statuses_count,LongType,true), StructField(time_zone,StringType,true), StructField(translator_type,StringType,true), StructField(url,StringType,true), StructField(utc_offset,StringType,true), StructField(verified,BooleanType,true)),true))),List(),None,Map(path -> s3a://j2training/data),None), FileSource[s3a://j2training/data], [contributors#465, coordinates#466, created_at#467, display_text_range#468, entities#469, extended_entities#470, extended_tweet#471, favorite_count#472L, favorited#473, filter_level#474, geo#475, id#476L, id_str#477, in_reply_to_screen_name#478, in_reply_to_status_id#479L, in_reply_to_status_id_str#480, in_reply_to_user_id#481L, in_reply_to_user_id_str#482, is_quote_status#483, lang#484, place#485, possibly_sensitive#486, quote_count#487L, quoted_status#488, ... 12 more fields]\n'
 INFO [2020-01-22 19:51:39,465] ({pool-2-thread-19} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:51:39,478] ({pool-2-thread-19} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:52:35,097] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:52:35,111] ({pool-2-thread-37} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:52:35,112] ({pool-2-thread-37} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 19:52:39,900] ({pool-2-thread-37} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mNameError[0mTraceback (most recent call last)
[0;32m<ipython-input-30-cb7642fc4afe>[0m in [0;36m<module>[0;34m()[0m
[1;32m      3[0m [0mquery[0m [0;34m=[0m [0mdataDf[0m[0;34m.[0m[0mwriteStream[0m[0;34m.[0m[0mtrigger[0m[0;34m([0m[0mprocessingTime[0m[0;34m=[0m[0;34m"15 seconds"[0m[0;34m)[0m             [0;34m.[0m[0moption[0m[0;34m([0m[0;34m"checkpointLocation"[0m[0;34m,[0m [0;34m"s3a://j2training/checkpoint/"[0m[0;34m)[0m             [0;34m.[0m[0mforeachBatch[0m[0;34m([0m[0mforeach_batch_function2[0m[0;34m)[0m             [0;34m.[0m[0moutputMode[0m[0;34m([0m[0;34m"complete"[0m[0;34m)[0m             [0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[1;32m      4[0m [0;34m[0m[0m
[0;32m----> 5[0;31m [0mterminate[0m [0;34m=[0m [0mfalse[0m[0;34m[0m[0m
[0m[1;32m      6[0m [0;34m[0m[0m
[1;32m      7[0m [0;32mwhile[0m [0;32mnot[0m [0mterminate[0m[0;34m:[0m[0;34m[0m[0m

[0;31mNameError[0m: name 'false' is not defined
 INFO [2020-01-22 19:52:39,931] ({pool-2-thread-37} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:52:39,944] ({pool-2-thread-37} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:52:49,213] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:52:49,229] ({pool-2-thread-6} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:52:49,230] ({pool-2-thread-6} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 19:52:50,525] ({pool-2-thread-6} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mPy4JJavaError[0mTraceback (most recent call last)
[0;32m<ipython-input-32-97de183fb55a>[0m in [0;36m<module>[0;34m()[0m
[1;32m      1[0m [0;32mimport[0m [0mtime[0m[0;34m[0m[0m
[1;32m      2[0m [0;34m[0m[0m
[0;32m----> 3[0;31m [0mquery[0m [0;34m=[0m [0mdataDf[0m[0;34m.[0m[0mwriteStream[0m[0;34m.[0m[0mtrigger[0m[0;34m([0m[0mprocessingTime[0m[0;34m=[0m[0;34m"15 seconds"[0m[0;34m)[0m             [0;34m.[0m[0moption[0m[0;34m([0m[0;34m"checkpointLocation"[0m[0;34m,[0m [0;34m"s3a://j2training/checkpoint/"[0m[0;34m)[0m             [0;34m.[0m[0mforeachBatch[0m[0;34m([0m[0mforeach_batch_function2[0m[0;34m)[0m             [0;34m.[0m[0moutputMode[0m[0;34m([0m[0;34m"complete"[0m[0;34m)[0m             [0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m      4[0m [0;34m[0m[0m
[1;32m      5[0m [0mterminate[0m [0;34m=[0m [0mFalse[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py[0m in [0;36mstart[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)[0m
[1;32m   1103[0m             [0mself[0m[0;34m.[0m[0mqueryName[0m[0;34m([0m[0mqueryName[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1104[0m         [0;32mif[0m [0mpath[0m [0;32mis[0m [0mNone[0m[0;34m:[0m[0;34m[0m[0m
[0;32m-> 1105[0;31m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_sq[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jwrite[0m[0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1106[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m   1107[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_sq[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jwrite[0m[0;34m.[0m[0mstart[0m[0;34m([0m[0mpath[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     61[0m     [0;32mdef[0m [0mdeco[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     62[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 63[0;31m             [0;32mreturn[0m [0mf[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     64[0m         [0;32mexcept[0m [0mpy4j[0m[0;34m.[0m[0mprotocol[0m[0;34m.[0m[0mPy4JJavaError[0m [0;32mas[0m [0me[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     65[0m             [0ms[0m [0;34m=[0m [0me[0m[0;34m.[0m[0mjava_exception[0m[0;34m.[0m[0mtoString[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py[0m in [0;36mget_return_value[0;34m(answer, gateway_client, target_id, name)[0m
[1;32m    326[0m                 raise Py4JJavaError(
[1;32m    327[0m                     [0;34m"An error occurred while calling {0}{1}{2}.\n"[0m[0;34m.[0m[0;34m[0m[0m
[0;32m--> 328[0;31m                     format(target_id, ".", name), value)
[0m[1;32m    329[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    330[0m                 raise Py4JError(

[0;31mPy4JJavaError[0m: An error occurred while calling o301.start.
: java.lang.IllegalStateException: Cannot start query with id 38371359-6e61-4830-8052-4b961d779e18 as another query with same id is already active. Perhaps you are attempting to restart a query from checkpoint that is already active.
	at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:339)
	at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:297)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)

 INFO [2020-01-22 19:52:50,537] ({pool-2-thread-6} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:52:50,625] ({pool-2-thread-6} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:53:26,893] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:53:26,906] ({pool-2-thread-38} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:53:26,911] ({pool-2-thread-38} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 19:53:26,996] ({pool-2-thread-38} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 19:53:27,003] ({pool-2-thread-38} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:53:27,021] ({pool-2-thread-38} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:53:28,197] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 19:53:28,219] ({pool-2-thread-20} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 19:53:28,219] ({pool-2-thread-20} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 20:10:26,107] ({pool-2-thread-20} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ERROR, exception: null, result: %text data/2020/1/22/19/tweets-53-ab6cdffb-7759-4f3e-88e0-a487e36156dc.json
[0;31m[0m
[0;31mStreamingQueryException[0mTraceback (most recent call last)
[0;32m<ipython-input-36-97de183fb55a>[0m in [0;36m<module>[0;34m()[0m
[1;32m      6[0m [0;34m[0m[0m
[1;32m      7[0m [0;32mwhile[0m [0;32mnot[0m [0mterminate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m----> 8[0;31m     [0mquery[0m[0;34m.[0m[0mawaitTermination[0m[0;34m([0m[0;36m60000[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m      9[0m     [0mterminate[0m [0;34m=[0m [0mquery[0m[0;34m.[0m[0mlastProgress[0m[0;34m.[0m[0mnumInputRows[0m [0;34m==[0m [0;36m0[0m[0;34m[0m[0m
[1;32m     10[0m [0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py[0m in [0;36mawaitTermination[0;34m(self, timeout)[0m
[1;32m     99[0m             [0;32mif[0m [0;32mnot[0m [0misinstance[0m[0;34m([0m[0mtimeout[0m[0;34m,[0m [0;34m([0m[0mint[0m[0;34m,[0m [0mfloat[0m[0;34m)[0m[0;34m)[0m [0;32mor[0m [0mtimeout[0m [0;34m<[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    100[0m                 [0;32mraise[0m [0mValueError[0m[0;34m([0m[0;34m"timeout must be a positive integer or float. Got %s"[0m [0;34m%[0m [0mtimeout[0m[0;34m)[0m[0;34m[0m[0m
[0;32m--> 101[0;31m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_jsq[0m[0;34m.[0m[0mawaitTermination[0m[0;34m([0m[0mint[0m[0;34m([0m[0mtimeout[0m [0;34m*[0m [0;36m1000[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    102[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    103[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_jsq[0m[0;34m.[0m[0mawaitTermination[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     73[0m                 [0;32mraise[0m [0mParseException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m
[1;32m     74[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.streaming.StreamingQueryException: '[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 75[0;31m                 [0;32mraise[0m [0mStreamingQueryException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     76[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.execution.QueryExecutionException: '[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     77[0m                 [0;32mraise[0m [0mQueryExecutionException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m

[0;31mStreamingQueryException[0m: u'An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File "/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 2381, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 191, in call\n    raise e\nAttributeError: toJson\n\n=== Streaming Query ===\nIdentifier: [id = 1af099f6-db98-4654-be74-0696072360fc, runId = bd10ffc0-2571-49ab-8347-d3fd4ae0c667]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {FileStreamSource[s3a://j2training/data]: {"logOffset":0}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nEventTimeWatermark timestamp#551: timestamp, interval 10 minutes\n+- Project [source#538, created_at#467, lang#484, count#546L, 1579722696381844 AS timestamp#551]\n   +- Aggregate [source#538, created_at#467, lang#484], [source#538, created_at#467, lang#484, count(1) AS count#546L]\n      +- Project [<lambda>(source#496) AS source#538, created_at#467, lang#484]\n         +- StreamingExecutionRelation FileStreamSource[s3a://j2training/data], [contributors#465, coordinates#466, created_at#467, display_text_range#468, entities#469, extended_entities#470, extended_tweet#471, favorite_count#472L, favorited#473, filter_level#474, geo#475, id#476L, id_str#477, in_reply_to_screen_name#478, in_reply_to_status_id#479L, in_reply_to_status_id_str#480, in_reply_to_user_id#481L, in_reply_to_user_id_str#482, is_quote_status#483, lang#484, place#485, possibly_sensitive#486, quote_count#487L, quoted_status#488, ... 12 more fields]\n'
 INFO [2020-01-22 20:10:26,135] ({pool-2-thread-20} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:10:26,171] ({pool-2-thread-20} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:14:14,009] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:14:14,025] ({pool-2-thread-39} SchedulerFactory.java[jobStarted]:114) - Job 20200121-183236_977276561 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:14:14,025] ({pool-2-thread-39} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-183236_977276561, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:14:16,212] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:14:16,265] ({pool-2-thread-11} SchedulerFactory.java[jobStarted]:114) - Job 20200122-174128_1762695518 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:14:16,266] ({pool-2-thread-11} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-174128_1762695518, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:14:16,507] ({pool-2-thread-39} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-183236_977276561 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:14:16,540] ({pool-2-thread-39} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:14:16,562] ({pool-2-thread-39} SchedulerFactory.java[jobFinished]:120) - Job 20200121-183236_977276561 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:14:16,609] ({pool-2-thread-11} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-174128_1762695518 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:14:16,645] ({pool-2-thread-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:14:16,673] ({pool-2-thread-11} SchedulerFactory.java[jobFinished]:120) - Job 20200122-174128_1762695518 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:14:18,320] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:14:18,337] ({pool-2-thread-40} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:14:18,338] ({pool-2-thread-40} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:14:20,860] ({pool-2-thread-40} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-173241_479707786 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:14:20,880] ({pool-2-thread-40} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:14:20,894] ({pool-2-thread-40} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:14:26,550] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:14:26,562] ({pool-2-thread-21} SchedulerFactory.java[jobStarted]:114) - Job 20200121-185449_2110594246 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:14:26,562] ({pool-2-thread-21} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-185449_2110594246, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:14:27,993] ({pool-2-thread-21} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-185449_2110594246 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:14:28,007] ({pool-2-thread-21} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:14:28,037] ({pool-2-thread-21} SchedulerFactory.java[jobFinished]:120) - Job 20200121-185449_2110594246 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:16:13,810] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:16:13,821] ({qtp2107447833-9} NotebookServer.java[broadcastNewParagraph]:688) - Broadcasting paragraph on run call instead of note.
 INFO [2020-01-22 20:16:16,687] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:16:16,708] ({pool-2-thread-41} SchedulerFactory.java[jobStarted]:114) - Job 20200122-201613_89995087 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:16:16,710] ({pool-2-thread-41} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-201613_89995087, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:16:18,025] ({pool-2-thread-41} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-201613_89995087 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:16:18,050] ({pool-2-thread-41} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:16:18,079] ({pool-2-thread-41} SchedulerFactory.java[jobFinished]:120) - Job 20200122-201613_89995087 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:16:23,464] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:16:23,475] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200122-201613_89995087 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:16:23,476] ({pool-2-thread-4} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-201613_89995087, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 20:16:24,426] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-201613_89995087 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mAttributeError[0mTraceback (most recent call last)
[0;32m<ipython-input-48-59df3c949a5c>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdf[0m[0;34m.[0m[0mtoJSON[0m[0;34m([0m[0;34m)[0m[0;34m.[0m[0mshow[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;31mAttributeError[0m: 'RDD' object has no attribute 'show'
 INFO [2020-01-22 20:16:24,453] ({pool-2-thread-4} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:16:24,464] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200122-201613_89995087 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:16:48,352] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:16:48,371] ({pool-2-thread-42} SchedulerFactory.java[jobStarted]:114) - Job 20200122-201613_89995087 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:16:48,372] ({pool-2-thread-42} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-201613_89995087, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:17:02,077] ({pool-2-thread-42} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-201613_89995087 is finished successfully, status: FINISHED
 WARN [2020-01-22 20:17:02,091] ({pool-8-thread-1} AppendOutputRunner.java[run]:104) - Processing size for buffered append-output is high: 100072 characters.
 INFO [2020-01-22 20:17:02,109] ({pool-2-thread-42} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:17:02,129] ({pool-2-thread-42} SchedulerFactory.java[jobFinished]:120) - Job 20200122-201613_89995087 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:17:21,335] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:17:21,351] ({pool-2-thread-22} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:17:21,352] ({pool-2-thread-22} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:17:24,488] ({pool-2-thread-22} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-173241_479707786 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:17:24,535] ({pool-2-thread-22} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:17:24,566] ({pool-2-thread-22} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:17:28,447] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:17:28,463] ({pool-2-thread-43} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:17:28,463] ({pool-2-thread-43} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 20:17:28,580] ({pool-2-thread-43} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-173241_479707786 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mAttributeError[0mTraceback (most recent call last)
[0;32m<ipython-input-54-6bd9a0d60e64>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdf[0m [0;34m=[0m [0mdf[0m[0;34m.[0m[0mselect[0m[0;34m([0m[0msourceExtractor[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"source"[0m[0;34m)[0m[0;34m,[0m[0;34m"created_at"[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mgroupBy[0m[0;34m([0m[0;34m"source"[0m[0;34m,[0m[0;34m"created_at"[0m[0;34m,[0m[0;34m"lang"[0m[0;34m)[0m[0;34m.[0m[0mcount[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;31mAttributeError[0m: 'NoneType' object has no attribute 'select'
 INFO [2020-01-22 20:17:28,599] ({pool-2-thread-43} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:17:28,617] ({pool-2-thread-43} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:17:34,727] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:17:34,747] ({pool-2-thread-12} SchedulerFactory.java[jobStarted]:114) - Job 20200121-183236_977276561 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:17:34,748] ({pool-2-thread-12} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-183236_977276561, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:17:37,095] ({pool-2-thread-12} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-183236_977276561 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:17:37,103] ({pool-2-thread-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:17:37,119] ({pool-2-thread-12} SchedulerFactory.java[jobFinished]:120) - Job 20200121-183236_977276561 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:17:41,214] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:17:41,598] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:17:41,613] ({pool-2-thread-44} SchedulerFactory.java[jobStarted]:114) - Job 20200122-174128_1762695518 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:17:41,614] ({pool-2-thread-44} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-174128_1762695518, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:17:41,704] ({pool-2-thread-44} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-174128_1762695518 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:17:41,713] ({pool-2-thread-44} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:17:41,729] ({pool-2-thread-44} SchedulerFactory.java[jobFinished]:120) - Job 20200122-174128_1762695518 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:17:42,875] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:17:42,891] ({pool-2-thread-23} SchedulerFactory.java[jobStarted]:114) - Job 20200122-173241_479707786 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:17:42,896] ({pool-2-thread-23} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-173241_479707786, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:17:43,036] ({pool-2-thread-23} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-173241_479707786 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:17:43,047] ({pool-2-thread-23} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:17:43,064] ({pool-2-thread-23} SchedulerFactory.java[jobFinished]:120) - Job 20200122-173241_479707786 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:17:49,132] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:17:49,165] ({pool-2-thread-45} SchedulerFactory.java[jobStarted]:114) - Job 20200122-201613_89995087 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:17:49,166] ({pool-2-thread-45} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-201613_89995087, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:17:58,544] ({pool-2-thread-45} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-201613_89995087 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:17:58,554] ({pool-2-thread-45} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:17:58,585] ({pool-2-thread-45} SchedulerFactory.java[jobFinished]:120) - Job 20200122-201613_89995087 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 WARN [2020-01-22 20:17:58,628] ({pool-8-thread-1} AppendOutputRunner.java[run]:104) - Processing size for buffered append-output is high: 100072 characters.
 INFO [2020-01-22 20:18:13,686] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:18:13,709] ({pool-2-thread-7} SchedulerFactory.java[jobStarted]:114) - Job 20200122-201613_89995087 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:18:13,714] ({pool-2-thread-7} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-201613_89995087, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:18:17,994] ({pool-2-thread-7} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-201613_89995087 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:18:18,004] ({pool-2-thread-7} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:18:18,050] ({pool-2-thread-7} SchedulerFactory.java[jobFinished]:120) - Job 20200122-201613_89995087 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:20:48,752] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:20:48,763] ({qtp2107447833-36} NotebookServer.java[broadcastNewParagraph]:688) - Broadcasting paragraph on run call instead of note.
 INFO [2020-01-22 20:20:57,653] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:21:07,211] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:21:15,404] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:21:37,899] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:25:09,008] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:25:33,078] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:29:01,518] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:29:04,284] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:29:09,277] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:29:09,289] ({qtp2107447833-14} NotebookServer.java[broadcastNewParagraph]:688) - Broadcasting paragraph on run call instead of note.
 INFO [2020-01-22 20:29:40,448] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:30:16,229] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:30:16,242] ({pool-2-thread-46} SchedulerFactory.java[jobStarted]:114) - Job 20200122-201613_89995087 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:30:16,243] ({pool-2-thread-46} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-201613_89995087, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:30:16,419] ({pool-2-thread-46} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-201613_89995087 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:30:16,438] ({pool-2-thread-46} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:30:16,456] ({pool-2-thread-46} SchedulerFactory.java[jobFinished]:120) - Job 20200122-201613_89995087 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:30:50,475] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:31:05,908] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:31:38,599] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:32:12,821] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:32:37,172] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:32:48,194] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:32:48,207] ({pool-2-thread-24} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:32:48,207] ({pool-2-thread-24} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:32:48,303] ({pool-2-thread-24} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:32:48,318] ({pool-2-thread-24} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:32:48,329] ({pool-2-thread-24} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:36:07,947] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:36:07,972] ({pool-2-thread-47} SchedulerFactory.java[jobStarted]:114) - Job 20200121-183200_1639671760 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:36:07,972] ({pool-2-thread-47} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-183200_1639671760, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:36:08,038] ({pool-2-thread-47} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-183200_1639671760 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:36:08,047] ({pool-2-thread-47} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:36:08,061] ({pool-2-thread-47} SchedulerFactory.java[jobFinished]:120) - Job 20200121-183200_1639671760 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:36:09,748] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:36:09,760] ({pool-2-thread-13} SchedulerFactory.java[jobStarted]:114) - Job 20200121-183236_977276561 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:36:09,760] ({pool-2-thread-13} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-183236_977276561, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:36:12,062] ({pool-2-thread-13} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-183236_977276561 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:36:12,080] ({pool-2-thread-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:36:12,092] ({pool-2-thread-13} SchedulerFactory.java[jobFinished]:120) - Job 20200121-183236_977276561 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:36:12,408] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:36:12,419] ({pool-2-thread-48} SchedulerFactory.java[jobStarted]:114) - Job 20200122-174128_1762695518 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:36:12,420] ({pool-2-thread-48} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-174128_1762695518, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:36:12,469] ({pool-2-thread-48} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-174128_1762695518 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:36:12,486] ({pool-2-thread-48} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:36:12,496] ({pool-2-thread-48} SchedulerFactory.java[jobFinished]:120) - Job 20200122-174128_1762695518 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:36:16,967] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:36:16,987] ({pool-2-thread-25} SchedulerFactory.java[jobStarted]:114) - Job 20200121-185449_2110594246 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:36:16,995] ({pool-2-thread-25} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-185449_2110594246, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:36:18,405] ({pool-2-thread-25} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-185449_2110594246 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:36:18,431] ({pool-2-thread-25} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:36:18,447] ({pool-2-thread-25} SchedulerFactory.java[jobFinished]:120) - Job 20200121-185449_2110594246 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:36:22,521] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:36:22,532] ({pool-2-thread-49} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:36:22,533] ({pool-2-thread-49} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:36:22,622] ({pool-2-thread-49} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:36:22,639] ({pool-2-thread-49} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:36:22,650] ({pool-2-thread-49} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:36:31,821] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:36:31,838] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:36:31,840] ({pool-2-thread-3} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 20:36:50,891] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mStreamingQueryException[0mTraceback (most recent call last)
[0;32m<ipython-input-80-99c8d82c000b>[0m in [0;36m<module>[0;34m()[0m
[1;32m      6[0m [0;34m[0m[0m
[1;32m      7[0m [0;32mwhile[0m [0;32mnot[0m [0mterminate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m----> 8[0;31m     [0mquery[0m[0;34m.[0m[0mawaitTermination[0m[0;34m([0m[0;36m60000[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m      9[0m     [0mterminate[0m [0;34m=[0m [0mquery[0m[0;34m.[0m[0mlastProgress[0m[0;34m.[0m[0mnumInputRows[0m [0;34m==[0m [0;36m0[0m[0;34m[0m[0m
[1;32m     10[0m [0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py[0m in [0;36mawaitTermination[0;34m(self, timeout)[0m
[1;32m     99[0m             [0;32mif[0m [0;32mnot[0m [0misinstance[0m[0;34m([0m[0mtimeout[0m[0;34m,[0m [0;34m([0m[0mint[0m[0;34m,[0m [0mfloat[0m[0;34m)[0m[0;34m)[0m [0;32mor[0m [0mtimeout[0m [0;34m<[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    100[0m                 [0;32mraise[0m [0mValueError[0m[0;34m([0m[0;34m"timeout must be a positive integer or float. Got %s"[0m [0;34m%[0m [0mtimeout[0m[0;34m)[0m[0;34m[0m[0m
[0;32m--> 101[0;31m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_jsq[0m[0;34m.[0m[0mawaitTermination[0m[0;34m([0m[0mint[0m[0;34m([0m[0mtimeout[0m [0;34m*[0m [0;36m1000[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    102[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    103[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_jsq[0m[0;34m.[0m[0mawaitTermination[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     73[0m                 [0;32mraise[0m [0mParseException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m
[1;32m     74[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.streaming.StreamingQueryException: '[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 75[0;31m                 [0;32mraise[0m [0mStreamingQueryException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     76[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.execution.QueryExecutionException: '[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     77[0m                 [0;32mraise[0m [0mQueryExecutionException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m

[0;31mStreamingQueryException[0m: u'An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File "/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 2381, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 191, in call\n    raise e\nAttributeError: \'DataFrame\' object has no attribute \'toJson\'\n\n=== Streaming Query ===\nIdentifier: [id = 15c8ff01-1819-4d63-925d-088ec32fff33, runId = e7f18906-c3ae-4b07-96cd-9dae02c77946]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {FileStreamSource[s3a://j2training/data]: {"logOffset":0}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nRepartition 2, true\n+- EventTimeWatermark timestamp#1431: timestamp, interval 10 minutes\n   +- Project [source#1418, created_at#1347, lang#1364, count#1426L, 1579725378377097 AS timestamp#1431]\n      +- Aggregate [source#1418, created_at#1347, lang#1364], [source#1418, created_at#1347, lang#1364, count(1) AS count#1426L]\n         +- Project [<lambda>(source#1376) AS source#1418, created_at#1347, lang#1364]\n            +- StreamingExecutionRelation FileStreamSource[s3a://j2training/data], [contributors#1345, coordinates#1346, created_at#1347, display_text_range#1348, entities#1349, extended_entities#1350, extended_tweet#1351, favorite_count#1352L, favorited#1353, filter_level#1354, geo#1355, id#1356L, id_str#1357, in_reply_to_screen_name#1358, in_reply_to_status_id#1359L, in_reply_to_status_id_str#1360, in_reply_to_user_id#1361L, in_reply_to_user_id_str#1362, is_quote_status#1363, lang#1364, place#1365, possibly_sensitive#1366, quote_count#1367L, quoted_status#1368, ... 12 more fields]\n'
 INFO [2020-01-22 20:36:50,910] ({pool-2-thread-3} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:36:50,925] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:39:12,689] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:39:14,730] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:39:29,801] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:39:29,813] ({pool-2-thread-50} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:39:29,814] ({pool-2-thread-50} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:39:29,896] ({pool-2-thread-50} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:39:29,906] ({pool-2-thread-50} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:39:29,917] ({pool-2-thread-50} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:39:32,020] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:39:32,036] ({pool-2-thread-26} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:39:32,037] ({pool-2-thread-26} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 20:39:38,080] ({pool-2-thread-26} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mStreamingQueryException[0mTraceback (most recent call last)
[0;32m<ipython-input-84-99c8d82c000b>[0m in [0;36m<module>[0;34m()[0m
[1;32m      6[0m [0;34m[0m[0m
[1;32m      7[0m [0;32mwhile[0m [0;32mnot[0m [0mterminate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m----> 8[0;31m     [0mquery[0m[0;34m.[0m[0mawaitTermination[0m[0;34m([0m[0;36m60000[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m      9[0m     [0mterminate[0m [0;34m=[0m [0mquery[0m[0;34m.[0m[0mlastProgress[0m[0;34m.[0m[0mnumInputRows[0m [0;34m==[0m [0;36m0[0m[0;34m[0m[0m
[1;32m     10[0m [0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py[0m in [0;36mawaitTermination[0;34m(self, timeout)[0m
[1;32m     99[0m             [0;32mif[0m [0;32mnot[0m [0misinstance[0m[0;34m([0m[0mtimeout[0m[0;34m,[0m [0;34m([0m[0mint[0m[0;34m,[0m [0mfloat[0m[0;34m)[0m[0;34m)[0m [0;32mor[0m [0mtimeout[0m [0;34m<[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    100[0m                 [0;32mraise[0m [0mValueError[0m[0;34m([0m[0;34m"timeout must be a positive integer or float. Got %s"[0m [0;34m%[0m [0mtimeout[0m[0;34m)[0m[0;34m[0m[0m
[0;32m--> 101[0;31m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_jsq[0m[0;34m.[0m[0mawaitTermination[0m[0;34m([0m[0mint[0m[0;34m([0m[0mtimeout[0m [0;34m*[0m [0;36m1000[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    102[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    103[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_jsq[0m[0;34m.[0m[0mawaitTermination[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     73[0m                 [0;32mraise[0m [0mParseException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m
[1;32m     74[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.streaming.StreamingQueryException: '[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 75[0;31m                 [0;32mraise[0m [0mStreamingQueryException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     76[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.execution.QueryExecutionException: '[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     77[0m                 [0;32mraise[0m [0mQueryExecutionException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m

[0;31mStreamingQueryException[0m: u'An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File "/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 2381, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 191, in call\n    raise e\nAttributeError: \'DataFrame\' object has no attribute \'mapPartitions\'\n\n=== Streaming Query ===\nIdentifier: [id = 15c8ff01-1819-4d63-925d-088ec32fff33, runId = babcf0d1-b82d-48fa-9c57-652d35627351]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {FileStreamSource[s3a://j2training/data]: {"logOffset":0}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nRepartition 2, true\n+- EventTimeWatermark timestamp#1431: timestamp, interval 10 minutes\n   +- Project [source#1418, created_at#1347, lang#1364, count#1426L, 1579725378377097 AS timestamp#1431]\n      +- Aggregate [source#1418, created_at#1347, lang#1364], [source#1418, created_at#1347, lang#1364, count(1) AS count#1426L]\n         +- Project [<lambda>(source#1376) AS source#1418, created_at#1347, lang#1364]\n            +- StreamingExecutionRelation FileStreamSource[s3a://j2training/data], [contributors#1345, coordinates#1346, created_at#1347, display_text_range#1348, entities#1349, extended_entities#1350, extended_tweet#1351, favorite_count#1352L, favorited#1353, filter_level#1354, geo#1355, id#1356L, id_str#1357, in_reply_to_screen_name#1358, in_reply_to_status_id#1359L, in_reply_to_status_id_str#1360, in_reply_to_user_id#1361L, in_reply_to_user_id_str#1362, is_quote_status#1363, lang#1364, place#1365, possibly_sensitive#1366, quote_count#1367L, quoted_status#1368, ... 12 more fields]\n'
 INFO [2020-01-22 20:39:38,108] ({pool-2-thread-26} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:39:38,125] ({pool-2-thread-26} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:39:59,553] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:39:59,565] ({pool-2-thread-14} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:39:59,565] ({pool-2-thread-14} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:39:59,656] ({pool-2-thread-14} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:39:59,672] ({pool-2-thread-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:39:59,683] ({pool-2-thread-14} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:40:01,683] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:40:01,694] ({pool-2-thread-51} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:40:01,695] ({pool-2-thread-51} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 20:40:07,985] ({pool-2-thread-51} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ERROR, exception: null, result: %text data/2020/1/22/20/tweets-40-efeb967f-c244-4367-ba80-21eba3490116.json
[0;31m[0m
[0;31mStreamingQueryException[0mTraceback (most recent call last)
[0;32m<ipython-input-88-99c8d82c000b>[0m in [0;36m<module>[0;34m()[0m
[1;32m      6[0m [0;34m[0m[0m
[1;32m      7[0m [0;32mwhile[0m [0;32mnot[0m [0mterminate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m----> 8[0;31m     [0mquery[0m[0;34m.[0m[0mawaitTermination[0m[0;34m([0m[0;36m60000[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m      9[0m     [0mterminate[0m [0;34m=[0m [0mquery[0m[0;34m.[0m[0mlastProgress[0m[0;34m.[0m[0mnumInputRows[0m [0;34m==[0m [0;36m0[0m[0;34m[0m[0m
[1;32m     10[0m [0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py[0m in [0;36mawaitTermination[0;34m(self, timeout)[0m
[1;32m     99[0m             [0;32mif[0m [0;32mnot[0m [0misinstance[0m[0;34m([0m[0mtimeout[0m[0;34m,[0m [0;34m([0m[0mint[0m[0;34m,[0m [0mfloat[0m[0;34m)[0m[0;34m)[0m [0;32mor[0m [0mtimeout[0m [0;34m<[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    100[0m                 [0;32mraise[0m [0mValueError[0m[0;34m([0m[0;34m"timeout must be a positive integer or float. Got %s"[0m [0;34m%[0m [0mtimeout[0m[0;34m)[0m[0;34m[0m[0m
[0;32m--> 101[0;31m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_jsq[0m[0;34m.[0m[0mawaitTermination[0m[0;34m([0m[0mint[0m[0;34m([0m[0mtimeout[0m [0;34m*[0m [0;36m1000[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    102[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    103[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_jsq[0m[0;34m.[0m[0mawaitTermination[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     73[0m                 [0;32mraise[0m [0mParseException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m
[1;32m     74[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.streaming.StreamingQueryException: '[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 75[0;31m                 [0;32mraise[0m [0mStreamingQueryException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     76[0m             [0;32mif[0m [0ms[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0;34m'org.apache.spark.sql.execution.QueryExecutionException: '[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     77[0m                 [0;32mraise[0m [0mQueryExecutionException[0m[0;34m([0m[0ms[0m[0;34m.[0m[0msplit[0m[0;34m([0m[0;34m': '[0m[0;34m,[0m [0;36m1[0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0mstackTrace[0m[0;34m)[0m[0;34m[0m[0m

[0;31mStreamingQueryException[0m: u'An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File "/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 2381, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 191, in call\n    raise e\nNameError: global name \'dataCOllected\' is not defined\n\n=== Streaming Query ===\nIdentifier: [id = 15c8ff01-1819-4d63-925d-088ec32fff33, runId = 64bb4169-6a30-4fac-b4b4-208889fc3bcb]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {FileStreamSource[s3a://j2training/data]: {"logOffset":0}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nRepartition 2, true\n+- EventTimeWatermark timestamp#1431: timestamp, interval 10 minutes\n   +- Project [source#1418, created_at#1347, lang#1364, count#1426L, 1579725378377097 AS timestamp#1431]\n      +- Aggregate [source#1418, created_at#1347, lang#1364], [source#1418, created_at#1347, lang#1364, count(1) AS count#1426L]\n         +- Project [<lambda>(source#1376) AS source#1418, created_at#1347, lang#1364]\n            +- StreamingExecutionRelation FileStreamSource[s3a://j2training/data], [contributors#1345, coordinates#1346, created_at#1347, display_text_range#1348, entities#1349, extended_entities#1350, extended_tweet#1351, favorite_count#1352L, favorited#1353, filter_level#1354, geo#1355, id#1356L, id_str#1357, in_reply_to_screen_name#1358, in_reply_to_status_id#1359L, in_reply_to_status_id_str#1360, in_reply_to_user_id#1361L, in_reply_to_user_id_str#1362, is_quote_status#1363, lang#1364, place#1365, possibly_sensitive#1366, quote_count#1367L, quoted_status#1368, ... 12 more fields]\n'
 INFO [2020-01-22 20:40:07,998] ({pool-2-thread-51} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:40:08,023] ({pool-2-thread-51} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:40:58,633] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:41:18,215] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:41:22,598] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:41:22,611] ({pool-2-thread-27} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:41:22,617] ({pool-2-thread-27} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 20:41:22,708] ({pool-2-thread-27} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-051101_1395511314 is finished, status: ERROR, exception: null, result: %text [0;36m  File [0;32m"<ipython-input-90-66a6c4cf9f4d>"[0;36m, line [0;32m17[0m
[0;31m    jsonList = for row in row_list:[0m
[0m                 ^[0m
[0;31mSyntaxError[0m[0;31m:[0m invalid syntax

 INFO [2020-01-22 20:41:22,723] ({pool-2-thread-27} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:41:22,745] ({pool-2-thread-27} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:41:47,368] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:41:47,384] ({pool-2-thread-52} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:41:47,388] ({pool-2-thread-52} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:41:47,511] ({pool-2-thread-52} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:41:47,527] ({pool-2-thread-52} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:41:47,541] ({pool-2-thread-52} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:41:48,910] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:41:48,922] ({pool-2-thread-1} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:41:48,922] ({pool-2-thread-1} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 20:45:35,370] ({pool-2-thread-1} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ABORT, exception: null, result: %text [0;31m[0m
[0;31mKeyboardInterrupt[0mTraceback (most recent call last)
[0;32m<ipython-input-94-99c8d82c000b>[0m in [0;36m<module>[0;34m()[0m
[1;32m      6[0m [0;34m[0m[0m
[1;32m      7[0m [0;32mwhile[0m [0;32mnot[0m [0mterminate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m----> 8[0;31m     [0mquery[0m[0;34m.[0m[0mawaitTermination[0m[0;34m([0m[0;36m60000[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m      9[0m     [0mterminate[0m [0;34m=[0m [0mquery[0m[0;34m.[0m[0mlastProgress[0m[0;34m.[0m[0mnumInputRows[0m [0;34m==[0m [0;36m0[0m[0;34m[0m[0m
[1;32m     10[0m [0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py[0m in [0;36mawaitTermination[0;34m(self, timeout)[0m
[1;32m     99[0m             [0;32mif[0m [0;32mnot[0m [0misinstance[0m[0;34m([0m[0mtimeout[0m[0;34m,[0m [0;34m([0m[0mint[0m[0;34m,[0m [0mfloat[0m[0;34m)[0m[0;34m)[0m [0;32mor[0m [0mtimeout[0m [0;34m<[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    100[0m                 [0;32mraise[0m [0mValueError[0m[0;34m([0m[0;34m"timeout must be a positive integer or float. Got %s"[0m [0;34m%[0m [0mtimeout[0m[0;34m)[0m[0;34m[0m[0m
[0;32m--> 101[0;31m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_jsq[0m[0;34m.[0m[0mawaitTermination[0m[0;34m([0m[0mint[0m[0;34m([0m[0mtimeout[0m [0;34m*[0m [0;36m1000[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    102[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    103[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_jsq[0m[0;34m.[0m[0mawaitTermination[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1253[0m             [0mproto[0m[0;34m.[0m[0mEND_COMMAND_PART[0m[0;34m[0m[0m
[1;32m   1254[0m [0;34m[0m[0m
[0;32m-> 1255[0;31m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1256[0m         return_value = get_return_value(
[1;32m   1257[0m             answer, self.gateway_client, self.target_id, self.name)

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36msend_command[0;34m(self, command, retry, binary)[0m
[1;32m    983[0m         [0mconnection[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_get_connection[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[1;32m    984[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 985[0;31m             [0mresponse[0m [0;34m=[0m [0mconnection[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    986[0m             [0;32mif[0m [0mbinary[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    987[0m                 [0;32mreturn[0m [0mresponse[0m[0;34m,[0m [0mself[0m[0;34m.[0m[0m_create_connection_guard[0m[0;34m([0m[0mconnection[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36msend_command[0;34m(self, command)[0m
[1;32m   1150[0m [0;34m[0m[0m
[1;32m   1151[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m-> 1152[0;31m             [0manswer[0m [0;34m=[0m [0msmart_decode[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mstream[0m[0;34m.[0m[0mreadline[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0;34m:[0m[0;34m-[0m[0;36m1[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1153[0m             [0mlogger[0m[0;34m.[0m[0mdebug[0m[0;34m([0m[0;34m"Answer received: {0}"[0m[0;34m.[0m[0mformat[0m[0;34m([0m[0manswer[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1154[0m             [0;32mif[0m [0manswer[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0mproto[0m[0;34m.[0m[0mRETURN_MESSAGE[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/opt/conda/lib/python2.7/socket.pyc[0m in [0;36mreadline[0;34m(self, size)[0m
[1;32m    449[0m             [0;32mwhile[0m [0mTrue[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    450[0m                 [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 451[0;31m                     [0mdata[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_sock[0m[0;34m.[0m[0mrecv[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_rbufsize[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    452[0m                 [0;32mexcept[0m [0merror[0m[0;34m,[0m [0me[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    453[0m                     [0;32mif[0m [0me[0m[0;34m.[0m[0margs[0m[0;34m[[0m[0;36m0[0m[0;34m][0m [0;34m==[0m [0mEINTR[0m[0;34m:[0m[0;34m[0m[0m

[0;31mKeyboardInterrupt[0m: 
 INFO [2020-01-22 20:45:35,401] ({pool-2-thread-1} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:45:35,413] ({pool-2-thread-1} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:46:16,067] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:46:16,085] ({pool-2-thread-53} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:46:16,085] ({pool-2-thread-53} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:46:16,201] ({pool-2-thread-53} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:46:16,211] ({pool-2-thread-53} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:46:16,222] ({pool-2-thread-53} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:46:18,045] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:46:18,057] ({pool-2-thread-28} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:46:18,058] ({pool-2-thread-28} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 20:46:19,397] ({pool-2-thread-28} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mPy4JJavaError[0mTraceback (most recent call last)
[0;32m<ipython-input-98-99c8d82c000b>[0m in [0;36m<module>[0;34m()[0m
[1;32m      1[0m [0;32mimport[0m [0mtime[0m[0;34m[0m[0m
[1;32m      2[0m [0;34m[0m[0m
[0;32m----> 3[0;31m [0mquery[0m [0;34m=[0m [0mdataDf[0m[0;34m.[0m[0mrepartition[0m[0;34m([0m[0;36m2[0m[0;34m)[0m[0;34m.[0m[0mwriteStream[0m[0;34m.[0m[0mtrigger[0m[0;34m([0m[0mprocessingTime[0m[0;34m=[0m[0;34m"15 seconds"[0m[0;34m)[0m             [0;34m.[0m[0moption[0m[0;34m([0m[0;34m"checkpointLocation"[0m[0;34m,[0m [0;34m"s3a://j2training/checkpoint/"[0m[0;34m)[0m             [0;34m.[0m[0mforeachBatch[0m[0;34m([0m[0mforeach_batch_function2[0m[0;34m)[0m             [0;34m.[0m[0moutputMode[0m[0;34m([0m[0;34m"complete"[0m[0;34m)[0m             [0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m      4[0m [0;34m[0m[0m
[1;32m      5[0m [0mterminate[0m [0;34m=[0m [0mFalse[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py[0m in [0;36mstart[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)[0m
[1;32m   1103[0m             [0mself[0m[0;34m.[0m[0mqueryName[0m[0;34m([0m[0mqueryName[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1104[0m         [0;32mif[0m [0mpath[0m [0;32mis[0m [0mNone[0m[0;34m:[0m[0;34m[0m[0m
[0;32m-> 1105[0;31m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_sq[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jwrite[0m[0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1106[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m   1107[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_sq[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jwrite[0m[0;34m.[0m[0mstart[0m[0;34m([0m[0mpath[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     61[0m     [0;32mdef[0m [0mdeco[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     62[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 63[0;31m             [0;32mreturn[0m [0mf[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     64[0m         [0;32mexcept[0m [0mpy4j[0m[0;34m.[0m[0mprotocol[0m[0;34m.[0m[0mPy4JJavaError[0m [0;32mas[0m [0me[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     65[0m             [0ms[0m [0;34m=[0m [0me[0m[0;34m.[0m[0mjava_exception[0m[0;34m.[0m[0mtoString[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py[0m in [0;36mget_return_value[0;34m(answer, gateway_client, target_id, name)[0m
[1;32m    326[0m                 raise Py4JJavaError(
[1;32m    327[0m                     [0;34m"An error occurred while calling {0}{1}{2}.\n"[0m[0;34m.[0m[0;34m[0m[0m
[0;32m--> 328[0;31m                     format(target_id, ".", name), value)
[0m[1;32m    329[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    330[0m                 raise Py4JError(

[0;31mPy4JJavaError[0m: An error occurred while calling o630.start.
: java.lang.IllegalStateException: Cannot start query with id 15c8ff01-1819-4d63-925d-088ec32fff33 as another query with same id is already active. Perhaps you are attempting to restart a query from checkpoint that is already active.
	at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:339)
	at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:297)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)

 INFO [2020-01-22 20:46:19,413] ({pool-2-thread-28} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:46:19,426] ({pool-2-thread-28} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:47:17,416] ({qtp2107447833-9} NotebookServer.java[broadcastNewParagraph]:688) - Broadcasting paragraph on run call instead of note.
 INFO [2020-01-22 20:47:17,427] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:47:17,440] ({pool-2-thread-8} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194243_52509483 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:47:17,441] ({pool-2-thread-8} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194243_52509483, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:47:17,496] ({pool-2-thread-8} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-194243_52509483 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:47:17,522] ({pool-2-thread-8} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:47:17,536] ({pool-2-thread-8} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194243_52509483 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:47:21,321] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:47:21,342] ({pool-2-thread-54} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:47:21,342] ({pool-2-thread-54} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 20:49:37,560] ({pool-2-thread-54} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ABORT, exception: null, result: %text MapPartitionsRDD[214] at javaToPython at NativeMethodAccessorImpl.java:0
[0;31m[0m
[0;31mKeyboardInterrupt[0mTraceback (most recent call last)
[0;32m<ipython-input-102-99c8d82c000b>[0m in [0;36m<module>[0;34m()[0m
[1;32m      6[0m [0;34m[0m[0m
[1;32m      7[0m [0;32mwhile[0m [0;32mnot[0m [0mterminate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m----> 8[0;31m     [0mquery[0m[0;34m.[0m[0mawaitTermination[0m[0;34m([0m[0;36m60000[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m      9[0m     [0mterminate[0m [0;34m=[0m [0mquery[0m[0;34m.[0m[0mlastProgress[0m[0;34m.[0m[0mnumInputRows[0m [0;34m==[0m [0;36m0[0m[0;34m[0m[0m
[1;32m     10[0m [0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py[0m in [0;36mawaitTermination[0;34m(self, timeout)[0m
[1;32m     99[0m             [0;32mif[0m [0;32mnot[0m [0misinstance[0m[0;34m([0m[0mtimeout[0m[0;34m,[0m [0;34m([0m[0mint[0m[0;34m,[0m [0mfloat[0m[0;34m)[0m[0;34m)[0m [0;32mor[0m [0mtimeout[0m [0;34m<[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    100[0m                 [0;32mraise[0m [0mValueError[0m[0;34m([0m[0;34m"timeout must be a positive integer or float. Got %s"[0m [0;34m%[0m [0mtimeout[0m[0;34m)[0m[0;34m[0m[0m
[0;32m--> 101[0;31m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_jsq[0m[0;34m.[0m[0mawaitTermination[0m[0;34m([0m[0mint[0m[0;34m([0m[0mtimeout[0m [0;34m*[0m [0;36m1000[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    102[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    103[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_jsq[0m[0;34m.[0m[0mawaitTermination[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1253[0m             [0mproto[0m[0;34m.[0m[0mEND_COMMAND_PART[0m[0;34m[0m[0m
[1;32m   1254[0m [0;34m[0m[0m
[0;32m-> 1255[0;31m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1256[0m         return_value = get_return_value(
[1;32m   1257[0m             answer, self.gateway_client, self.target_id, self.name)

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36msend_command[0;34m(self, command, retry, binary)[0m
[1;32m    983[0m         [0mconnection[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_get_connection[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[1;32m    984[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 985[0;31m             [0mresponse[0m [0;34m=[0m [0mconnection[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    986[0m             [0;32mif[0m [0mbinary[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    987[0m                 [0;32mreturn[0m [0mresponse[0m[0;34m,[0m [0mself[0m[0;34m.[0m[0m_create_connection_guard[0m[0;34m([0m[0mconnection[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36msend_command[0;34m(self, command)[0m
[1;32m   1150[0m [0;34m[0m[0m
[1;32m   1151[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m-> 1152[0;31m             [0manswer[0m [0;34m=[0m [0msmart_decode[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mstream[0m[0;34m.[0m[0mreadline[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0;34m:[0m[0;34m-[0m[0;36m1[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1153[0m             [0mlogger[0m[0;34m.[0m[0mdebug[0m[0;34m([0m[0;34m"Answer received: {0}"[0m[0;34m.[0m[0mformat[0m[0;34m([0m[0manswer[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1154[0m             [0;32mif[0m [0manswer[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0mproto[0m[0;34m.[0m[0mRETURN_MESSAGE[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/opt/conda/lib/python2.7/socket.pyc[0m in [0;36mreadline[0;34m(self, size)[0m
[1;32m    449[0m             [0;32mwhile[0m [0mTrue[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    450[0m                 [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 451[0;31m                     [0mdata[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_sock[0m[0;34m.[0m[0mrecv[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_rbufsize[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    452[0m                 [0;32mexcept[0m [0merror[0m[0;34m,[0m [0me[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    453[0m                     [0;32mif[0m [0me[0m[0;34m.[0m[0margs[0m[0;34m[[0m[0;36m0[0m[0;34m][0m [0;34m==[0m [0mEINTR[0m[0;34m:[0m[0;34m[0m[0m

[0;31mKeyboardInterrupt[0m: 
 INFO [2020-01-22 20:49:37,572] ({pool-2-thread-54} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:49:37,589] ({pool-2-thread-54} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:52:13,293] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:52:13,309] ({pool-2-thread-55} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:52:13,309] ({pool-2-thread-55} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:52:13,394] ({pool-2-thread-55} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:52:13,405] ({pool-2-thread-55} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:52:13,418] ({pool-2-thread-55} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:52:37,844] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:52:38,469] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:52:38,482] ({pool-2-thread-29} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:52:38,483] ({pool-2-thread-29} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:52:38,580] ({pool-2-thread-29} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:52:38,596] ({pool-2-thread-29} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:52:38,608] ({pool-2-thread-29} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:52:40,015] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:52:40,026] ({pool-2-thread-56} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:52:40,027] ({pool-2-thread-56} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 20:54:04,306] ({pool-2-thread-56} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ABORT, exception: null, result: %text MapPartitionsRDD[235] at javaToPython at NativeMethodAccessorImpl.java:0
[0;31m[0m
[0;31mKeyboardInterrupt[0mTraceback (most recent call last)
[0;32m<ipython-input-108-8da3eed526cc>[0m in [0;36m<module>[0;34m()[0m
[1;32m      6[0m [0;34m[0m[0m
[1;32m      7[0m [0;32mwhile[0m [0;32mnot[0m [0mterminate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m----> 8[0;31m     [0mquery[0m[0;34m.[0m[0mawaitTermination[0m[0;34m([0m[0;36m600[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m      9[0m     [0;32mprint[0m [0;34m"looping"[0m[0;34m[0m[0m
[1;32m     10[0m     [0;32mprint[0m [0mquery[0m[0;34m.[0m[0mlastProgress[0m[0;34m.[0m[0mnumInputRows[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py[0m in [0;36mawaitTermination[0;34m(self, timeout)[0m
[1;32m     99[0m             [0;32mif[0m [0;32mnot[0m [0misinstance[0m[0;34m([0m[0mtimeout[0m[0;34m,[0m [0;34m([0m[0mint[0m[0;34m,[0m [0mfloat[0m[0;34m)[0m[0;34m)[0m [0;32mor[0m [0mtimeout[0m [0;34m<[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    100[0m                 [0;32mraise[0m [0mValueError[0m[0;34m([0m[0;34m"timeout must be a positive integer or float. Got %s"[0m [0;34m%[0m [0mtimeout[0m[0;34m)[0m[0;34m[0m[0m
[0;32m--> 101[0;31m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_jsq[0m[0;34m.[0m[0mawaitTermination[0m[0;34m([0m[0mint[0m[0;34m([0m[0mtimeout[0m [0;34m*[0m [0;36m1000[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    102[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    103[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_jsq[0m[0;34m.[0m[0mawaitTermination[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1253[0m             [0mproto[0m[0;34m.[0m[0mEND_COMMAND_PART[0m[0;34m[0m[0m
[1;32m   1254[0m [0;34m[0m[0m
[0;32m-> 1255[0;31m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1256[0m         return_value = get_return_value(
[1;32m   1257[0m             answer, self.gateway_client, self.target_id, self.name)

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36msend_command[0;34m(self, command, retry, binary)[0m
[1;32m    983[0m         [0mconnection[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_get_connection[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[1;32m    984[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 985[0;31m             [0mresponse[0m [0;34m=[0m [0mconnection[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    986[0m             [0;32mif[0m [0mbinary[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    987[0m                 [0;32mreturn[0m [0mresponse[0m[0;34m,[0m [0mself[0m[0;34m.[0m[0m_create_connection_guard[0m[0;34m([0m[0mconnection[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36msend_command[0;34m(self, command)[0m
[1;32m   1150[0m [0;34m[0m[0m
[1;32m   1151[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m-> 1152[0;31m             [0manswer[0m [0;34m=[0m [0msmart_decode[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mstream[0m[0;34m.[0m[0mreadline[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0;34m:[0m[0;34m-[0m[0;36m1[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1153[0m             [0mlogger[0m[0;34m.[0m[0mdebug[0m[0;34m([0m[0;34m"Answer received: {0}"[0m[0;34m.[0m[0mformat[0m[0;34m([0m[0manswer[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1154[0m             [0;32mif[0m [0manswer[0m[0;34m.[0m[0mstartswith[0m[0;34m([0m[0mproto[0m[0;34m.[0m[0mRETURN_MESSAGE[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/opt/conda/lib/python2.7/socket.pyc[0m in [0;36mreadline[0;34m(self, size)[0m
[1;32m    449[0m             [0;32mwhile[0m [0mTrue[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    450[0m                 [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 451[0;31m                     [0mdata[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_sock[0m[0;34m.[0m[0mrecv[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_rbufsize[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    452[0m                 [0;32mexcept[0m [0merror[0m[0;34m,[0m [0me[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    453[0m                     [0;32mif[0m [0me[0m[0;34m.[0m[0margs[0m[0;34m[[0m[0;36m0[0m[0;34m][0m [0;34m==[0m [0mEINTR[0m[0;34m:[0m[0;34m[0m[0m

[0;31mKeyboardInterrupt[0m: 
 INFO [2020-01-22 20:54:04,320] ({pool-2-thread-56} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:54:04,334] ({pool-2-thread-56} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:54:07,228] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:54:07,245] ({pool-2-thread-15} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194243_52509483 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:54:07,246] ({pool-2-thread-15} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194243_52509483, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:54:21,401] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:54:22,181] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:54:22,203] ({pool-2-thread-57} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:54:22,204] ({pool-2-thread-57} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:54:47,352] ({pool-2-thread-15} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-194243_52509483 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:54:47,367] ({pool-2-thread-15} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:54:47,394] ({pool-2-thread-15} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194243_52509483 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:54:47,524] ({pool-2-thread-57} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:54:47,572] ({pool-2-thread-57} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:54:47,605] ({pool-2-thread-57} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:55:00,609] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:55:00,620] ({pool-2-thread-58} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:55:00,620] ({pool-2-thread-58} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:55:00,722] ({pool-2-thread-58} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:55:00,740] ({pool-2-thread-58} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:55:00,751] ({pool-2-thread-58} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:55:09,632] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:55:09,645] ({pool-2-thread-30} SchedulerFactory.java[jobStarted]:114) - Job 20200121-185449_2110594246 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:55:09,645] ({pool-2-thread-30} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-185449_2110594246, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:55:10,993] ({pool-2-thread-30} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-185449_2110594246 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:55:11,006] ({pool-2-thread-30} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:55:11,019] ({pool-2-thread-30} SchedulerFactory.java[jobFinished]:120) - Job 20200121-185449_2110594246 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:55:13,435] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:55:13,447] ({pool-2-thread-59} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:55:13,448] ({pool-2-thread-59} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:55:13,539] ({pool-2-thread-59} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:55:13,548] ({pool-2-thread-59} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:55:13,559] ({pool-2-thread-59} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:55:20,736] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:55:20,748] ({pool-2-thread-60} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:55:20,748] ({pool-2-thread-60} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 20:55:34,479] ({pool-2-thread-60} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ERROR, exception: null, result: %text looping
[0;31m[0m
[0;31mAttributeError[0mTraceback (most recent call last)
[0;32m<ipython-input-120-eaf5b344c702>[0m in [0;36m<module>[0;34m()[0m
[1;32m      8[0m     [0mtime[0m[0;34m.[0m[0msleep[0m[0;34m([0m[0;36m10[0m[0;34m)[0m[0;34m[0m[0m
[1;32m      9[0m     [0;32mprint[0m [0;34m"looping"[0m[0;34m[0m[0m
[0;32m---> 10[0;31m     [0;32mprint[0m [0mquery[0m[0;34m.[0m[0mlastProgress[0m[0;34m.[0m[0mnumInputRows[0m[0;34m[0m[0m
[0m[1;32m     11[0m     [0mterminate[0m [0;34m=[0m [0mquery[0m[0;34m.[0m[0mlastProgress[0m[0;34m.[0m[0mnumInputRows[0m [0;34m==[0m [0;36m0[0m[0;34m[0m[0m
[1;32m     12[0m [0;34m[0m[0m

[0;31mAttributeError[0m: 'NoneType' object has no attribute 'numInputRows'
 INFO [2020-01-22 20:55:34,508] ({pool-2-thread-60} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:55:34,522] ({pool-2-thread-60} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:55:48,097] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:56:20,571] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:56:20,627] ({pool-2-thread-16} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:56:20,628] ({pool-2-thread-16} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 20:56:34,482] ({pool-2-thread-16} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ERROR, exception: null, result: %text <pyspark.sql.streaming.StreamingQuery object at 0x7f9849423d10>
looping
None
[0;31m[0m
[0;31mAttributeError[0mTraceback (most recent call last)
[0;32m<ipython-input-122-98c77f273f40>[0m in [0;36m<module>[0;34m()[0m
[1;32m     10[0m     [0;32mprint[0m [0;34m"looping"[0m[0;34m[0m[0m
[1;32m     11[0m     [0;32mprint[0m [0mquery[0m[0;34m.[0m[0mlastProgress[0m[0;34m[0m[0m
[0;32m---> 12[0;31m     [0mterminate[0m [0;34m=[0m [0mquery[0m[0;34m.[0m[0mlastProgress[0m[0;34m.[0m[0mnumInputRows[0m [0;34m==[0m [0;36m0[0m[0;34m[0m[0m
[0m[1;32m     13[0m [0;34m[0m[0m
[1;32m     14[0m [0;34m[0m[0m

[0;31mAttributeError[0m: 'NoneType' object has no attribute 'numInputRows'
 INFO [2020-01-22 20:56:34,499] ({pool-2-thread-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:56:34,512] ({pool-2-thread-16} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:57:07,985] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:57:23,595] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:57:23,609] ({pool-2-thread-61} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:57:23,610] ({pool-2-thread-61} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 20:57:23,717] ({pool-2-thread-61} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 20:57:23,730] ({pool-2-thread-61} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:57:23,744] ({pool-2-thread-61} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:57:24,654] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:57:24,670] ({pool-2-thread-31} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:57:24,675] ({pool-2-thread-31} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 20:57:24,739] ({pool-2-thread-31} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ERROR, exception: null, result: %text [0;36m  File [0;32m"<ipython-input-126-056b17bb50ea>"[0;36m, line [0;32m12[0m
[0;31m    if query.lastProgress is not None[0m
[0m                                     ^[0m
[0;31mSyntaxError[0m[0;31m:[0m invalid syntax

 INFO [2020-01-22 20:57:24,759] ({pool-2-thread-31} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:57:24,771] ({pool-2-thread-31} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:57:32,689] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 20:57:32,701] ({pool-2-thread-9} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 20:57:32,701] ({pool-2-thread-9} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 21:15:08,067] ({pool-2-thread-9} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ERROR, exception: null, result: %text <pyspark.sql.streaming.StreamingQuery object at 0x7f9849419590>
looping
None
MapPartitionsRDD[301] at javaToPython at NativeMethodAccessorImpl.java:0
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
637
looping
{u'stateOperators': [{u'customMetrics': {u'loadedMapCacheHitCount': 0, u'loadedMapCacheMissCount': 0, u'stateOnCurrentVersionSizeBytes': 189591}, u'numRowsTotal': 637, u'memoryUsedBytes': 218391, u'numRowsUpdated': 0}], u'eventTime': {u'watermark': u'2020-01-22T20:45:10.955Z'}, u'name': None, u'timestamp': u'2020-01-22T21:15:04.319Z', u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'batchId': 1, u'sources': [{u'description': u'FileStreamSource[s3a://j2training/data]', u'endOffset': {u'logOffset': 0}, u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'startOffset': {u'logOffset': 0}}], u'durationMs': {u'getOffset': 445, u'triggerExecution': 445}, u'runId': u'7eb1d58d-fb37-4c8b-b905-db9082db8b3d', u'id': u'5a109a98-7949-4e11-ab67-3f90bcc0d757', u'sink': {u'description': u'ForeachBatchSink'}}
[0;31m[0m
[0;31mAttributeError[0mTraceback (most recent call last)
[0;32m<ipython-input-128-8509fe75af61>[0m in [0;36m<module>[0;34m()[0m
[1;32m     11[0m     [0;32mprint[0m [0mquery[0m[0;34m.[0m[0mlastProgress[0m[0;34m[0m[0m
[1;32m     12[0m     [0;32mif[0m [0mquery[0m[0;34m.[0m[0mlastProgress[0m [0;32mis[0m [0;32mnot[0m [0mNone[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 13[0;31m         [0mterminate[0m [0;34m=[0m [0mquery[0m[0;34m.[0m[0mlastProgress[0m[0;34m.[0m[0mnumInputRows[0m [0;34m==[0m [0;36m0[0m[0;34m[0m[0m
[0m[1;32m     14[0m [0;34m[0m[0m
[1;32m     15[0m [0;34m[0m[0m

[0;31mAttributeError[0m: 'dict' object has no attribute 'numInputRows'
 INFO [2020-01-22 21:15:08,078] ({pool-2-thread-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:15:08,137] ({pool-2-thread-9} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:17:17,313] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:17:21,707] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:17:21,719] ({pool-2-thread-62} SchedulerFactory.java[jobStarted]:114) - Job 20200121-185449_2110594246 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:17:21,720] ({pool-2-thread-62} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200121-185449_2110594246, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:17:23,004] ({pool-2-thread-62} NotebookServer.java[afterStatusChange]:2314) - Job 20200121-185449_2110594246 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:17:23,017] ({pool-2-thread-62} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:17:23,028] ({pool-2-thread-62} SchedulerFactory.java[jobFinished]:120) - Job 20200121-185449_2110594246 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:17:25,831] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:17:25,845] ({pool-2-thread-63} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:17:25,845] ({pool-2-thread-63} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:17:25,928] ({pool-2-thread-63} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:17:25,938] ({pool-2-thread-63} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:17:25,951] ({pool-2-thread-63} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:17:37,444] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:17:37,455] ({pool-2-thread-32} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:17:37,455] ({pool-2-thread-32} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 21:17:38,984] ({pool-2-thread-32} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mPy4JJavaError[0mTraceback (most recent call last)
[0;32m<ipython-input-134-ad68f0026352>[0m in [0;36m<module>[0;34m()[0m
[1;32m      1[0m [0;32mimport[0m [0mtime[0m[0;34m[0m[0m
[1;32m      2[0m [0;34m[0m[0m
[0;32m----> 3[0;31m [0mquery[0m [0;34m=[0m [0mdataDf[0m[0;34m.[0m[0mrepartition[0m[0;34m([0m[0;36m2[0m[0;34m)[0m[0;34m.[0m[0mwriteStream[0m[0;34m.[0m[0mtrigger[0m[0;34m([0m[0mprocessingTime[0m[0;34m=[0m[0;34m"15 seconds"[0m[0;34m)[0m             [0;34m.[0m[0moption[0m[0;34m([0m[0;34m"checkpointLocation"[0m[0;34m,[0m [0;34m"s3a://j2training/checkpoint/"[0m[0;34m)[0m             [0;34m.[0m[0mforeachBatch[0m[0;34m([0m[0mforeach_batch_function2[0m[0;34m)[0m             [0;34m.[0m[0moutputMode[0m[0;34m([0m[0;34m"complete"[0m[0;34m)[0m             [0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m      4[0m [0;34m[0m[0m
[1;32m      5[0m [0mterminate[0m [0;34m=[0m [0mFalse[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py[0m in [0;36mstart[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)[0m
[1;32m   1103[0m             [0mself[0m[0;34m.[0m[0mqueryName[0m[0;34m([0m[0mqueryName[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1104[0m         [0;32mif[0m [0mpath[0m [0;32mis[0m [0mNone[0m[0;34m:[0m[0;34m[0m[0m
[0;32m-> 1105[0;31m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_sq[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jwrite[0m[0;34m.[0m[0mstart[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1106[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m   1107[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_sq[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jwrite[0m[0;34m.[0m[0mstart[0m[0;34m([0m[0mpath[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     61[0m     [0;32mdef[0m [0mdeco[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     62[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 63[0;31m             [0;32mreturn[0m [0mf[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     64[0m         [0;32mexcept[0m [0mpy4j[0m[0;34m.[0m[0mprotocol[0m[0;34m.[0m[0mPy4JJavaError[0m [0;32mas[0m [0me[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     65[0m             [0ms[0m [0;34m=[0m [0me[0m[0;34m.[0m[0mjava_exception[0m[0;34m.[0m[0mtoString[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py[0m in [0;36mget_return_value[0;34m(answer, gateway_client, target_id, name)[0m
[1;32m    326[0m                 raise Py4JJavaError(
[1;32m    327[0m                     [0;34m"An error occurred while calling {0}{1}{2}.\n"[0m[0;34m.[0m[0;34m[0m[0m
[0;32m--> 328[0;31m                     format(target_id, ".", name), value)
[0m[1;32m    329[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    330[0m                 raise Py4JError(

[0;31mPy4JJavaError[0m: An error occurred while calling o900.start.
: java.lang.IllegalStateException: Cannot start query with id 5a109a98-7949-4e11-ab67-3f90bcc0d757 as another query with same id is already active. Perhaps you are attempting to restart a query from checkpoint that is already active.
	at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:339)
	at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:297)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)

 INFO [2020-01-22 21:17:38,997] ({pool-2-thread-32} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:17:39,010] ({pool-2-thread-32} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:17:45,618] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:17:45,638] ({pool-2-thread-64} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194243_52509483 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:17:45,638] ({pool-2-thread-64} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194243_52509483, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:17:45,773] ({pool-2-thread-64} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-194243_52509483 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:17:45,814] ({pool-2-thread-64} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:17:45,851] ({pool-2-thread-64} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194243_52509483 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:17:47,701] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:17:47,713] ({pool-2-thread-17} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:17:47,714] ({pool-2-thread-17} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:18:28,779] ({pool-2-thread-17} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-194435_1903717418 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:18:28,791] ({pool-2-thread-17} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:18:28,805] ({pool-2-thread-17} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:19:19,572] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:19:19,586] ({pool-2-thread-33} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:19:19,587] ({pool-2-thread-33} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 21:20:09,954] ({pool-2-thread-33} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ABORT, exception: null, result: %text <pyspark.sql.streaming.StreamingQuery object at 0x7f984947fd50>
MapPartitionsRDD[323] at javaToPython at NativeMethodAccessorImpl.java:0
looping
None
[0;31m[0m
[0;31mKeyboardInterrupt[0mTraceback (most recent call last)
[0;32m<ipython-input-140-ad68f0026352>[0m in [0;36m<module>[0;34m()[0m
[1;32m      7[0m [0;34m[0m[0m
[1;32m      8[0m [0;32mwhile[0m [0;32mnot[0m [0mterminate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m----> 9[0;31m     [0mtime[0m[0;34m.[0m[0msleep[0m[0;34m([0m[0;36m40[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     10[0m     [0;32mprint[0m [0;34m"looping"[0m[0;34m[0m[0m
[1;32m     11[0m     [0;32mprint[0m [0mquery[0m[0;34m.[0m[0mlastProgress[0m[0;34m[0m[0m

[0;31mKeyboardInterrupt[0m: 
 INFO [2020-01-22 21:20:09,970] ({pool-2-thread-33} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:20:09,983] ({pool-2-thread-33} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:20:12,666] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:20:12,678] ({pool-2-thread-65} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194243_52509483 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:20:12,682] ({pool-2-thread-65} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194243_52509483, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:20:12,745] ({pool-2-thread-65} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-194243_52509483 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:20:12,763] ({pool-2-thread-65} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:20:12,776] ({pool-2-thread-65} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194243_52509483 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:20:22,855] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:20:22,882] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:20:22,883] ({pool-2-thread-2} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 21:20:23,015] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ERROR, exception: null, result: %text [0;36m  File [0;32m"<ipython-input-144-5defef01a04c>"[0;36m, line [0;32m4[0m
[0;31m    .foreachBatch(foreach_batch_function2)             .outputMode("complete")             .start()[0m
[0m    ^[0m
[0;31mIndentationError[0m[0;31m:[0m unexpected indent

 INFO [2020-01-22 21:20:23,037] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:20:23,049] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:20:31,779] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:20:31,791] ({pool-2-thread-66} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:20:31,791] ({pool-2-thread-66} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 21:20:31,864] ({pool-2-thread-66} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ERROR, exception: null, result: %text [0;36m  File [0;32m"<ipython-input-146-09dd1506e268>"[0;36m, line [0;32m4[0m
[0;31m    .foreachBatch(foreach_batch_function2)             .outputMode("complete")             .start()[0m
[0m    ^[0m
[0;31mIndentationError[0m[0;31m:[0m unexpected indent

 INFO [2020-01-22 21:20:31,879] ({pool-2-thread-66} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:20:31,889] ({pool-2-thread-66} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:20:37,545] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:20:37,564] ({pool-2-thread-34} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:20:37,565] ({pool-2-thread-34} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 21:20:37,637] ({pool-2-thread-34} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ERROR, exception: null, result: %text [0;36m  File [0;32m"<ipython-input-148-406b551e0904>"[0;36m, line [0;32m3[0m
[0;31m    query = dataDf.repartition(2).writeStream.trigger(processingTime="15 seconds")             //.option("checkpointLocation", "s3a://j2training/checkpoint/")             .foreachBatch(foreach_batch_function2)             .outputMode("complete")             .start()[0m
[0m                                                                                                 ^[0m
[0;31mSyntaxError[0m[0;31m:[0m invalid syntax

 INFO [2020-01-22 21:20:37,653] ({pool-2-thread-34} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:20:37,669] ({pool-2-thread-34} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:20:50,735] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:20:50,753] ({pool-2-thread-67} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:20:50,761] ({pool-2-thread-67} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:21:31,286] ({pool-2-thread-67} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-194435_1903717418 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:21:31,300] ({pool-2-thread-67} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:21:31,317] ({pool-2-thread-67} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:24:04,559] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:24:30,996] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:24:56,032] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:24:56,043] ({pool-2-thread-18} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:24:56,043] ({pool-2-thread-18} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:24:56,152] ({pool-2-thread-18} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:24:56,163] ({pool-2-thread-18} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:24:56,173] ({pool-2-thread-18} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:25:01,974] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:25:01,985] ({pool-2-thread-68} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:25:01,986] ({pool-2-thread-68} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:25:02,094] ({pool-2-thread-68} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:25:02,108] ({pool-2-thread-68} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:25:02,119] ({pool-2-thread-68} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:25:04,444] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:25:04,456] ({pool-2-thread-35} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:25:04,456] ({pool-2-thread-35} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 21:28:38,666] ({pool-2-thread-35} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ABORT, exception: null, result: %text <pyspark.sql.streaming.StreamingQuery object at 0x7f98492afa90>
MapPartitionsRDD[367] at javaToPython at NativeMethodAccessorImpl.java:0
637
looping
None
looping
None
looping
None
looping
None
looping
None
[0;31m[0m
[0;31mKeyboardInterrupt[0mTraceback (most recent call last)
[0;32m<ipython-input-156-bfa1663b4182>[0m in [0;36m<module>[0;34m()[0m
[1;32m      7[0m [0;34m[0m[0m
[1;32m      8[0m [0;32mwhile[0m [0;32mnot[0m [0mterminate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m----> 9[0;31m     [0mtime[0m[0;34m.[0m[0msleep[0m[0;34m([0m[0;36m40[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     10[0m     [0;32mprint[0m [0;34m"looping"[0m[0;34m[0m[0m
[1;32m     11[0m     [0;32mprint[0m [0mquery[0m[0;34m.[0m[0mlastProgress[0m[0;34m[0m[0m

[0;31mKeyboardInterrupt[0m: 
 INFO [2020-01-22 21:28:38,683] ({pool-2-thread-35} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:28:38,697] ({pool-2-thread-35} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:28:54,512] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:28:54,527] ({pool-2-thread-69} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:28:54,527] ({pool-2-thread-69} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:28:54,622] ({pool-2-thread-69} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:28:54,636] ({pool-2-thread-69} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:28:54,647] ({pool-2-thread-69} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:28:56,434] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:28:56,445] ({pool-2-thread-10} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:28:56,446] ({pool-2-thread-10} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 21:30:37,915] ({pool-2-thread-10} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ABORT, exception: null, result: %text <pyspark.sql.streaming.StreamingQuery object at 0x7f984937a890>
MapPartitionsRDD[389] at javaToPython at NativeMethodAccessorImpl.java:0
637
looping
None
looping
None
[0;31m[0m
[0;31mKeyboardInterrupt[0mTraceback (most recent call last)
[0;32m<ipython-input-160-bfa1663b4182>[0m in [0;36m<module>[0;34m()[0m
[1;32m      7[0m [0;34m[0m[0m
[1;32m      8[0m [0;32mwhile[0m [0;32mnot[0m [0mterminate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m----> 9[0;31m     [0mtime[0m[0;34m.[0m[0msleep[0m[0;34m([0m[0;36m40[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     10[0m     [0;32mprint[0m [0;34m"looping"[0m[0;34m[0m[0m
[1;32m     11[0m     [0;32mprint[0m [0mquery[0m[0;34m.[0m[0mlastProgress[0m[0;34m[0m[0m

[0;31mKeyboardInterrupt[0m: 
 INFO [2020-01-22 21:30:37,934] ({pool-2-thread-10} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:30:37,945] ({pool-2-thread-10} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:30:47,449] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:30:47,460] ({pool-2-thread-70} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:30:47,460] ({pool-2-thread-70} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:30:47,566] ({pool-2-thread-70} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:30:47,583] ({pool-2-thread-70} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:30:47,594] ({pool-2-thread-70} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:30:49,682] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:30:49,695] ({pool-2-thread-36} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:30:49,695] ({pool-2-thread-36} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:31:33,755] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:31:33,770] ({pool-2-thread-71} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:31:33,775] ({pool-2-thread-71} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 21:31:35,566] ({pool-2-thread-36} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ABORT, exception: null, result: %text <pyspark.sql.streaming.StreamingQuery object at 0x7f984944b750>
MapPartitionsRDD[411] at javaToPython at NativeMethodAccessorImpl.java:0
637
looping
None
[0;31m[0m
[0;31mKeyboardInterrupt[0mTraceback (most recent call last)
[0;32m<ipython-input-164-bfa1663b4182>[0m in [0;36m<module>[0;34m()[0m
[1;32m      7[0m [0;34m[0m[0m
[1;32m      8[0m [0;32mwhile[0m [0;32mnot[0m [0mterminate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m----> 9[0;31m     [0mtime[0m[0;34m.[0m[0msleep[0m[0;34m([0m[0;36m40[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     10[0m     [0;32mprint[0m [0;34m"looping"[0m[0;34m[0m[0m
[1;32m     11[0m     [0;32mprint[0m [0mquery[0m[0;34m.[0m[0mlastProgress[0m[0;34m[0m[0m

[0;31mKeyboardInterrupt[0m: 
 INFO [2020-01-22 21:31:35,601] ({pool-2-thread-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:31:35,625] ({pool-2-thread-36} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:31:35,690] ({pool-2-thread-71} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:31:35,702] ({pool-2-thread-71} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:31:35,713] ({pool-2-thread-71} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:31:36,769] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:31:36,781] ({pool-2-thread-19} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:31:36,781] ({pool-2-thread-19} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:31:36,883] ({pool-2-thread-19} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:31:36,899] ({pool-2-thread-19} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:31:36,910] ({pool-2-thread-19} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:31:37,985] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:31:37,996] ({pool-2-thread-72} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:31:37,996] ({pool-2-thread-72} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:32:57,911] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 WARN [2020-01-22 21:32:58,017] ({pool-2-thread-72} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ABORT, exception: null, result: %text <pyspark.sql.streaming.StreamingQuery object at 0x7f98493d4a10>
MapPartitionsRDD[434] at javaToPython at NativeMethodAccessorImpl.java:0
looping
None
[0;31m[0m
[0;31mKeyboardInterrupt[0mTraceback (most recent call last)
[0;32m<ipython-input-170-bfa1663b4182>[0m in [0;36m<module>[0;34m()[0m
[1;32m      7[0m [0;34m[0m[0m
[1;32m      8[0m [0;32mwhile[0m [0;32mnot[0m [0mterminate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m----> 9[0;31m     [0mtime[0m[0;34m.[0m[0msleep[0m[0;34m([0m[0;36m40[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     10[0m     [0;32mprint[0m [0;34m"looping"[0m[0;34m[0m[0m
[1;32m     11[0m     [0;32mprint[0m [0mquery[0m[0;34m.[0m[0mlastProgress[0m[0;34m[0m[0m

[0;31mKeyboardInterrupt[0m: 
 INFO [2020-01-22 21:32:58,029] ({pool-2-thread-72} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:32:58,041] ({pool-2-thread-72} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:32:59,811] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:32:59,823] ({pool-2-thread-37} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194243_52509483 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:32:59,823] ({pool-2-thread-37} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194243_52509483, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:32:59,888] ({pool-2-thread-37} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-194243_52509483 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:32:59,906] ({pool-2-thread-37} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:32:59,918] ({pool-2-thread-37} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194243_52509483 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:33:01,428] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:33:01,454] ({pool-2-thread-73} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:33:01,455] ({pool-2-thread-73} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 21:33:15,693] ({pool-2-thread-73} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ABORT, exception: null, result: %text <pyspark.sql.streaming.StreamingQuery object at 0x7f98492fcd10>
MapPartitionsRDD[456] at javaToPython at NativeMethodAccessorImpl.java:0
[0;31m[0m
[0;31mKeyboardInterrupt[0mTraceback (most recent call last)
[0;32m<ipython-input-174-bfa1663b4182>[0m in [0;36m<module>[0;34m()[0m
[1;32m      7[0m [0;34m[0m[0m
[1;32m      8[0m [0;32mwhile[0m [0;32mnot[0m [0mterminate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m----> 9[0;31m     [0mtime[0m[0;34m.[0m[0msleep[0m[0;34m([0m[0;36m40[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     10[0m     [0;32mprint[0m [0;34m"looping"[0m[0;34m[0m[0m
[1;32m     11[0m     [0;32mprint[0m [0mquery[0m[0;34m.[0m[0mlastProgress[0m[0;34m[0m[0m

[0;31mKeyboardInterrupt[0m: 
 INFO [2020-01-22 21:33:15,710] ({pool-2-thread-73} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:33:15,758] ({pool-2-thread-73} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:33:25,429] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:33:25,441] ({pool-2-thread-6} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:33:25,442] ({pool-2-thread-6} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:33:25,542] ({pool-2-thread-6} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:33:25,553] ({pool-2-thread-6} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:33:25,565] ({pool-2-thread-6} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:33:27,077] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:33:27,091] ({pool-2-thread-74} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:33:27,097] ({pool-2-thread-74} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 21:34:15,170] ({pool-2-thread-74} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ABORT, exception: null, result: %text <pyspark.sql.streaming.StreamingQuery object at 0x7f98491274d0>
looping
None
[0;31m[0m
[0;31mKeyboardInterrupt[0mTraceback (most recent call last)
[0;32m<ipython-input-178-bfa1663b4182>[0m in [0;36m<module>[0;34m()[0m
[1;32m      7[0m [0;34m[0m[0m
[1;32m      8[0m [0;32mwhile[0m [0;32mnot[0m [0mterminate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m----> 9[0;31m     [0mtime[0m[0;34m.[0m[0msleep[0m[0;34m([0m[0;36m40[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     10[0m     [0;32mprint[0m [0;34m"looping"[0m[0;34m[0m[0m
[1;32m     11[0m     [0;32mprint[0m [0mquery[0m[0;34m.[0m[0mlastProgress[0m[0;34m[0m[0m

[0;31mKeyboardInterrupt[0m: 
 INFO [2020-01-22 21:34:15,185] ({pool-2-thread-74} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:34:15,196] ({pool-2-thread-74} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:34:30,496] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:34:30,510] ({pool-2-thread-38} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:34:30,511] ({pool-2-thread-38} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:34:30,674] ({pool-2-thread-38} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:34:30,689] ({pool-2-thread-38} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:34:30,701] ({pool-2-thread-38} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:34:32,284] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:34:32,296] ({pool-2-thread-75} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:34:32,296] ({pool-2-thread-75} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:35:53,568] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 WARN [2020-01-22 21:35:53,698] ({pool-2-thread-75} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ABORT, exception: null, result: %text <pyspark.sql.streaming.StreamingQuery object at 0x7f98491277d0>
dfis MapPartitionsRDD[499] at javaToPython at NativeMethodAccessorImpl.java:0
count is 637
looping
None
looping
None
[0;31m[0m
[0;31mKeyboardInterrupt[0mTraceback (most recent call last)
[0;32m<ipython-input-182-bfa1663b4182>[0m in [0;36m<module>[0;34m()[0m
[1;32m      7[0m [0;34m[0m[0m
[1;32m      8[0m [0;32mwhile[0m [0;32mnot[0m [0mterminate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m----> 9[0;31m     [0mtime[0m[0;34m.[0m[0msleep[0m[0;34m([0m[0;36m40[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     10[0m     [0;32mprint[0m [0;34m"looping"[0m[0;34m[0m[0m
[1;32m     11[0m     [0;32mprint[0m [0mquery[0m[0;34m.[0m[0mlastProgress[0m[0;34m[0m[0m

[0;31mKeyboardInterrupt[0m: 
 INFO [2020-01-22 21:35:53,709] ({pool-2-thread-75} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:35:53,720] ({pool-2-thread-75} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:35:56,068] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:35:56,082] ({pool-2-thread-20} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:35:56,083] ({pool-2-thread-20} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:35:56,183] ({pool-2-thread-20} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:35:56,197] ({pool-2-thread-20} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:35:56,209] ({pool-2-thread-20} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:35:58,546] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:35:58,557] ({pool-2-thread-76} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194243_52509483 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:35:58,557] ({pool-2-thread-76} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194243_52509483, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:35:58,613] ({pool-2-thread-76} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-194243_52509483 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:35:58,629] ({pool-2-thread-76} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:35:58,639] ({pool-2-thread-76} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194243_52509483 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:36:00,402] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:36:00,413] ({pool-2-thread-77} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:36:00,413] ({pool-2-thread-77} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:36:40,643] ({pool-2-thread-77} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-194435_1903717418 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:36:40,668] ({pool-2-thread-77} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:36:40,681] ({pool-2-thread-77} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:36:53,564] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:36:53,576] ({pool-2-thread-39} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:36:53,577] ({pool-2-thread-39} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 21:37:01,138] ({pool-2-thread-39} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ABORT, exception: null, result: %text <pyspark.sql.streaming.StreamingQuery object at 0x7f984928e510>
dfis MapPartitionsRDD[544] at javaToPython at <unknown>:0
[0;31m[0m
[0;31mKeyboardInterrupt[0mTraceback (most recent call last)
[0;32m<ipython-input-190-b689d733b398>[0m in [0;36m<module>[0;34m()[0m
[1;32m      7[0m [0;34m[0m[0m
[1;32m      8[0m [0;32mwhile[0m [0;32mnot[0m [0mterminate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m----> 9[0;31m     [0mtime[0m[0;34m.[0m[0msleep[0m[0;34m([0m[0;36m40[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     10[0m     [0;32mprint[0m [0;34m"looping"[0m[0;34m[0m[0m
[1;32m     11[0m     [0;32mprint[0m [0mquery[0m[0;34m.[0m[0mlastProgress[0m[0;34m[0m[0m

[0;31mKeyboardInterrupt[0m: 
 INFO [2020-01-22 21:37:01,149] ({pool-2-thread-39} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:37:01,160] ({pool-2-thread-39} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:37:03,686] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:37:03,697] ({pool-2-thread-11} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:37:03,697] ({pool-2-thread-11} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 21:37:55,201] ({pool-2-thread-11} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ABORT, exception: null, result: %text <pyspark.sql.streaming.StreamingQuery object at 0x7f984928ea50>
dfis MapPartitionsRDD[566] at javaToPython at <unknown>:0
looping
None
count is 637
looping
{u'stateOperators': [{u'customMetrics': {u'loadedMapCacheHitCount': 0, u'loadedMapCacheMissCount': 0, u'stateOnCurrentVersionSizeBytes': 189591}, u'numRowsTotal': 637, u'memoryUsedBytes': 218391, u'numRowsUpdated': 0}], u'eventTime': {u'watermark': u'2020-01-22T21:16:22.974Z'}, u'name': None, u'timestamp': u'2020-01-22T21:37:21.230Z', u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'batchId': 1, u'sources': [{u'description': u'FileStreamSource[s3a://j2training/data]', u'endOffset': {u'logOffset': 0}, u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'startOffset': {u'logOffset': 0}}], u'durationMs': {u'getOffset': 443, u'triggerExecution': 443}, u'runId': u'f3c88b58-0c58-4acb-bd6f-1dc45bfbc07d', u'id': u'3b9f6080-6a97-4d5c-9811-869387ecbfa8', u'sink': {u'description': u'ForeachBatchSink'}}
looping
{u'stateOperators': [{u'customMetrics': {u'loadedMapCacheHitCount': 0, u'loadedMapCacheMissCount': 0, u'stateOnCurrentVersionSizeBytes': 189591}, u'numRowsTotal': 637, u'memoryUsedBytes': 218391, u'numRowsUpdated': 0}], u'eventTime': {u'watermark': u'2020-01-22T21:16:22.974Z'}, u'name': None, u'timestamp': u'2020-01-22T21:37:21.230Z', u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'batchId': 1, u'sources': [{u'description': u'FileStreamSource[s3a://j2training/data]', u'endOffset': {u'logOffset': 0}, u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'startOffset': {u'logOffset': 0}}], u'durationMs': {u'getOffset': 443, u'triggerExecution': 443}, u'runId': u'f3c88b58-0c58-4acb-bd6f-1dc45bfbc07d', u'id': u'3b9f6080-6a97-4d5c-9811-869387ecbfa8', u'sink': {u'description': u'ForeachBatchSink'}}
looping
{u'stateOperators': [{u'customMetrics': {u'loadedMapCacheHitCount': 0, u'loadedMapCacheMissCount': 0, u'stateOnCurrentVersionSizeBytes': 189591}, u'numRowsTotal': 637, u'memoryUsedBytes': 218391, u'numRowsUpdated': 0}], u'eventTime': {u'watermark': u'2020-01-22T21:16:22.974Z'}, u'name': None, u'timestamp': u'2020-01-22T21:37:21.230Z', u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'batchId': 1, u'sources': [{u'description': u'FileStreamSource[s3a://j2training/data]', u'endOffset': {u'logOffset': 0}, u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'startOffset': {u'logOffset': 0}}], u'durationMs': {u'getOffset': 443, u'triggerExecution': 443}, u'runId': u'f3c88b58-0c58-4acb-bd6f-1dc45bfbc07d', u'id': u'3b9f6080-6a97-4d5c-9811-869387ecbfa8', u'sink': {u'description': u'ForeachBatchSink'}}
looping
{u'stateOperators': [{u'customMetrics': {u'loadedMapCacheHitCount': 0, u'loadedMapCacheMissCount': 0, u'stateOnCurrentVersionSizeBytes': 189591}, u'numRowsTotal': 637, u'memoryUsedBytes': 218391, u'numRowsUpdated': 0}], u'eventTime': {u'watermark': u'2020-01-22T21:16:22.974Z'}, u'name': None, u'timestamp': u'2020-01-22T21:37:45.001Z', u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'batchId': 1, u'sources': [{u'description': u'FileStreamSource[s3a://j2training/data]', u'endOffset': {u'logOffset': 0}, u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'startOffset': {u'logOffset': 0}}], u'durationMs': {u'getOffset': 801, u'triggerExecution': 801}, u'runId': u'f3c88b58-0c58-4acb-bd6f-1dc45bfbc07d', u'id': u'3b9f6080-6a97-4d5c-9811-869387ecbfa8', u'sink': {u'description': u'ForeachBatchSink'}}
[0;31m[0m
[0;31mKeyboardInterrupt[0mTraceback (most recent call last)
[0;32m<ipython-input-192-7e8dde629d4a>[0m in [0;36m<module>[0;34m()[0m
[1;32m      7[0m [0;34m[0m[0m
[1;32m      8[0m [0;32mwhile[0m [0;32mnot[0m [0mterminate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m----> 9[0;31m     [0mtime[0m[0;34m.[0m[0msleep[0m[0;34m([0m[0;36m10[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     10[0m     [0;32mprint[0m [0;34m"looping"[0m[0;34m[0m[0m
[1;32m     11[0m     [0;32mprint[0m [0mquery[0m[0;34m.[0m[0mlastProgress[0m[0;34m[0m[0m

[0;31mKeyboardInterrupt[0m: 
 INFO [2020-01-22 21:37:55,217] ({pool-2-thread-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:37:55,230] ({pool-2-thread-11} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:38:03,791] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:38:03,805] ({pool-2-thread-78} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:38:03,809] ({pool-2-thread-78} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 21:38:03,907] ({pool-2-thread-78} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-051101_1395511314 is finished, status: ERROR, exception: null, result: %text [0;36m  File [0;32m"<ipython-input-194-f2b0dd41b8b3>"[0;36m, line [0;32m39[0m
[0;31m    print dfJson.rdd..collect()[0m
[0m                     ^[0m
[0;31mSyntaxError[0m[0;31m:[0m invalid syntax

 INFO [2020-01-22 21:38:03,926] ({pool-2-thread-78} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:38:03,940] ({pool-2-thread-78} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:38:11,291] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:38:11,302] ({pool-2-thread-40} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:38:11,304] ({pool-2-thread-40} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:38:11,392] ({pool-2-thread-40} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:38:11,405] ({pool-2-thread-40} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:38:11,416] ({pool-2-thread-40} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:38:13,081] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:38:13,092] ({pool-2-thread-79} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:38:13,093] ({pool-2-thread-79} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 21:38:30,783] ({pool-8-thread-1} AppendOutputRunner.java[run]:104) - Processing size for buffered append-output is high: 104383 characters.
 WARN [2020-01-22 21:38:48,714] ({pool-2-thread-79} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ABORT, exception: null, result: %text <pyspark.sql.streaming.StreamingQuery object at 0x7f98493f6ad0>
dfis MapPartitionsRDD[588] at javaToPython at <unknown>:0
looping
None
count is 637
[Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'hi', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:45 +0000 2020', lang=u'uk', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:33 +0000 2020', lang=u'und', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:47 +0000 2020', lang=u'ko', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'en', count=10, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:47 +0000 2020', lang=u'pt', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:42 +0000 2020', lang=u'en', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:39 +0000 2020', lang=u'es', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'en', count=12, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:34 +0000 2020', lang=u'th', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Peing', created_at=u'Fri Jan 17 15:47:45 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:41 +0000 2020', lang=u'tr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'in', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:41 +0000 2020', lang=u'en', count=24, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:50 +0000 2020', lang=u'tr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'tr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:35 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:56 +0000 2020', lang=u'es', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:35 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:42 +0000 2020', lang=u'en', count=18, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:34 +0000 2020', lang=u'en', count=14, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:51 +0000 2020', lang=u'hi', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:34 +0000 2020', lang=u'ar', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:33 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:55 +0000 2020', lang=u'es', count=10, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:45 +0000 2020', lang=u'en', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'BelugaCampaignSEA', created_at=u'Fri Jan 17 15:47:41 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'ko', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:45 +0000 2020', lang=u'ja', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:38 +0000 2020', lang=u'en', count=22, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Instagram', created_at=u'Fri Jan 17 15:47:35 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'en', count=16, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:41 +0000 2020', lang=u'es', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:34 +0000 2020', lang=u'pt', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:55 +0000 2020', lang=u'en', count=18, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:39 +0000 2020', lang=u'ro', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:35 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'twittbot.net', created_at=u'Fri Jan 17 15:47:33 +0000 2020', lang=u'zh', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:35 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'tl', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:54 +0000 2020', lang=u'fr', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:52 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPad', created_at=u'Fri Jan 17 15:47:33 +0000 2020', lang=u'th', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:40 +0000 2020', lang=u'ja', count=10, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:51 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:45 +0000 2020', lang=u'und', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:33 +0000 2020', lang=u'th', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:42 +0000 2020', lang=u'in', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:47 +0000 2020', lang=u'hi', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Mobile Web (M2)', created_at=u'Fri Jan 17 15:47:34 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:46 +0000 2020', lang=u'es', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'ja', count=12, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:32 +0000 2020', lang=u'pt', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:40 +0000 2020', lang=u'es', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:39 +0000 2020', lang=u'en', count=16, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:40 +0000 2020', lang=u'hi', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:38 +0000 2020', lang=u'es', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:33 +0000 2020', lang=u'it', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:56 +0000 2020', lang=u'in', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:54 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Untappd', created_at=u'Fri Jan 17 15:47:42 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'twitcle plus for iOS', created_at=u'Fri Jan 17 15:47:35 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:38 +0000 2020', lang=u'ca', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:49 +0000 2020', lang=u'ar', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'pt', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:40 +0000 2020', lang=u'th', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'twittbot.net', created_at=u'Fri Jan 17 15:47:52 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:52 +0000 2020', lang=u'fa', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:44 +0000 2020', lang=u'tl', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'en', count=10, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:32 +0000 2020', lang=u'ja', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:46 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:35 +0000 2020', lang=u'fr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:34 +0000 2020', lang=u'th', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:55 +0000 2020', lang=u'pt', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'hi', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'en', count=16, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'fr', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:39 +0000 2020', lang=u'en', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:54 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:44 +0000 2020', lang=u'en', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'TweetDeck', created_at=u'Fri Jan 17 15:47:49 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'fa', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:55 +0000 2020', lang=u'en', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'ar', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:41 +0000 2020', lang=u'pt', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPad', created_at=u'Fri Jan 17 15:47:49 +0000 2020', lang=u'th', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'ar', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'tl', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:52 +0000 2020', lang=u'en', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'pt', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Facebook', created_at=u'Fri Jan 17 15:47:44 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:55 +0000 2020', lang=u'ja', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:49 +0000 2020', lang=u'hi', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'en', count=14, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:46 +0000 2020', lang=u'es', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Instagram', created_at=u'Fri Jan 17 15:47:44 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:39 +0000 2020', lang=u'en', count=10, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'th', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:51 +0000 2020', lang=u'fa', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:32 +0000 2020', lang=u'en', count=12, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:39 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:55 +0000 2020', lang=u'th', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'pt', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:44 +0000 2020', lang=u'pt', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'TweetDeck', created_at=u'Fri Jan 17 15:47:55 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPad', created_at=u'Fri Jan 17 15:47:51 +0000 2020', lang=u'es', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'en', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:47 +0000 2020', lang=u'tl', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:52 +0000 2020', lang=u'ja', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:33 +0000 2020', lang=u'in', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:47 +0000 2020', lang=u'ar', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:47 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:55 +0000 2020', lang=u'zh', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:47 +0000 2020', lang=u'ur', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:41 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web Client', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:35 +0000 2020', lang=u'fr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:39 +0000 2020', lang=u'pt', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'ca', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:44 +0000 2020', lang=u'es', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:46 +0000 2020', lang=u'tr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:50 +0000 2020', lang=u'tl', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:39 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Facebook', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:52 +0000 2020', lang=u'pt', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:32 +0000 2020', lang=u'es', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:51 +0000 2020', lang=u'und', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:55 +0000 2020', lang=u'ar', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:32 +0000 2020', lang=u'in', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:35 +0000 2020', lang=u'en', count=20, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:38 +0000 2020', lang=u'pt', count=10, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'ja', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'ko', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'ja', count=14, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:52 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'und', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:49 +0000 2020', lang=u'ja', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:50 +0000 2020', lang=u'pt', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:42 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:40 +0000 2020', lang=u'tl', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:38 +0000 2020', lang=u'th', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:38 +0000 2020', lang=u'ja', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'pl', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:51 +0000 2020', lang=u'ko', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'4hiphop', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:54 +0000 2020', lang=u'en', count=30, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:54 +0000 2020', lang=u'th', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'TweetDeck', created_at=u'Fri Jan 17 15:47:39 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'th', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:41 +0000 2020', lang=u'es', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:47 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:49 +0000 2020', lang=u'ja', count=12, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:35 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'th', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'TweetDeck', created_at=u'Fri Jan 17 15:47:56 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:42 +0000 2020', lang=u'ar', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'pt', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:34 +0000 2020', lang=u'et', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:39 +0000 2020', lang=u'pt', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:56 +0000 2020', lang=u'sv', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:45 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'th', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:55 +0000 2020', lang=u'ko', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:52 +0000 2020', lang=u'pl', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'hi', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:46 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'fr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:44 +0000 2020', lang=u'tr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'TweetDeck', created_at=u'Fri Jan 17 15:47:52 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:35 +0000 2020', lang=u'hi', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'pt', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'fr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Radio.co now playing', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:35 +0000 2020', lang=u'tr', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:54 +0000 2020', lang=u'tr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:38 +0000 2020', lang=u'ja', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:50 +0000 2020', lang=u'ar', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:40 +0000 2020', lang=u'en', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:42 +0000 2020', lang=u'fr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:56 +0000 2020', lang=u'tl', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:54 +0000 2020', lang=u'ja', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:33 +0000 2020', lang=u'es', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:40 +0000 2020', lang=u'ar', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:46 +0000 2020', lang=u'tl', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:42 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPad', created_at=u'Fri Jan 17 15:47:39 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'VidaDeHumanas', created_at=u'Fri Jan 17 15:47:49 +0000 2020', lang=u'pt', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'DDSuriyaSivakumar', created_at=u'Fri Jan 17 15:47:54 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'pt', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'es', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:46 +0000 2020', lang=u'et', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:32 +0000 2020', lang=u'ar', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'hi', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'ja', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:50 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:56 +0000 2020', lang=u'it', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:51 +0000 2020', lang=u'pt', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:52 +0000 2020', lang=u'en', count=18, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:47 +0000 2020', lang=u'ja', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'f3_app', created_at=u'Fri Jan 17 15:47:41 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:41 +0000 2020', lang=u'th', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:51 +0000 2020', lang=u'th', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'ar', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:50 +0000 2020', lang=u'ru', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web Client', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPad', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:40 +0000 2020', lang=u'ko', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Tabtter Free', created_at=u'Fri Jan 17 15:47:45 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:38 +0000 2020', lang=u'en', count=16, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:50 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'en', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:35 +0000 2020', lang=u'hi', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:54 +0000 2020', lang=u'tl', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'TweetDeck', created_at=u'Fri Jan 17 15:47:47 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:40 +0000 2020', lang=u'und', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'ur', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:52 +0000 2020', lang=u'pt', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'twittbot.net', created_at=u'Fri Jan 17 15:47:49 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:49 +0000 2020', lang=u'ko', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'es', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:45 +0000 2020', lang=u'pl', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:41 +0000 2020', lang=u'ja', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'ar', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'TweetDeck', created_at=u'Fri Jan 17 15:47:45 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:41 +0000 2020', lang=u'und', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'ar', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:51 +0000 2020', lang=u'en', count=20, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:44 +0000 2020', lang=u'pt', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:52 +0000 2020', lang=u'hi', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:50 +0000 2020', lang=u'en', count=20, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'TweetDeck', created_at=u'Fri Jan 17 15:47:35 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'ko', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:39 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:33 +0000 2020', lang=u'es', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:46 +0000 2020', lang=u'es', count=10, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:42 +0000 2020', lang=u'fr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'pt', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:54 +0000 2020', lang=u'fa', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:56 +0000 2020', lang=u'ko', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:35 +0000 2020', lang=u'th', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:33 +0000 2020', lang=u'ko', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:41 +0000 2020', lang=u'pt', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:38 +0000 2020', lang=u'ja', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'ko', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:46 +0000 2020', lang=u'th', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:41 +0000 2020', lang=u'ar', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:42 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:38 +0000 2020', lang=u'und', count=10, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:55 +0000 2020', lang=u'es', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:45 +0000 2020', lang=u'hi', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:33 +0000 2020', lang=u'ja', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:41 +0000 2020', lang=u'th', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:44 +0000 2020', lang=u'en', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:46 +0000 2020', lang=u'tr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:52 +0000 2020', lang=u'fr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:41 +0000 2020', lang=u'fr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'und', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:32 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'und', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:55 +0000 2020', lang=u'und', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:56 +0000 2020', lang=u'th', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:54 +0000 2020', lang=u'en', count=18, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:46 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPad', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'th', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'fr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:44 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:40 +0000 2020', lang=u'zh', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:40 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:42 +0000 2020', lang=u'ja', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:47 +0000 2020', lang=u'und', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:54 +0000 2020', lang=u'in', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:35 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:42 +0000 2020', lang=u'en', count=10, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:52 +0000 2020', lang=u'ja', count=18, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:54 +0000 2020', lang=u'it', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:51 +0000 2020', lang=u'pt', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'twittbot.net', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'und', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:56 +0000 2020', lang=u'ja', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'ht', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:33 +0000 2020', lang=u'en', count=16, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPad', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:44 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:49 +0000 2020', lang=u'ar', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:40 +0000 2020', lang=u'en', count=14, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:45 +0000 2020', lang=u'en', count=16, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'ar', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:52 +0000 2020', lang=u'es', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:51 +0000 2020', lang=u'nl', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:44 +0000 2020', lang=u'ja', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:50 +0000 2020', lang=u'ja', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'in', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:38 +0000 2020', lang=u'ar', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'twittbot.net', created_at=u'Fri Jan 17 15:47:33 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:55 +0000 2020', lang=u'ko', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:50 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:55 +0000 2020', lang=u'hi', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPad', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Janetter', created_at=u'Fri Jan 17 15:47:50 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'cmsn-dev', created_at=u'Fri Jan 17 15:47:33 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:32 +0000 2020', lang=u'en', count=18, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'ar', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Tweetbot for i\xce\x9fS', created_at=u'Fri Jan 17 15:47:38 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:46 +0000 2020', lang=u'th', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:34 +0000 2020', lang=u'fr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:50 +0000 2020', lang=u'und', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:34 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:49 +0000 2020', lang=u'th', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'th', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'in', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:42 +0000 2020', lang=u'th', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:34 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'TweetDeck', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:34 +0000 2020', lang=u'it', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:49 +0000 2020', lang=u'pt', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:56 +0000 2020', lang=u'en', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'en', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:44 +0000 2020', lang=u'ar', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'en', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web Client', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:39 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:45 +0000 2020', lang=u'th', count=12, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:46 +0000 2020', lang=u'it', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'tr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:42 +0000 2020', lang=u'ar', count=10, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'ar', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'ko', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'ja', count=10, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'th', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:33 +0000 2020', lang=u'in', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:41 +0000 2020', lang=u'in', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'hi', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:55 +0000 2020', lang=u'th', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'fr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:47 +0000 2020', lang=u'pt', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'twittbot.net', created_at=u'Fri Jan 17 15:47:42 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:46 +0000 2020', lang=u'ko', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:44 +0000 2020', lang=u'ja', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:41 +0000 2020', lang=u'ar', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:40 +0000 2020', lang=u'th', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitcom - Comunidades ', created_at=u'Fri Jan 17 15:47:34 +0000 2020', lang=u'pt', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:32 +0000 2020', lang=u'ar', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:45 +0000 2020', lang=u'es', count=10, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'es', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:42 +0000 2020', lang=u'in', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:46 +0000 2020', lang=u'pt', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:35 +0000 2020', lang=u'en', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:38 +0000 2020', lang=u'ko', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'in', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Echoes Act2', created_at=u'Fri Jan 17 15:47:50 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'ta', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPad', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:33 +0000 2020', lang=u'pt', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:46 +0000 2020', lang=u'hi', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:56 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:32 +0000 2020', lang=u'th', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:46 +0000 2020', lang=u'fi', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'in', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Flamingo for Android', created_at=u'Fri Jan 17 15:47:38 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:39 +0000 2020', lang=u'fr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'cy', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'et', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:35 +0000 2020', lang=u'es', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:40 +0000 2020', lang=u'in', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'es', count=10, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:39 +0000 2020', lang=u'es', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPad', created_at=u'Fri Jan 17 15:47:39 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:49 +0000 2020', lang=u'th', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'\xe3\x82\xb0\xe3\x83\xa9\xe3\x83\xb3\xe3\x83\x96\xe3\x83\xab\xe3\x83\xbc \xe3\x83\x95\xe3\x82\xa1\xe3\x83\xb3\xe3\x82\xbf\xe3\x82\xb8\xe3\x83\xbc', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'ja', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:47 +0000 2020', lang=u'en', count=22, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:46 +0000 2020', lang=u'en', count=16, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:35 +0000 2020', lang=u'ja', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:45 +0000 2020', lang=u'in', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:39 +0000 2020', lang=u'tl', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Facebook', created_at=u'Fri Jan 17 15:47:42 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:51 +0000 2020', lang=u'ar', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:34 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:45 +0000 2020', lang=u'pt', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'BIGO LIVE', created_at=u'Fri Jan 17 15:47:44 +0000 2020', lang=u'ar', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Google', created_at=u'Fri Jan 17 15:47:50 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'th', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:34 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:45 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:38 +0000 2020', lang=u'es', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'en', count=22, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:50 +0000 2020', lang=u'en', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:51 +0000 2020', lang=u'en', count=10, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:38 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'tr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:39 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPad', created_at=u'Fri Jan 17 15:47:50 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:46 +0000 2020', lang=u'en', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:54 +0000 2020', lang=u'fr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:50 +0000 2020', lang=u'pt', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:49 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:32 +0000 2020', lang=u'ja', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPad', created_at=u'Fri Jan 17 15:47:45 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:34 +0000 2020', lang=u'pl', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:44 +0000 2020', lang=u'ar', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'\xe3\x82\xb0\xe3\x83\xa9\xe3\x83\xb3\xe3\x83\x96\xe3\x83\xab\xe3\x83\xbc \xe3\x83\x95\xe3\x82\xa1\xe3\x83\xb3\xe3\x82\xbf\xe3\x82\xb8\xe3\x83\xbc', created_at=u'Fri Jan 17 15:47:34 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:32 +0000 2020', lang=u'el', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:41 +0000 2020', lang=u'ca', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:55 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:51 +0000 2020', lang=u'ar', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:41 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:54 +0000 2020', lang=u'es', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:46 +0000 2020', lang=u'en', count=12, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:47 +0000 2020', lang=u'en', count=20, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:52 +0000 2020', lang=u'de', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:55 +0000 2020', lang=u'in', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:45 +0000 2020', lang=u'es', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:34 +0000 2020', lang=u'es', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:33 +0000 2020', lang=u'en', count=14, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:41 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:46 +0000 2020', lang=u'pt', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:40 +0000 2020', lang=u'fr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:55 +0000 2020', lang=u'ar', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:50 +0000 2020', lang=u'ar', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'pt', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:54 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:39 +0000 2020', lang=u'ja', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:56 +0000 2020', lang=u'en', count=10, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:32 +0000 2020', lang=u'th', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:35 +0000 2020', lang=u'en', count=16, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:32 +0000 2020', lang=u'en', count=10, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:49 +0000 2020', lang=u'es', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:51 +0000 2020', lang=u'und', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:44 +0000 2020', lang=u'ja', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:35 +0000 2020', lang=u'es', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:34 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Peing', created_at=u'Fri Jan 17 15:47:46 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'in', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'ar', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'es', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:34 +0000 2020', lang=u'es', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'es', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'ar', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:55 +0000 2020', lang=u'en', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:52 +0000 2020', lang=u'th', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:40 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:35 +0000 2020', lang=u'ko', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Instagram', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'tr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:50 +0000 2020', lang=u'ja', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:33 +0000 2020', lang=u'th', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:49 +0000 2020', lang=u'en', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:54 +0000 2020', lang=u'th', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:40 +0000 2020', lang=u'pt', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:46 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'ja', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPad', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'pt', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'pl', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'twittbot.net', created_at=u'Fri Jan 17 15:47:38 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'en', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:33 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'ja', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:49 +0000 2020', lang=u'my', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:50 +0000 2020', lang=u'und', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'dlvr.it', created_at=u'Fri Jan 17 15:47:32 +0000 2020', lang=u'pt', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:33 +0000 2020', lang=u'pt', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'pt', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'ja', count=10, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:56 +0000 2020', lang=u'fr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:39 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Facebook', created_at=u'Fri Jan 17 15:47:39 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPad', created_at=u'Fri Jan 17 15:47:34 +0000 2020', lang=u'en', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'es', count=10, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:45 +0000 2020', lang=u'fr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:55 +0000 2020', lang=u'tl', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:35 +0000 2020', lang=u'fr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:45 +0000 2020', lang=u'es', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'en', count=18, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:45 +0000 2020', lang=u'ja', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'TwitCasting', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:34 +0000 2020', lang=u'en', count=12, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:42 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:54 +0000 2020', lang=u'en', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'tr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:51 +0000 2020', lang=u'fa', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:44 +0000 2020', lang=u'th', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:42 +0000 2020', lang=u'th', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:47 +0000 2020', lang=u'in', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:51 +0000 2020', lang=u'ja', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPad', created_at=u'Fri Jan 17 15:47:42 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:40 +0000 2020', lang=u'es', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:38 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:45 +0000 2020', lang=u'th', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:42 +0000 2020', lang=u'es', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web Client', created_at=u'Fri Jan 17 15:47:44 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:32 +0000 2020', lang=u'ar', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:56 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:39 +0000 2020', lang=u'th', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:32 +0000 2020', lang=u'es', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:51 +0000 2020', lang=u'th', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:46 +0000 2020', lang=u'pt', count=10, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:41 +0000 2020', lang=u'en', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:35 +0000 2020', lang=u'hu', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:56 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Dynamic Tweets', created_at=u'Fri Jan 17 15:47:52 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:49 +0000 2020', lang=u'und', count=10, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:50 +0000 2020', lang=u'in', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:33 +0000 2020', lang=u'ar', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:55 +0000 2020', lang=u'in', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:47 +0000 2020', lang=u'ko', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'ar', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Xbox One Social', created_at=u'Fri Jan 17 15:47:41 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:32 +0000 2020', lang=u'fa', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'es', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'dlvr.it', created_at=u'Fri Jan 17 15:47:32 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:56 +0000 2020', lang=u'ru', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'und', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:32 +0000 2020', lang=u'ko', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:40 +0000 2020', lang=u'in', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:32 +0000 2020', lang=u'pt', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:52 +0000 2020', lang=u'ar', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:49 +0000 2020', lang=u'en', count=22, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:40 +0000 2020', lang=u'sr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:44 +0000 2020', lang=u'in', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'th', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:41 +0000 2020', lang=u'tl', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'TweetDeck', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:49 +0000 2020', lang=u'fa', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:44 +0000 2020', lang=u'es', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:41 +0000 2020', lang=u'en', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:49 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:49 +0000 2020', lang=u'tr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'SocialFlow', created_at=u'Fri Jan 17 15:47:32 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:38 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'in', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'\xec\x96\xb8\xeb\x8d\x94\xec\xbb\xa4\xeb\xb2\x84', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:51 +0000 2020', lang=u'es', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'und', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:56 +0000 2020', lang=u'ja', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Google', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'pt', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPad', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'th', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:38 +0000 2020', lang=u'in', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:33 +0000 2020', lang=u'fr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:47 +0000 2020', lang=u'en', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:52 +0000 2020', lang=u'th', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:35 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:40 +0000 2020', lang=u'ja', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:52 +0000 2020', lang=u'tr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:52 +0000 2020', lang=u'und', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web Client', created_at=u'Fri Jan 17 15:47:56 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'es', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'ja', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:35 +0000 2020', lang=u'pt', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:44 +0000 2020', lang=u'fr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:49 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'tl', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:34 +0000 2020', lang=u'ja', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'TwitPane for Android', created_at=u'Fri Jan 17 15:47:49 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Facebook', created_at=u'Fri Jan 17 15:47:40 +0000 2020', lang=u'pl', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:42 +0000 2020', lang=u'es', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:34 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:52 +0000 2020', lang=u'en', count=16, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:33 +0000 2020', lang=u'pt', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:33 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'TweetDeck', created_at=u'Fri Jan 17 15:47:51 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'fr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:56 +0000 2020', lang=u'en', count=16, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:34 +0000 2020', lang=u'es', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:54 +0000 2020', lang=u'sr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:56 +0000 2020', lang=u'ur', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'es', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:51 +0000 2020', lang=u'ja', count=10, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'TweetDeck', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:33 +0000 2020', lang=u'tl', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitcom - Comunidades ', created_at=u'Fri Jan 17 15:47:50 +0000 2020', lang=u'pt', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:55 +0000 2020', lang=u'tr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web Client', created_at=u'Fri Jan 17 15:47:50 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:45 +0000 2020', lang=u'ar', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:41 +0000 2020', lang=u'es', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'twittbot.net', created_at=u'Fri Jan 17 15:47:32 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:44 +0000 2020', lang=u'und', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:37 +0000 2020', lang=u'en', count=20, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:35 +0000 2020', lang=u'in', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'SNPD', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'es', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:49 +0000 2020', lang=u'es', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:42 +0000 2020', lang=u'pt', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:49 +0000 2020', lang=u'en', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'ne', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:33 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:34 +0000 2020', lang=u'pt', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:44 +0000 2020', lang=u'th', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:45 +0000 2020', lang=u'in', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'ko', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:50 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:54 +0000 2020', lang=u'ar', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:46 +0000 2020', lang=u'ar', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:55 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:45 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'und', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:48 +0000 2020', lang=u'tr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:44 +0000 2020', lang=u'ru', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:44 +0000 2020', lang=u'en', count=16, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPad', created_at=u'Fri Jan 17 15:47:34 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:45 +0000 2020', lang=u'ko', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPad', created_at=u'Fri Jan 17 15:47:47 +0000 2020', lang=u'en', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'pl', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:54 +0000 2020', lang=u'pt', count=12, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'twittbot.net', created_at=u'Fri Jan 17 15:47:32 +0000 2020', lang=u'ko', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:47 +0000 2020', lang=u'ja', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web Client', created_at=u'Fri Jan 17 15:47:56 +0000 2020', lang=u'ar', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:52 +0000 2020', lang=u'in', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:42 +0000 2020', lang=u'zh', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Tween', created_at=u'Fri Jan 17 15:47:52 +0000 2020', lang=u'ja', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:54 +0000 2020', lang=u'ja', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:35 +0000 2020', lang=u'pt', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:49 +0000 2020', lang=u'es', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:47 +0000 2020', lang=u'th', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:55 +0000 2020', lang=u'tr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:34 +0000 2020', lang=u'fr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'in', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'es', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'WordpressBotDocker', created_at=u'Fri Jan 17 15:47:33 +0000 2020', lang=u'de', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:38 +0000 2020', lang=u'ht', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:50 +0000 2020', lang=u'th', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:49 +0000 2020', lang=u'ur', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'und', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:52 +0000 2020', lang=u'und', count=4, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:38 +0000 2020', lang=u'es', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:53 +0000 2020', lang=u'th', count=6, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:43 +0000 2020', lang=u'fr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter Web App', created_at=u'Fri Jan 17 15:47:40 +0000 2020', lang=u'en', count=12, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for iPhone', created_at=u'Fri Jan 17 15:47:36 +0000 2020', lang=u'fr', count=2, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518)), Row(source=u'Twitter for Android', created_at=u'Fri Jan 17 15:47:47 +0000 2020', lang=u'es', count=8, timestamp=datetime.datetime(2020, 1, 22, 21, 17, 22, 974518))]
%html <div class="result-alert alert-warning" role="alert"><button type="button" class="close" data-dismiss="alert" aria-label="Close"><span aria-hidden="true">&times;</span></button><strong>Output is truncated</strong> to 102400 bytes. Learn more about <strong>ZEPPELIN_INTERPRETER_OUTPUT_LIMIT</strong></div>
 INFO [2020-01-22 21:38:48,723] ({pool-2-thread-79} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:38:48,741] ({pool-2-thread-79} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:39:29,733] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:40:07,631] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:40:07,649] ({pool-2-thread-21} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:40:07,649] ({pool-2-thread-21} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 21:40:07,725] ({pool-2-thread-21} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-051101_1395511314 is finished, status: ERROR, exception: null, result: %text [0;36m  File [0;32m"<ipython-input-200-967bd1f2b6c0>"[0;36m, line [0;32m39[0m
[0;31m    print dfJson.rdd.mapPartitions(lambda x:x).foreachRDD(lambda x: print(x))[0m
[0m        ^[0m
[0;31mSyntaxError[0m[0;31m:[0m invalid syntax

 INFO [2020-01-22 21:40:07,741] ({pool-2-thread-21} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:40:07,780] ({pool-2-thread-21} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:40:12,892] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:40:12,909] ({pool-2-thread-80} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:40:12,909] ({pool-2-thread-80} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 21:40:12,985] ({pool-2-thread-80} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-051101_1395511314 is finished, status: ERROR, exception: null, result: %text [0;36m  File [0;32m"<ipython-input-202-5a4455abda35>"[0;36m, line [0;32m39[0m
[0;31m    dfJson.rdd.mapPartitions(lambda x:x).foreachRDD(lambda x: print(x))[0m
[0m         ^[0m
[0;31mSyntaxError[0m[0;31m:[0m invalid syntax

 INFO [2020-01-22 21:40:13,001] ({pool-2-thread-80} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:40:13,017] ({pool-2-thread-80} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:40:20,077] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:40:20,093] ({pool-2-thread-41} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:40:20,094] ({pool-2-thread-41} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 21:40:20,181] ({pool-2-thread-41} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-051101_1395511314 is finished, status: ERROR, exception: null, result: %text [0;36m  File [0;32m"<ipython-input-204-38295ab0c648>"[0;36m, line [0;32m39[0m
[0;31m    dfJson.rdd.mapPartitions(lambda x:x).foreachRDD(lambda x: print(x))[0m
[0m                                                                  ^[0m
[0;31mSyntaxError[0m[0;31m:[0m invalid syntax

 INFO [2020-01-22 21:40:20,198] ({pool-2-thread-41} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:40:20,217] ({pool-2-thread-41} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:40:25,964] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:40:25,980] ({pool-2-thread-81} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:40:25,981] ({pool-2-thread-81} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 21:40:26,110] ({pool-2-thread-81} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-051101_1395511314 is finished, status: ERROR, exception: null, result: %text [0;36m  File [0;32m"<ipython-input-206-f85d2c9ac460>"[0;36m, line [0;32m39[0m
[0;31m    dfJson.rdd.mapPartitions(lambda x:x).foreachRDD(lambda x: print x )[0m
[0m                                                                  ^[0m
[0;31mSyntaxError[0m[0;31m:[0m invalid syntax

 INFO [2020-01-22 21:40:26,137] ({pool-2-thread-81} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:40:26,153] ({pool-2-thread-81} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:40:35,739] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:40:35,755] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:40:35,755] ({pool-2-thread-4} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:40:35,849] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:40:35,864] ({pool-2-thread-4} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:40:35,880] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:40:53,097] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:43:37,537] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:43:37,554] ({pool-2-thread-82} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:43:37,554] ({pool-2-thread-82} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:43:37,646] ({pool-2-thread-82} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:43:37,656] ({pool-2-thread-82} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:43:37,671] ({pool-2-thread-82} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:43:39,509] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:43:39,528] ({pool-2-thread-42} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:43:39,528] ({pool-2-thread-42} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:47:31,025] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 WARN [2020-01-22 21:47:31,143] ({pool-2-thread-42} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ABORT, exception: null, result: %text <pyspark.sql.streaming.StreamingQuery object at 0x7f985533fa50>
dfis MapPartitionsRDD[610] at javaToPython at <unknown>:0
looping
None
count is 637
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
looping
None
[0;31m[0m
[0;31mKeyboardInterrupt[0mTraceback (most recent call last)
[0;32m<ipython-input-212-7e8dde629d4a>[0m in [0;36m<module>[0;34m()[0m
[1;32m      7[0m [0;34m[0m[0m
[1;32m      8[0m [0;32mwhile[0m [0;32mnot[0m [0mterminate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m----> 9[0;31m     [0mtime[0m[0;34m.[0m[0msleep[0m[0;34m([0m[0;36m10[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     10[0m     [0;32mprint[0m [0;34m"looping"[0m[0;34m[0m[0m
[1;32m     11[0m     [0;32mprint[0m [0mquery[0m[0;34m.[0m[0mlastProgress[0m[0;34m[0m[0m

[0;31mKeyboardInterrupt[0m: 
 INFO [2020-01-22 21:47:31,161] ({pool-2-thread-42} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:47:31,172] ({pool-2-thread-42} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:47:33,829] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:47:33,890] ({pool-2-thread-83} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:47:33,891] ({pool-2-thread-83} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:47:34,053] ({pool-2-thread-83} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:47:34,074] ({pool-2-thread-83} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:47:34,086] ({pool-2-thread-83} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:47:35,587] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:47:35,600] ({pool-2-thread-22} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:47:35,600] ({pool-2-thread-22} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 21:49:46,419] ({pool-2-thread-22} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ABORT, exception: null, result: %text <pyspark.sql.streaming.StreamingQuery object at 0x7f984939fd50>
dfis MapPartitionsRDD[632] at javaToPython at <unknown>:0
looping
None
count is 637
looping
{u'stateOperators': [{u'customMetrics': {u'loadedMapCacheHitCount': 0, u'loadedMapCacheMissCount': 0, u'stateOnCurrentVersionSizeBytes': 189591}, u'numRowsTotal': 637, u'memoryUsedBytes': 218391, u'numRowsUpdated': 0}], u'eventTime': {u'watermark': u'2020-01-22T21:16:22.974Z'}, u'name': None, u'timestamp': u'2020-01-22T21:47:51.901Z', u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'batchId': 1, u'sources': [{u'description': u'FileStreamSource[s3a://j2training/data]', u'endOffset': {u'logOffset': 0}, u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'startOffset': {u'logOffset': 0}}], u'durationMs': {u'getOffset': 727, u'triggerExecution': 727}, u'runId': u'47edc142-2786-420d-9a3f-19327e1e289d', u'id': u'1aa50637-3cc4-4f0c-8a80-82e7e3ed59a6', u'sink': {u'description': u'ForeachBatchSink'}}
looping
{u'stateOperators': [{u'customMetrics': {u'loadedMapCacheHitCount': 0, u'loadedMapCacheMissCount': 0, u'stateOnCurrentVersionSizeBytes': 189591}, u'numRowsTotal': 637, u'memoryUsedBytes': 218391, u'numRowsUpdated': 0}], u'eventTime': {u'watermark': u'2020-01-22T21:16:22.974Z'}, u'name': None, u'timestamp': u'2020-01-22T21:47:51.901Z', u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'batchId': 1, u'sources': [{u'description': u'FileStreamSource[s3a://j2training/data]', u'endOffset': {u'logOffset': 0}, u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'startOffset': {u'logOffset': 0}}], u'durationMs': {u'getOffset': 727, u'triggerExecution': 727}, u'runId': u'47edc142-2786-420d-9a3f-19327e1e289d', u'id': u'1aa50637-3cc4-4f0c-8a80-82e7e3ed59a6', u'sink': {u'description': u'ForeachBatchSink'}}
looping
{u'stateOperators': [{u'customMetrics': {u'loadedMapCacheHitCount': 0, u'loadedMapCacheMissCount': 0, u'stateOnCurrentVersionSizeBytes': 189591}, u'numRowsTotal': 637, u'memoryUsedBytes': 218391, u'numRowsUpdated': 0}], u'eventTime': {u'watermark': u'2020-01-22T21:16:22.974Z'}, u'name': None, u'timestamp': u'2020-01-22T21:48:15.000Z', u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'batchId': 1, u'sources': [{u'description': u'FileStreamSource[s3a://j2training/data]', u'endOffset': {u'logOffset': 0}, u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'startOffset': {u'logOffset': 0}}], u'durationMs': {u'getOffset': 737, u'triggerExecution': 738}, u'runId': u'47edc142-2786-420d-9a3f-19327e1e289d', u'id': u'1aa50637-3cc4-4f0c-8a80-82e7e3ed59a6', u'sink': {u'description': u'ForeachBatchSink'}}
looping
{u'stateOperators': [{u'customMetrics': {u'loadedMapCacheHitCount': 0, u'loadedMapCacheMissCount': 0, u'stateOnCurrentVersionSizeBytes': 189591}, u'numRowsTotal': 637, u'memoryUsedBytes': 218391, u'numRowsUpdated': 0}], u'eventTime': {u'watermark': u'2020-01-22T21:16:22.974Z'}, u'name': None, u'timestamp': u'2020-01-22T21:48:15.000Z', u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'batchId': 1, u'sources': [{u'description': u'FileStreamSource[s3a://j2training/data]', u'endOffset': {u'logOffset': 0}, u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'startOffset': {u'logOffset': 0}}], u'durationMs': {u'getOffset': 737, u'triggerExecution': 738}, u'runId': u'47edc142-2786-420d-9a3f-19327e1e289d', u'id': u'1aa50637-3cc4-4f0c-8a80-82e7e3ed59a6', u'sink': {u'description': u'ForeachBatchSink'}}
looping
{u'stateOperators': [{u'customMetrics': {u'loadedMapCacheHitCount': 0, u'loadedMapCacheMissCount': 0, u'stateOnCurrentVersionSizeBytes': 189591}, u'numRowsTotal': 637, u'memoryUsedBytes': 218391, u'numRowsUpdated': 0}], u'eventTime': {u'watermark': u'2020-01-22T21:16:22.974Z'}, u'name': None, u'timestamp': u'2020-01-22T21:48:30.005Z', u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'batchId': 1, u'sources': [{u'description': u'FileStreamSource[s3a://j2training/data]', u'endOffset': {u'logOffset': 0}, u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'startOffset': {u'logOffset': 0}}], u'durationMs': {u'getOffset': 895, u'triggerExecution': 896}, u'runId': u'47edc142-2786-420d-9a3f-19327e1e289d', u'id': u'1aa50637-3cc4-4f0c-8a80-82e7e3ed59a6', u'sink': {u'description': u'ForeachBatchSink'}}
looping
{u'stateOperators': [{u'customMetrics': {u'loadedMapCacheHitCount': 0, u'loadedMapCacheMissCount': 0, u'stateOnCurrentVersionSizeBytes': 189591}, u'numRowsTotal': 637, u'memoryUsedBytes': 218391, u'numRowsUpdated': 0}], u'eventTime': {u'watermark': u'2020-01-22T21:16:22.974Z'}, u'name': None, u'timestamp': u'2020-01-22T21:48:30.005Z', u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'batchId': 1, u'sources': [{u'description': u'FileStreamSource[s3a://j2training/data]', u'endOffset': {u'logOffset': 0}, u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'startOffset': {u'logOffset': 0}}], u'durationMs': {u'getOffset': 895, u'triggerExecution': 896}, u'runId': u'47edc142-2786-420d-9a3f-19327e1e289d', u'id': u'1aa50637-3cc4-4f0c-8a80-82e7e3ed59a6', u'sink': {u'description': u'ForeachBatchSink'}}
looping
{u'stateOperators': [{u'customMetrics': {u'loadedMapCacheHitCount': 0, u'loadedMapCacheMissCount': 0, u'stateOnCurrentVersionSizeBytes': 189591}, u'numRowsTotal': 637, u'memoryUsedBytes': 218391, u'numRowsUpdated': 0}], u'eventTime': {u'watermark': u'2020-01-22T21:16:22.974Z'}, u'name': None, u'timestamp': u'2020-01-22T21:48:45.005Z', u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'batchId': 1, u'sources': [{u'description': u'FileStreamSource[s3a://j2training/data]', u'endOffset': {u'logOffset': 0}, u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'startOffset': {u'logOffset': 0}}], u'durationMs': {u'getOffset': 993, u'triggerExecution': 1002}, u'runId': u'47edc142-2786-420d-9a3f-19327e1e289d', u'id': u'1aa50637-3cc4-4f0c-8a80-82e7e3ed59a6', u'sink': {u'description': u'ForeachBatchSink'}}
looping
{u'stateOperators': [{u'customMetrics': {u'loadedMapCacheHitCount': 0, u'loadedMapCacheMissCount': 0, u'stateOnCurrentVersionSizeBytes': 189591}, u'numRowsTotal': 637, u'memoryUsedBytes': 218391, u'numRowsUpdated': 0}], u'eventTime': {u'watermark': u'2020-01-22T21:16:22.974Z'}, u'name': None, u'timestamp': u'2020-01-22T21:49:00.000Z', u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'batchId': 1, u'sources': [{u'description': u'FileStreamSource[s3a://j2training/data]', u'endOffset': {u'logOffset': 0}, u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'startOffset': {u'logOffset': 0}}], u'durationMs': {u'getOffset': 716, u'triggerExecution': 716}, u'runId': u'47edc142-2786-420d-9a3f-19327e1e289d', u'id': u'1aa50637-3cc4-4f0c-8a80-82e7e3ed59a6', u'sink': {u'description': u'ForeachBatchSink'}}
looping
{u'stateOperators': [{u'customMetrics': {u'loadedMapCacheHitCount': 0, u'loadedMapCacheMissCount': 0, u'stateOnCurrentVersionSizeBytes': 189591}, u'numRowsTotal': 637, u'memoryUsedBytes': 218391, u'numRowsUpdated': 0}], u'eventTime': {u'watermark': u'2020-01-22T21:16:22.974Z'}, u'name': None, u'timestamp': u'2020-01-22T21:49:15.003Z', u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'batchId': 1, u'sources': [{u'description': u'FileStreamSource[s3a://j2training/data]', u'endOffset': {u'logOffset': 0}, u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'startOffset': {u'logOffset': 0}}], u'durationMs': {u'getOffset': 821, u'triggerExecution': 821}, u'runId': u'47edc142-2786-420d-9a3f-19327e1e289d', u'id': u'1aa50637-3cc4-4f0c-8a80-82e7e3ed59a6', u'sink': {u'description': u'ForeachBatchSink'}}
looping
{u'stateOperators': [{u'customMetrics': {u'loadedMapCacheHitCount': 0, u'loadedMapCacheMissCount': 0, u'stateOnCurrentVersionSizeBytes': 189591}, u'numRowsTotal': 637, u'memoryUsedBytes': 218391, u'numRowsUpdated': 0}], u'eventTime': {u'watermark': u'2020-01-22T21:16:22.974Z'}, u'name': None, u'timestamp': u'2020-01-22T21:49:15.003Z', u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'batchId': 1, u'sources': [{u'description': u'FileStreamSource[s3a://j2training/data]', u'endOffset': {u'logOffset': 0}, u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'startOffset': {u'logOffset': 0}}], u'durationMs': {u'getOffset': 821, u'triggerExecution': 821}, u'runId': u'47edc142-2786-420d-9a3f-19327e1e289d', u'id': u'1aa50637-3cc4-4f0c-8a80-82e7e3ed59a6', u'sink': {u'description': u'ForeachBatchSink'}}
looping
{u'stateOperators': [{u'customMetrics': {u'loadedMapCacheHitCount': 0, u'loadedMapCacheMissCount': 0, u'stateOnCurrentVersionSizeBytes': 189591}, u'numRowsTotal': 637, u'memoryUsedBytes': 218391, u'numRowsUpdated': 0}], u'eventTime': {u'watermark': u'2020-01-22T21:16:22.974Z'}, u'name': None, u'timestamp': u'2020-01-22T21:49:30.004Z', u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'batchId': 1, u'sources': [{u'description': u'FileStreamSource[s3a://j2training/data]', u'endOffset': {u'logOffset': 0}, u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'startOffset': {u'logOffset': 0}}], u'durationMs': {u'getOffset': 774, u'triggerExecution': 774}, u'runId': u'47edc142-2786-420d-9a3f-19327e1e289d', u'id': u'1aa50637-3cc4-4f0c-8a80-82e7e3ed59a6', u'sink': {u'description': u'ForeachBatchSink'}}
looping
{u'stateOperators': [{u'customMetrics': {u'loadedMapCacheHitCount': 0, u'loadedMapCacheMissCount': 0, u'stateOnCurrentVersionSizeBytes': 189591}, u'numRowsTotal': 637, u'memoryUsedBytes': 218391, u'numRowsUpdated': 0}], u'eventTime': {u'watermark': u'2020-01-22T21:16:22.974Z'}, u'name': None, u'timestamp': u'2020-01-22T21:49:45.005Z', u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'batchId': 1, u'sources': [{u'description': u'FileStreamSource[s3a://j2training/data]', u'endOffset': {u'logOffset': 0}, u'processedRowsPerSecond': 0.0, u'inputRowsPerSecond': 0.0, u'numInputRows': 0, u'startOffset': {u'logOffset': 0}}], u'durationMs': {u'getOffset': 772, u'triggerExecution': 773}, u'runId': u'47edc142-2786-420d-9a3f-19327e1e289d', u'id': u'1aa50637-3cc4-4f0c-8a80-82e7e3ed59a6', u'sink': {u'description': u'ForeachBatchSink'}}
[0;31m[0m
[0;31mKeyboardInterrupt[0mTraceback (most recent call last)
[0;32m<ipython-input-216-7e8dde629d4a>[0m in [0;36m<module>[0;34m()[0m
[1;32m      7[0m [0;34m[0m[0m
[1;32m      8[0m [0;32mwhile[0m [0;32mnot[0m [0mterminate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m----> 9[0;31m     [0mtime[0m[0;34m.[0m[0msleep[0m[0;34m([0m[0;36m10[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     10[0m     [0;32mprint[0m [0;34m"looping"[0m[0;34m[0m[0m
[1;32m     11[0m     [0;32mprint[0m [0mquery[0m[0;34m.[0m[0mlastProgress[0m[0;34m[0m[0m

[0;31mKeyboardInterrupt[0m: 
 INFO [2020-01-22 21:49:46,434] ({pool-2-thread-22} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:49:46,449] ({pool-2-thread-22} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:49:52,800] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:49:52,812] ({pool-2-thread-84} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:49:52,812] ({pool-2-thread-84} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 21:50:03,020] ({pool-2-thread-84} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ERROR, exception: null, result: %text <pyspark.sql.streaming.StreamingQuery object at 0x7f9855420790>
dfis MapPartitionsRDD[655] at javaToPython at <unknown>:0
looping
[0;31m[0m
[0;31mAttributeError[0mTraceback (most recent call last)
[0;32m<ipython-input-218-8ad03699a2ad>[0m in [0;36m<module>[0;34m()[0m
[1;32m      9[0m     [0mtime[0m[0;34m.[0m[0msleep[0m[0;34m([0m[0;36m10[0m[0;34m)[0m[0;34m[0m[0m
[1;32m     10[0m     [0;32mprint[0m [0;34m"looping"[0m[0;34m[0m[0m
[0;32m---> 11[0;31m     [0;32mprint[0m [0mquery[0m[0;34m.[0m[0mstatus[0m[0;34m.[0m[0misDataAvailable[0m[0;34m[0m[0m
[0m[1;32m     12[0m     [0;31m#if query.lastProgress is not None:[0m[0;34m[0m[0;34m[0m[0m
[1;32m     13[0m         [0;31m#terminate = query.lastProgress["numInputRows"] == 0[0m[0;34m[0m[0;34m[0m[0m

[0;31mAttributeError[0m: 'dict' object has no attribute 'isDataAvailable'
 INFO [2020-01-22 21:50:03,035] ({pool-2-thread-84} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:50:03,047] ({pool-2-thread-84} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:50:13,966] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:50:14,653] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:50:14,664] ({pool-2-thread-43} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194243_52509483 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:50:14,665] ({pool-2-thread-43} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194243_52509483, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:50:14,734] ({pool-2-thread-43} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-194243_52509483 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:50:14,747] ({pool-2-thread-43} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:50:14,757] ({pool-2-thread-43} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194243_52509483 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:50:16,720] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:50:16,731] ({pool-2-thread-85} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:50:16,738] ({pool-2-thread-85} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 21:50:52,888] ({pool-2-thread-85} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ABORT, exception: null, result: %text <pyspark.sql.streaming.StreamingQuery object at 0x7f9849148250>
dfis MapPartitionsRDD[678] at javaToPython at <unknown>:0
looping
True
count is 637
looping
False
looping
False
[0;31m[0m
[0;31mKeyboardInterrupt[0mTraceback (most recent call last)
[0;32m<ipython-input-222-e22dd8d6eb08>[0m in [0;36m<module>[0;34m()[0m
[1;32m      7[0m [0;34m[0m[0m
[1;32m      8[0m [0;32mwhile[0m [0;32mnot[0m [0mterminate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m----> 9[0;31m     [0mtime[0m[0;34m.[0m[0msleep[0m[0;34m([0m[0;36m10[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     10[0m     [0;32mprint[0m [0;34m"looping"[0m[0;34m[0m[0m
[1;32m     11[0m     [0;32mprint[0m [0mquery[0m[0;34m.[0m[0mstatus[0m[0;34m[[0m[0;34m"isDataAvailable"[0m[0;34m][0m[0;34m[0m[0m

[0;31mKeyboardInterrupt[0m: 
 INFO [2020-01-22 21:50:52,899] ({pool-2-thread-85} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:50:52,910] ({pool-2-thread-85} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:51:11,174] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:51:23,485] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:51:23,497] ({pool-2-thread-12} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:51:23,497] ({pool-2-thread-12} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:51:23,610] ({pool-2-thread-12} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:51:23,619] ({pool-2-thread-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:51:23,630] ({pool-2-thread-12} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:51:25,284] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:51:25,295] ({pool-2-thread-86} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:51:25,296] ({pool-2-thread-86} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 21:51:25,386] ({pool-2-thread-86} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ERROR, exception: null, result: %text [0;36m  File [0;32m"<ipython-input-226-db49db640155>"[0;36m, line [0;32m13[0m
[0;31m    terminate = not query.status["isDataAvailable"][0m
[0m    ^[0m
[0;31mIndentationError[0m[0;31m:[0m unexpected indent

 INFO [2020-01-22 21:51:25,401] ({pool-2-thread-86} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:51:25,418] ({pool-2-thread-86} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:51:33,281] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:51:33,916] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:51:33,926] ({pool-2-thread-44} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194243_52509483 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:51:33,926] ({pool-2-thread-44} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194243_52509483, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:51:34,024] ({pool-2-thread-44} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-194243_52509483 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:51:34,035] ({pool-2-thread-44} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:51:34,045] ({pool-2-thread-44} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194243_52509483 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:51:36,805] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:51:36,818] ({pool-2-thread-87} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:51:36,819] ({pool-2-thread-87} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:52:20,742] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 WARN [2020-01-22 21:52:21,422] ({pool-2-thread-87} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ABORT, exception: null, result: %text <pyspark.sql.streaming.StreamingQuery object at 0x7f98494702d0>
dfis MapPartitionsRDD[701] at javaToPython at <unknown>:0
looping
True
count is 637
Traceback (most recent call last):
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 587, in dumps
    return cloudpickle.dumps(obj, 2)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 863, in dumps
    cp.dump(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 260, in dump
    return Pickler.dump(self, obj)
  File "/opt/conda/lib/python2.7/pickle.py", line 224, in dump
    self.save(obj)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 568, in save_tuple
    save(element)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 406, in save_function
    self.save_function_tuple(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 549, in save_function_tuple
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 606, in save_list
    self._batch_appends(iter(obj))
  File "/opt/conda/lib/python2.7/pickle.py", line 639, in _batch_appends
    save(x)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 406, in save_function
    self.save_function_tuple(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 549, in save_function_tuple
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 606, in save_list
    self._batch_appends(iter(obj))
  File "/opt/conda/lib/python2.7/pickle.py", line 639, in _batch_appends
    save(x)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 406, in save_function
    self.save_function_tuple(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 549, in save_function_tuple
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 606, in save_list
    self._batch_appends(iter(obj))
  File "/opt/conda/lib/python2.7/pickle.py", line 639, in _batch_appends
    save(x)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 406, in save_function
    self.save_function_tuple(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 549, in save_function_tuple
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 606, in save_list
    self._batch_appends(iter(obj))
  File "/opt/conda/lib/python2.7/pickle.py", line 639, in _batch_appends
    save(x)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 406, in save_function
    self.save_function_tuple(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 549, in save_function_tuple
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 606, in save_list
    self._batch_appends(iter(obj))
  File "/opt/conda/lib/python2.7/pickle.py", line 642, in _batch_appends
    save(tmp[0])
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 406, in save_function
    self.save_function_tuple(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 549, in save_function_tuple
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 606, in save_list
    self._batch_appends(iter(obj))
  File "/opt/conda/lib/python2.7/pickle.py", line 642, in _batch_appends
    save(tmp[0])
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 400, in save_function
    self.save_function_tuple(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 549, in save_function_tuple
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 331, in save
    self.save_reduce(obj=obj, *rv)
  File "/opt/conda/lib/python2.7/pickle.py", line 425, in save_reduce
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 692, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 331, in save
    self.save_reduce(obj=obj, *rv)
  File "/opt/conda/lib/python2.7/pickle.py", line 425, in save_reduce
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 331, in save
    self.save_reduce(obj=obj, *rv)
  File "/opt/conda/lib/python2.7/pickle.py", line 425, in save_reduce
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 331, in save
    self.save_reduce(obj=obj, *rv)
  File "/opt/conda/lib/python2.7/pickle.py", line 425, in save_reduce
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 331, in save
    self.save_reduce(obj=obj, *rv)
  File "/opt/conda/lib/python2.7/pickle.py", line 425, in save_reduce
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 331, in save
    self.save_reduce(obj=obj, *rv)
  File "/opt/conda/lib/python2.7/pickle.py", line 425, in save_reduce
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 331, in save
    self.save_reduce(obj=obj, *rv)
  File "/opt/conda/lib/python2.7/pickle.py", line 425, in save_reduce
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 331, in save
    self.save_reduce(obj=obj, *rv)
  File "/opt/conda/lib/python2.7/pickle.py", line 425, in save_reduce
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 306, in save
    rv = reduce(self.proto)
TypeError: can't pickle thread.lock objects
looping
False
looping
False
looping
False
[0;31m[0m
[0;31mKeyboardInterrupt[0mTraceback (most recent call last)
[0;32m<ipython-input-230-90356f1db49d>[0m in [0;36m<module>[0;34m()[0m
[1;32m      7[0m [0;34m[0m[0m
[1;32m      8[0m [0;32mwhile[0m [0;32mnot[0m [0mterminate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m----> 9[0;31m     [0mtime[0m[0;34m.[0m[0msleep[0m[0;34m([0m[0;36m10[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     10[0m     [0;32mprint[0m [0;34m"looping"[0m[0;34m[0m[0m
[1;32m     11[0m     [0;32mprint[0m [0mquery[0m[0;34m.[0m[0mstatus[0m[0;34m[[0m[0;34m"isDataAvailable"[0m[0;34m][0m[0;34m[0m[0m

[0;31mKeyboardInterrupt[0m: 
 INFO [2020-01-22 21:52:21,437] ({pool-2-thread-87} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:52:21,448] ({pool-2-thread-87} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:52:23,441] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:52:23,455] ({pool-2-thread-23} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194243_52509483 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:52:23,456] ({pool-2-thread-23} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194243_52509483, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:52:23,518] ({pool-2-thread-23} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-194243_52509483 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:52:23,533] ({pool-2-thread-23} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:52:23,544] ({pool-2-thread-23} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194243_52509483 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:52:31,963] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:52:31,975] ({pool-2-thread-88} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:52:31,975] ({pool-2-thread-88} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 21:53:40,512] ({pool-2-thread-88} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ABORT, exception: null, result: %text <pyspark.sql.streaming.StreamingQuery object at 0x7f9849442b50>
dfis MapPartitionsRDD[723] at javaToPython at <unknown>:0
looping
True
count is 637
Traceback (most recent call last):
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 587, in dumps
    return cloudpickle.dumps(obj, 2)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 863, in dumps
    cp.dump(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 260, in dump
    return Pickler.dump(self, obj)
  File "/opt/conda/lib/python2.7/pickle.py", line 224, in dump
    self.save(obj)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 568, in save_tuple
    save(element)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 406, in save_function
    self.save_function_tuple(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 549, in save_function_tuple
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 606, in save_list
    self._batch_appends(iter(obj))
  File "/opt/conda/lib/python2.7/pickle.py", line 639, in _batch_appends
    save(x)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 406, in save_function
    self.save_function_tuple(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 549, in save_function_tuple
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 606, in save_list
    self._batch_appends(iter(obj))
  File "/opt/conda/lib/python2.7/pickle.py", line 639, in _batch_appends
    save(x)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 406, in save_function
    self.save_function_tuple(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 549, in save_function_tuple
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 606, in save_list
    self._batch_appends(iter(obj))
  File "/opt/conda/lib/python2.7/pickle.py", line 639, in _batch_appends
    save(x)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 406, in save_function
    self.save_function_tuple(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 549, in save_function_tuple
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 606, in save_list
    self._batch_appends(iter(obj))
  File "/opt/conda/lib/python2.7/pickle.py", line 639, in _batch_appends
    save(x)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 406, in save_function
    self.save_function_tuple(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 549, in save_function_tuple
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 606, in save_list
    self._batch_appends(iter(obj))
  File "/opt/conda/lib/python2.7/pickle.py", line 642, in _batch_appends
    save(tmp[0])
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 406, in save_function
    self.save_function_tuple(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 549, in save_function_tuple
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 606, in save_list
    self._batch_appends(iter(obj))
  File "/opt/conda/lib/python2.7/pickle.py", line 642, in _batch_appends
    save(tmp[0])
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 400, in save_function
    self.save_function_tuple(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 549, in save_function_tuple
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 331, in save
    self.save_reduce(obj=obj, *rv)
  File "/opt/conda/lib/python2.7/pickle.py", line 425, in save_reduce
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 692, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 331, in save
    self.save_reduce(obj=obj, *rv)
  File "/opt/conda/lib/python2.7/pickle.py", line 425, in save_reduce
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 331, in save
    self.save_reduce(obj=obj, *rv)
  File "/opt/conda/lib/python2.7/pickle.py", line 425, in save_reduce
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 331, in save
    self.save_reduce(obj=obj, *rv)
  File "/opt/conda/lib/python2.7/pickle.py", line 425, in save_reduce
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 331, in save
    self.save_reduce(obj=obj, *rv)
  File "/opt/conda/lib/python2.7/pickle.py", line 425, in save_reduce
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 331, in save
    self.save_reduce(obj=obj, *rv)
  File "/opt/conda/lib/python2.7/pickle.py", line 425, in save_reduce
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 331, in save
    self.save_reduce(obj=obj, *rv)
  File "/opt/conda/lib/python2.7/pickle.py", line 425, in save_reduce
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 331, in save
    self.save_reduce(obj=obj, *rv)
  File "/opt/conda/lib/python2.7/pickle.py", line 425, in save_reduce
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 306, in save
    rv = reduce(self.proto)
TypeError: can't pickle thread.lock objects
looping
False
looping
False
looping
False
looping
False
looping
False
[0;31m[0m
[0;31mKeyboardInterrupt[0mTraceback (most recent call last)
[0;32m<ipython-input-234-90356f1db49d>[0m in [0;36m<module>[0;34m()[0m
[1;32m      7[0m [0;34m[0m[0m
[1;32m      8[0m [0;32mwhile[0m [0;32mnot[0m [0mterminate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m----> 9[0;31m     [0mtime[0m[0;34m.[0m[0msleep[0m[0;34m([0m[0;36m10[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     10[0m     [0;32mprint[0m [0;34m"looping"[0m[0;34m[0m[0m
[1;32m     11[0m     [0;32mprint[0m [0mquery[0m[0;34m.[0m[0mstatus[0m[0;34m[[0m[0;34m"isDataAvailable"[0m[0;34m][0m[0;34m[0m[0m

[0;31mKeyboardInterrupt[0m: 
 INFO [2020-01-22 21:53:40,524] ({pool-2-thread-88} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:53:40,537] ({pool-2-thread-88} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:53:42,588] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:53:42,603] ({pool-2-thread-45} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194243_52509483 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:53:42,603] ({pool-2-thread-45} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194243_52509483, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:53:42,654] ({pool-2-thread-45} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-194243_52509483 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:53:42,665] ({pool-2-thread-45} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:53:42,676] ({pool-2-thread-45} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194243_52509483 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:53:58,991] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:53:59,003] ({pool-2-thread-89} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:53:59,003] ({pool-2-thread-89} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 21:54:30,087] ({pool-2-thread-89} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ABORT, exception: null, result: %text <pyspark.sql.streaming.StreamingQuery object at 0x7f984841a690>
dfis MapPartitionsRDD[745] at javaToPython at <unknown>:0
looping
True
count is 637
Traceback (most recent call last):
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 587, in dumps
    return cloudpickle.dumps(obj, 2)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 863, in dumps
    cp.dump(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 260, in dump
    return Pickler.dump(self, obj)
  File "/opt/conda/lib/python2.7/pickle.py", line 224, in dump
    self.save(obj)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 568, in save_tuple
    save(element)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 406, in save_function
    self.save_function_tuple(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 549, in save_function_tuple
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 606, in save_list
    self._batch_appends(iter(obj))
  File "/opt/conda/lib/python2.7/pickle.py", line 639, in _batch_appends
    save(x)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 406, in save_function
    self.save_function_tuple(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 549, in save_function_tuple
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 606, in save_list
    self._batch_appends(iter(obj))
  File "/opt/conda/lib/python2.7/pickle.py", line 639, in _batch_appends
    save(x)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 406, in save_function
    self.save_function_tuple(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 549, in save_function_tuple
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 606, in save_list
    self._batch_appends(iter(obj))
  File "/opt/conda/lib/python2.7/pickle.py", line 639, in _batch_appends
    save(x)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 406, in save_function
    self.save_function_tuple(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 549, in save_function_tuple
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 606, in save_list
    self._batch_appends(iter(obj))
  File "/opt/conda/lib/python2.7/pickle.py", line 639, in _batch_appends
    save(x)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 406, in save_function
    self.save_function_tuple(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 549, in save_function_tuple
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 606, in save_list
    self._batch_appends(iter(obj))
  File "/opt/conda/lib/python2.7/pickle.py", line 642, in _batch_appends
    save(tmp[0])
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 406, in save_function
    self.save_function_tuple(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 549, in save_function_tuple
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 606, in save_list
    self._batch_appends(iter(obj))
  File "/opt/conda/lib/python2.7/pickle.py", line 642, in _batch_appends
    save(tmp[0])
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 400, in save_function
    self.save_function_tuple(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 549, in save_function_tuple
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 331, in save
    self.save_reduce(obj=obj, *rv)
  File "/opt/conda/lib/python2.7/pickle.py", line 425, in save_reduce
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 692, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 331, in save
    self.save_reduce(obj=obj, *rv)
  File "/opt/conda/lib/python2.7/pickle.py", line 425, in save_reduce
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 331, in save
    self.save_reduce(obj=obj, *rv)
  File "/opt/conda/lib/python2.7/pickle.py", line 425, in save_reduce
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 331, in save
    self.save_reduce(obj=obj, *rv)
  File "/opt/conda/lib/python2.7/pickle.py", line 425, in save_reduce
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 331, in save
    self.save_reduce(obj=obj, *rv)
  File "/opt/conda/lib/python2.7/pickle.py", line 425, in save_reduce
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 331, in save
    self.save_reduce(obj=obj, *rv)
  File "/opt/conda/lib/python2.7/pickle.py", line 425, in save_reduce
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 331, in save
    self.save_reduce(obj=obj, *rv)
  File "/opt/conda/lib/python2.7/pickle.py", line 425, in save_reduce
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 331, in save
    self.save_reduce(obj=obj, *rv)
  File "/opt/conda/lib/python2.7/pickle.py", line 425, in save_reduce
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 306, in save
    rv = reduce(self.proto)
TypeError: can't pickle thread.lock objects
looping
False
looping
False
[0;31m[0m
[0;31mKeyboardInterrupt[0mTraceback (most recent call last)
[0;32m<ipython-input-238-91553af2816d>[0m in [0;36m<module>[0;34m()[0m
[1;32m      7[0m [0;34m[0m[0m
[1;32m      8[0m [0;32mwhile[0m [0;32mnot[0m [0mterminate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m----> 9[0;31m     [0mtime[0m[0;34m.[0m[0msleep[0m[0;34m([0m[0;36m10[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     10[0m     [0;32mprint[0m [0;34m"looping"[0m[0;34m[0m[0m
[1;32m     11[0m     [0;32mprint[0m [0mquery[0m[0;34m.[0m[0mstatus[0m[0;34m[[0m[0;34m"isDataAvailable"[0m[0;34m][0m[0;34m[0m[0m

[0;31mKeyboardInterrupt[0m: 
 INFO [2020-01-22 21:54:30,097] ({pool-2-thread-89} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:54:30,111] ({pool-2-thread-89} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:54:32,546] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:54:32,557] ({pool-2-thread-7} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194243_52509483 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:54:32,562] ({pool-2-thread-7} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194243_52509483, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:54:32,622] ({pool-2-thread-7} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-194243_52509483 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:54:32,629] ({pool-2-thread-7} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:54:32,640] ({pool-2-thread-7} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194243_52509483 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:54:34,747] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:54:34,758] ({pool-2-thread-90} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:54:34,758] ({pool-2-thread-90} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:54:34,832] ({pool-2-thread-90} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:54:34,848] ({pool-2-thread-90} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:54:34,859] ({pool-2-thread-90} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:55:49,154] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:55:49,171] ({pool-2-thread-46} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:55:49,171] ({pool-2-thread-46} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:55:49,250] ({pool-2-thread-46} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:55:49,269] ({pool-2-thread-46} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:55:49,281] ({pool-2-thread-46} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:57:14,515] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:57:14,529] ({pool-2-thread-91} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:57:14,530] ({pool-2-thread-91} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:57:14,617] ({pool-2-thread-91} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:57:14,631] ({pool-2-thread-91} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:57:14,642] ({pool-2-thread-91} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:57:20,440] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:57:20,451] ({pool-2-thread-24} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:57:20,452] ({pool-2-thread-24} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 21:58:46,310] ({pool-2-thread-24} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ABORT, exception: null, result: %text <pyspark.sql.streaming.StreamingQuery object at 0x7f98484b5b10>
dfis MapPartitionsRDD[767] at javaToPython at <unknown>:0
looping
True
count is 637
looping
False
looping
False
looping
False
looping
False
looping
False
looping
False
looping
False
[0;31m[0m
[0;31mKeyboardInterrupt[0mTraceback (most recent call last)
[0;32m<ipython-input-248-91553af2816d>[0m in [0;36m<module>[0;34m()[0m
[1;32m      7[0m [0;34m[0m[0m
[1;32m      8[0m [0;32mwhile[0m [0;32mnot[0m [0mterminate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m----> 9[0;31m     [0mtime[0m[0;34m.[0m[0msleep[0m[0;34m([0m[0;36m10[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     10[0m     [0;32mprint[0m [0;34m"looping"[0m[0;34m[0m[0m
[1;32m     11[0m     [0;32mprint[0m [0mquery[0m[0;34m.[0m[0mstatus[0m[0;34m[[0m[0;34m"isDataAvailable"[0m[0;34m][0m[0;34m[0m[0m

[0;31mKeyboardInterrupt[0m: 
 INFO [2020-01-22 21:58:46,322] ({pool-2-thread-24} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:58:46,333] ({pool-2-thread-24} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:59:13,118] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:59:13,129] ({pool-2-thread-92} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:59:13,129] ({pool-2-thread-92} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:59:13,208] ({pool-2-thread-92} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:59:13,217] ({pool-2-thread-92} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:59:13,228] ({pool-2-thread-92} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:59:20,343] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:59:20,354] ({pool-2-thread-47} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:59:20,354] ({pool-2-thread-47} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 21:59:42,638] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:59:42,652] ({pool-2-thread-93} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:59:42,652] ({pool-2-thread-93} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 21:59:49,122] ({pool-2-thread-47} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ABORT, exception: null, result: %text <pyspark.sql.streaming.StreamingQuery object at 0x7f98485608d0>
dfis MapPartitionsRDD[790] at javaToPython at <unknown>:0
looping
True
count is 637
looping
False
[0;31m[0m
[0;31mKeyboardInterrupt[0mTraceback (most recent call last)
[0;32m<ipython-input-252-91553af2816d>[0m in [0;36m<module>[0;34m()[0m
[1;32m      7[0m [0;34m[0m[0m
[1;32m      8[0m [0;32mwhile[0m [0;32mnot[0m [0mterminate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m----> 9[0;31m     [0mtime[0m[0;34m.[0m[0msleep[0m[0;34m([0m[0;36m10[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     10[0m     [0;32mprint[0m [0;34m"looping"[0m[0;34m[0m[0m
[1;32m     11[0m     [0;32mprint[0m [0mquery[0m[0;34m.[0m[0mstatus[0m[0;34m[[0m[0;34m"isDataAvailable"[0m[0;34m][0m[0;34m[0m[0m

[0;31mKeyboardInterrupt[0m: 
 INFO [2020-01-22 21:59:49,137] ({pool-2-thread-47} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:59:49,151] ({pool-2-thread-47} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 21:59:49,221] ({pool-2-thread-93} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 21:59:49,235] ({pool-2-thread-93} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 21:59:49,246] ({pool-2-thread-93} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:00:02,078] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:00:03,085] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:00:03,096] ({pool-2-thread-13} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:00:03,097] ({pool-2-thread-13} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 22:00:03,187] ({pool-2-thread-13} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 22:00:03,199] ({pool-2-thread-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:00:03,210] ({pool-2-thread-13} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:00:05,217] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:00:05,229] ({pool-2-thread-94} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:00:05,229] ({pool-2-thread-94} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 WARN [2020-01-22 22:00:15,156] ({pool-2-thread-94} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-194435_1903717418 is finished, status: ABORT, exception: null, result: %text <pyspark.sql.streaming.StreamingQuery object at 0x7f98483b9610>
dfis MapPartitionsRDD[813] at javaToPython at <unknown>:0
[0;31m[0m
[0;31mKeyboardInterrupt[0mTraceback (most recent call last)
[0;32m<ipython-input-258-3d1cf0eaa3fd>[0m in [0;36m<module>[0;34m()[0m
[1;32m      7[0m [0;34m[0m[0m
[1;32m      8[0m [0;32mwhile[0m [0;32mnot[0m [0mterminate[0m[0;34m:[0m[0;34m[0m[0m
[0;32m----> 9[0;31m     [0mtime[0m[0;34m.[0m[0msleep[0m[0;34m([0m[0;36m10[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     10[0m     [0;32mprint[0m [0;34m"looping"[0m[0;34m[0m[0m
[1;32m     11[0m     [0;32mprint[0m [0mquery[0m[0;34m.[0m[0mstatus[0m[0;34m[[0m[0;34m"isDataAvailable"[0m[0;34m][0m[0;34m[0m[0m

[0;31mKeyboardInterrupt[0m: 
 INFO [2020-01-22 22:00:15,175] ({pool-2-thread-94} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:00:15,190] ({pool-2-thread-94} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:00:18,887] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:00:19,543] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:00:19,562] ({pool-2-thread-48} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194243_52509483 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:00:19,562] ({pool-2-thread-48} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194243_52509483, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 22:00:19,628] ({pool-2-thread-48} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-194243_52509483 is finished successfully, status: FINISHED
 INFO [2020-01-22 22:00:19,642] ({pool-2-thread-48} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:00:19,654] ({pool-2-thread-48} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194243_52509483 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:00:21,013] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:00:21,024] ({pool-2-thread-95} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:00:21,029] ({pool-2-thread-95} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 22:00:41,306] ({pool-2-thread-95} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-194435_1903717418 is finished successfully, status: FINISHED
 INFO [2020-01-22 22:00:41,319] ({pool-2-thread-95} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:00:41,331] ({pool-2-thread-95} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:01:31,567] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:01:31,581] ({pool-2-thread-25} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:01:31,586] ({pool-2-thread-25} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 22:01:31,663] ({pool-2-thread-25} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 22:01:31,672] ({pool-2-thread-25} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:01:31,683] ({pool-2-thread-25} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:01:33,470] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:01:33,482] ({pool-2-thread-96} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:01:33,483] ({pool-2-thread-96} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 22:01:53,746] ({pool-2-thread-96} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-194435_1903717418 is finished successfully, status: FINISHED
 INFO [2020-01-22 22:01:53,757] ({pool-2-thread-96} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:01:53,770] ({pool-2-thread-96} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:06:11,348] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:06:16,215] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:07:14,556] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:07:32,278] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:07:33,245] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:07:33,257] ({pool-2-thread-49} SchedulerFactory.java[jobStarted]:114) - Job 20200122-202909_568994244 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:07:33,257] ({pool-2-thread-49} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-202909_568994244, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 22:07:33,312] ({pool-2-thread-49} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-202909_568994244 is finished successfully, status: FINISHED
 INFO [2020-01-22 22:07:33,320] ({pool-2-thread-49} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:07:33,331] ({pool-2-thread-49} SchedulerFactory.java[jobFinished]:120) - Job 20200122-202909_568994244 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:07:38,317] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:07:38,332] ({pool-2-thread-97} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:07:38,334] ({pool-2-thread-97} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 22:07:38,423] ({pool-2-thread-97} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 22:07:38,433] ({pool-2-thread-97} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:07:38,443] ({pool-2-thread-97} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:07:41,064] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:07:41,075] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:07:41,076] ({pool-2-thread-3} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 22:08:01,335] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-194435_1903717418 is finished successfully, status: FINISHED
 INFO [2020-01-22 22:08:01,353] ({pool-2-thread-3} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:08:01,365] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:08:48,605] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:08:48,657] ({pool-2-thread-98} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:08:48,657] ({pool-2-thread-98} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 22:09:08,947] ({pool-2-thread-98} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-194435_1903717418 is finished successfully, status: FINISHED
 INFO [2020-01-22 22:09:08,961] ({pool-2-thread-98} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:09:08,980] ({pool-2-thread-98} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:10:22,084] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:10:22,099] ({pool-2-thread-50} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:10:22,100] ({pool-2-thread-50} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 22:10:22,211] ({pool-2-thread-50} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 22:10:22,229] ({pool-2-thread-50} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:10:22,251] ({pool-2-thread-50} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:10:27,299] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:10:27,311] ({pool-2-thread-99} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:10:27,311] ({pool-2-thread-99} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 22:10:57,628] ({pool-2-thread-99} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-194435_1903717418 is finished successfully, status: FINISHED
 INFO [2020-01-22 22:10:57,645] ({pool-2-thread-99} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:10:57,661] ({pool-2-thread-99} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:11:20,819] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:11:20,832] ({pool-2-thread-26} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:11:20,837] ({pool-2-thread-26} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 22:11:20,948] ({pool-2-thread-26} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 22:11:20,958] ({pool-2-thread-26} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:11:20,969] ({pool-2-thread-26} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:11:31,474] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:11:31,533] ({pool-2-thread-100} SchedulerFactory.java[jobStarted]:114) - Job 20200122-051101_1395511314 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:11:31,533] ({pool-2-thread-100} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-051101_1395511314, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 22:11:31,627] ({pool-2-thread-100} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-051101_1395511314 is finished successfully, status: FINISHED
 INFO [2020-01-22 22:11:31,639] ({pool-2-thread-100} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:11:31,649] ({pool-2-thread-100} SchedulerFactory.java[jobFinished]:120) - Job 20200122-051101_1395511314 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:11:34,338] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:11:34,349] ({pool-2-thread-14} SchedulerFactory.java[jobStarted]:114) - Job 20200122-194435_1903717418 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-22 22:11:34,350] ({pool-2-thread-14} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-194435_1903717418, interpreter: spark.pyspark, note_id: 2EXQEGYMG, user: anonymous]
 INFO [2020-01-22 22:11:54,554] ({pool-2-thread-14} NotebookServer.java[afterStatusChange]:2314) - Job 20200122-194435_1903717418 is finished successfully, status: FINISHED
 INFO [2020-01-22 22:11:54,567] ({pool-2-thread-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXQEGYMG
 INFO [2020-01-22 22:11:54,580] ({pool-2-thread-14} SchedulerFactory.java[jobFinished]:120) - Job 20200122-194435_1903717418 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 WARN [2020-01-22 23:01:12,523] ({qtp2107447833-36} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:01:12,523] ({qtp2107447833-36} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:01:12,524] ({qtp2107447833-36} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:01:12,524] ({qtp2107447833-36} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:01:12,524] ({qtp2107447833-36} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:01:12,524] ({qtp2107447833-36} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 INFO [2020-01-22 23:05:39,873] ({qtp2107447833-14} LocalConfigStorage.java[save]:53) - Save Interpreter Setting to /zeppelin/conf/interpreter.json
 INFO [2020-01-22 23:05:39,874] ({qtp2107447833-14} InterpreterRestApi.java[newSettings]:124) - new setting created with redshift
 WARN [2020-01-22 23:05:39,909] ({qtp2107447833-12} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:05:39,910] ({qtp2107447833-12} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:05:39,910] ({qtp2107447833-12} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:05:39,911] ({qtp2107447833-12} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:05:39,911] ({qtp2107447833-12} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:05:39,912] ({qtp2107447833-12} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
ERROR [2020-01-22 23:05:40,276] ({Thread-791} InterpreterSetting.java[run]:840) - Error while downloading repos for interpreter group : jdbc, go to interpreter setting page click on edit and save it again to make this interpreter work properly. : Cannot fetch dependencies for com.amazonaws:aws-java-sdk-redshift:1.11.51
org.sonatype.aether.RepositoryException: Cannot fetch dependencies for com.amazonaws:aws-java-sdk-redshift:1.11.51
	at org.apache.zeppelin.dep.DependencyResolver.getArtifactsWithDep(DependencyResolver.java:179)
	at org.apache.zeppelin.dep.DependencyResolver.loadFromMvn(DependencyResolver.java:128)
	at org.apache.zeppelin.dep.DependencyResolver.load(DependencyResolver.java:76)
	at org.apache.zeppelin.dep.DependencyResolver.load(DependencyResolver.java:93)
	at org.apache.zeppelin.dep.DependencyResolver.load(DependencyResolver.java:85)
	at org.apache.zeppelin.interpreter.InterpreterSetting$1.run(InterpreterSetting.java:832)
Caused by: java.lang.NullPointerException
	at org.sonatype.aether.impl.internal.DefaultRepositorySystem.resolveDependencies(DefaultRepositorySystem.java:352)
	at org.apache.zeppelin.dep.DependencyResolver.getArtifactsWithDep(DependencyResolver.java:176)
	... 5 more
 INFO [2020-01-22 23:05:58,774] ({qtp2107447833-12} InterpreterRestApi.java[restartSetting]:180) - Restart interpreterSetting redshift, msg=
 INFO [2020-01-22 23:05:58,774] ({qtp2107447833-12} InterpreterSetting.java[close]:483) - Close InterpreterSetting: redshift
 INFO [2020-01-22 23:06:00,979] ({qtp2107447833-12} NotebookServer.java[onClose]:372) - Closed connection to 192.168.99.1 : 16263. (1001) null
 WARN [2020-01-22 23:06:01,234] ({qtp2107447833-9} SecurityRestApi.java[ticket]:88) - {"status":"OK","message":"","body":{"principal":"anonymous","ticket":"anonymous","roles":"[]"}}
 INFO [2020-01-22 23:06:01,270] ({qtp2107447833-9} NotebookServer.java[onOpen]:151) - New connection from 192.168.99.1 : 21048
 WARN [2020-01-22 23:06:01,387] ({qtp2107447833-9} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:06:01,387] ({qtp2107447833-9} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:06:01,388] ({qtp2107447833-9} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:06:01,389] ({qtp2107447833-9} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:06:01,390] ({qtp2107447833-9} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:06:01,390] ({qtp2107447833-9} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:06:11,509] ({qtp2107447833-14} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:06:11,509] ({qtp2107447833-14} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:06:11,509] ({qtp2107447833-14} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:06:11,509] ({qtp2107447833-14} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:06:11,509] ({qtp2107447833-14} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:06:11,510] ({qtp2107447833-14} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:07:11,799] ({qtp2107447833-36} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:07:11,799] ({qtp2107447833-36} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:07:11,799] ({qtp2107447833-36} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:07:11,799] ({qtp2107447833-36} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:07:11,799] ({qtp2107447833-36} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:07:11,800] ({qtp2107447833-36} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 INFO [2020-01-22 23:07:11,800] ({qtp2107447833-36} LocalConfigStorage.java[save]:53) - Save Interpreter Setting to /zeppelin/conf/interpreter.json
 INFO [2020-01-22 23:07:11,803] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXSBN97B
 INFO [2020-01-22 23:07:11,813] ({qtp2107447833-36} FolderView.java[onNoteNameChanged]:205) - Note name changed: 2EXSBN97B -> RedshiftLoadFiles
 INFO [2020-01-22 23:07:11,814] ({qtp2107447833-36} Folder.java[addNote]:185) - Add note 2EXSBN97B to folder /
 INFO [2020-01-22 23:07:11,815] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXSBN97B
 INFO [2020-01-22 23:07:11,934] ({qtp2107447833-16} NotebookServer.java[sendNote]:828) - New operation from 192.168.99.1 : 21048 : anonymous : GET_NOTE : 2EXSBN97B
 WARN [2020-01-22 23:07:11,948] ({qtp2107447833-16} GitNotebookRepo.java[revisionHistory]:158) - No Head found for 2EXSBN97B, No HEAD exists and no explicit starting revision was specified
 WARN [2020-01-22 23:07:11,968] ({qtp2107447833-14} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:07:11,968] ({qtp2107447833-14} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:07:11,968] ({qtp2107447833-14} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:07:11,968] ({qtp2107447833-14} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:07:11,968] ({qtp2107447833-14} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:07:11,969] ({qtp2107447833-14} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 INFO [2020-01-22 23:07:12,091] ({qtp2107447833-12} InterpreterSetting.java[getOrCreateInterpreterGroup]:419) - Create InterpreterGroup with groupId: redshift:shared_process for user: anonymous and note: 2EXSBN97B
 INFO [2020-01-22 23:07:12,091] ({qtp2107447833-12} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.jdbc.JDBCInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-22 23:07:12,092] ({qtp2107447833-12} ManagedInterpreterGroup.java[getOrCreateSession]:158) - Create Session: shared_session in InterpreterGroup: redshift:shared_process for user: anonymous
 WARN [2020-01-22 23:07:18,343] ({qtp2107447833-12} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: dep
 INFO [2020-01-22 23:07:22,118] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EXSBN97B
 INFO [2020-01-22 23:11:53,808] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EXSBN97B
 INFO [2020-01-22 23:12:09,297] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXSBN97B
 INFO [2020-01-22 23:12:33,138] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EXSBN97B
 INFO [2020-01-22 23:12:42,038] ({qtp2107447833-9} NotebookServer.java[broadcastNewParagraph]:688) - Broadcasting paragraph on run call instead of note.
 INFO [2020-01-22 23:12:42,040] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXSBN97B
ERROR [2020-01-22 23:12:42,055] ({qtp2107447833-9} NotebookServer.java[afterStatusChange]:2308) - Error
org.apache.zeppelin.interpreter.InterpreterNotFoundException: Either no interpreter named dep or it is not binded to this note
	at org.apache.zeppelin.interpreter.InterpreterFactory.getInterpreter(InterpreterFactory.java:101)
	at org.apache.zeppelin.notebook.Paragraph.getBindedInterpreter(Paragraph.java:243)
	at org.apache.zeppelin.notebook.Paragraph.execute(Paragraph.java:351)
	at org.apache.zeppelin.notebook.Note.run(Note.java:683)
	at org.apache.zeppelin.socket.NotebookServer.persistAndExecuteSingleParagraph(NotebookServer.java:1881)
	at org.apache.zeppelin.socket.NotebookServer.runParagraph(NotebookServer.java:1840)
	at org.apache.zeppelin.socket.NotebookServer.onMessage(NotebookServer.java:262)
	at org.apache.zeppelin.socket.NotebookSocket.onWebSocketText(NotebookSocket.java:59)
	at org.eclipse.jetty.websocket.common.events.JettyListenerEventDriver.onTextMessage(JettyListenerEventDriver.java:189)
	at org.eclipse.jetty.websocket.common.message.SimpleTextMessage.messageComplete(SimpleTextMessage.java:69)
	at org.eclipse.jetty.websocket.common.events.AbstractEventDriver.appendMessage(AbstractEventDriver.java:66)
	at org.eclipse.jetty.websocket.common.events.JettyListenerEventDriver.onTextFrame(JettyListenerEventDriver.java:158)
	at org.eclipse.jetty.websocket.common.events.AbstractEventDriver.incomingFrame(AbstractEventDriver.java:162)
	at org.eclipse.jetty.websocket.common.WebSocketSession.incomingFrame(WebSocketSession.java:459)
	at org.eclipse.jetty.websocket.common.extensions.AbstractExtension.nextIncomingFrame(AbstractExtension.java:182)
	at org.eclipse.jetty.websocket.common.extensions.compress.PerMessageDeflateExtension.nextIncomingFrame(PerMessageDeflateExtension.java:105)
	at org.eclipse.jetty.websocket.common.extensions.compress.CompressExtension.forwardIncoming(CompressExtension.java:142)
	at org.eclipse.jetty.websocket.common.extensions.compress.PerMessageDeflateExtension.incomingFrame(PerMessageDeflateExtension.java:85)
	at org.eclipse.jetty.websocket.common.extensions.ExtensionStack.incomingFrame(ExtensionStack.java:220)
	at org.eclipse.jetty.websocket.common.Parser.notifyFrame(Parser.java:219)
	at org.eclipse.jetty.websocket.common.Parser.parse(Parser.java:244)
	at org.eclipse.jetty.websocket.common.io.AbstractWebSocketConnection.readParse(AbstractWebSocketConnection.java:559)
	at org.eclipse.jetty.websocket.common.io.AbstractWebSocketConnection.onFillable(AbstractWebSocketConnection.java:390)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)
	at java.lang.Thread.run(Thread.java:748)
 WARN [2020-01-22 23:12:42,055] ({qtp2107447833-9} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-230711_1942802516 is finished, status: ERROR, exception: org.apache.zeppelin.interpreter.InterpreterNotFoundException: Either no interpreter named dep or it is not binded to this note, result: 
 INFO [2020-01-22 23:12:42,057] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXSBN97B
ERROR [2020-01-22 23:12:42,070] ({qtp2107447833-9} NotebookServer.java[persistAndExecuteSingleParagraph]:1883) - Exception from run
java.lang.RuntimeException: org.apache.zeppelin.interpreter.InterpreterNotFoundException: Either no interpreter named dep or it is not binded to this note
	at org.apache.zeppelin.notebook.Paragraph.execute(Paragraph.java:375)
	at org.apache.zeppelin.notebook.Note.run(Note.java:683)
	at org.apache.zeppelin.socket.NotebookServer.persistAndExecuteSingleParagraph(NotebookServer.java:1881)
	at org.apache.zeppelin.socket.NotebookServer.runParagraph(NotebookServer.java:1840)
	at org.apache.zeppelin.socket.NotebookServer.onMessage(NotebookServer.java:262)
	at org.apache.zeppelin.socket.NotebookSocket.onWebSocketText(NotebookSocket.java:59)
	at org.eclipse.jetty.websocket.common.events.JettyListenerEventDriver.onTextMessage(JettyListenerEventDriver.java:189)
	at org.eclipse.jetty.websocket.common.message.SimpleTextMessage.messageComplete(SimpleTextMessage.java:69)
	at org.eclipse.jetty.websocket.common.events.AbstractEventDriver.appendMessage(AbstractEventDriver.java:66)
	at org.eclipse.jetty.websocket.common.events.JettyListenerEventDriver.onTextFrame(JettyListenerEventDriver.java:158)
	at org.eclipse.jetty.websocket.common.events.AbstractEventDriver.incomingFrame(AbstractEventDriver.java:162)
	at org.eclipse.jetty.websocket.common.WebSocketSession.incomingFrame(WebSocketSession.java:459)
	at org.eclipse.jetty.websocket.common.extensions.AbstractExtension.nextIncomingFrame(AbstractExtension.java:182)
	at org.eclipse.jetty.websocket.common.extensions.compress.PerMessageDeflateExtension.nextIncomingFrame(PerMessageDeflateExtension.java:105)
	at org.eclipse.jetty.websocket.common.extensions.compress.CompressExtension.forwardIncoming(CompressExtension.java:142)
	at org.eclipse.jetty.websocket.common.extensions.compress.PerMessageDeflateExtension.incomingFrame(PerMessageDeflateExtension.java:85)
	at org.eclipse.jetty.websocket.common.extensions.ExtensionStack.incomingFrame(ExtensionStack.java:220)
	at org.eclipse.jetty.websocket.common.Parser.notifyFrame(Parser.java:219)
	at org.eclipse.jetty.websocket.common.Parser.parse(Parser.java:244)
	at org.eclipse.jetty.websocket.common.io.AbstractWebSocketConnection.readParse(AbstractWebSocketConnection.java:559)
	at org.eclipse.jetty.websocket.common.io.AbstractWebSocketConnection.onFillable(AbstractWebSocketConnection.java:390)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.zeppelin.interpreter.InterpreterNotFoundException: Either no interpreter named dep or it is not binded to this note
	at org.apache.zeppelin.interpreter.InterpreterFactory.getInterpreter(InterpreterFactory.java:101)
	at org.apache.zeppelin.notebook.Paragraph.getBindedInterpreter(Paragraph.java:243)
	at org.apache.zeppelin.notebook.Paragraph.execute(Paragraph.java:351)
	... 31 more
 WARN [2020-01-22 23:12:56,112] ({qtp2107447833-14} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:12:56,112] ({qtp2107447833-14} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:12:56,112] ({qtp2107447833-14} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:12:56,113] ({qtp2107447833-14} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:12:56,113] ({qtp2107447833-14} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:12:56,113] ({qtp2107447833-14} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 INFO [2020-01-22 23:13:09,749] ({qtp2107447833-9} LocalConfigStorage.java[save]:53) - Save Interpreter Setting to /zeppelin/conf/interpreter.json
 WARN [2020-01-22 23:13:09,750] ({qtp2107447833-9} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:13:09,750] ({qtp2107447833-9} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:13:09,751] ({qtp2107447833-9} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:13:09,751] ({qtp2107447833-9} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:13:09,751] ({qtp2107447833-9} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:13:09,751] ({qtp2107447833-9} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:13:15,030] ({qtp2107447833-9} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: rdep
 WARN [2020-01-22 23:13:15,228] ({qtp2107447833-9} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: redep
 WARN [2020-01-22 23:13:15,470] ({qtp2107447833-16} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: reddep
 WARN [2020-01-22 23:13:16,398] ({qtp2107447833-9} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: redsdep
 WARN [2020-01-22 23:13:17,062] ({qtp2107447833-16} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: redshdep
 WARN [2020-01-22 23:13:17,116] ({qtp2107447833-9} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: redshidep
 WARN [2020-01-22 23:13:17,279] ({qtp2107447833-9} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: redshifdep
 WARN [2020-01-22 23:13:17,422] ({qtp2107447833-9} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: redshiftdep
 WARN [2020-01-22 23:13:18,295] ({qtp2107447833-16} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: redshift.dep
 INFO [2020-01-22 23:13:18,835] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EXSBN97B
ERROR [2020-01-22 23:13:18,894] ({qtp2107447833-16} NotebookServer.java[persistAndExecuteSingleParagraph]:1883) - Exception from run
java.lang.RuntimeException: org.apache.zeppelin.interpreter.InterpreterNotFoundException: No such interpreter: redshift.dep
	at org.apache.zeppelin.notebook.Paragraph.execute(Paragraph.java:375)
	at org.apache.zeppelin.notebook.Note.run(Note.java:683)
	at org.apache.zeppelin.socket.NotebookServer.persistAndExecuteSingleParagraph(NotebookServer.java:1881)
	at org.apache.zeppelin.socket.NotebookServer.runParagraph(NotebookServer.java:1840)
	at org.apache.zeppelin.socket.NotebookServer.onMessage(NotebookServer.java:262)
	at org.apache.zeppelin.socket.NotebookSocket.onWebSocketText(NotebookSocket.java:59)
	at org.eclipse.jetty.websocket.common.events.JettyListenerEventDriver.onTextMessage(JettyListenerEventDriver.java:189)
	at org.eclipse.jetty.websocket.common.message.SimpleTextMessage.messageComplete(SimpleTextMessage.java:69)
	at org.eclipse.jetty.websocket.common.events.AbstractEventDriver.appendMessage(AbstractEventDriver.java:66)
	at org.eclipse.jetty.websocket.common.events.JettyListenerEventDriver.onTextFrame(JettyListenerEventDriver.java:158)
	at org.eclipse.jetty.websocket.common.events.AbstractEventDriver.incomingFrame(AbstractEventDriver.java:162)
	at org.eclipse.jetty.websocket.common.WebSocketSession.incomingFrame(WebSocketSession.java:459)
	at org.eclipse.jetty.websocket.common.extensions.AbstractExtension.nextIncomingFrame(AbstractExtension.java:182)
	at org.eclipse.jetty.websocket.common.extensions.compress.PerMessageDeflateExtension.nextIncomingFrame(PerMessageDeflateExtension.java:105)
	at org.eclipse.jetty.websocket.common.extensions.compress.CompressExtension.forwardIncoming(CompressExtension.java:142)
	at org.eclipse.jetty.websocket.common.extensions.compress.PerMessageDeflateExtension.incomingFrame(PerMessageDeflateExtension.java:85)
	at org.eclipse.jetty.websocket.common.extensions.ExtensionStack.incomingFrame(ExtensionStack.java:220)
	at org.eclipse.jetty.websocket.common.Parser.notifyFrame(Parser.java:219)
	at org.eclipse.jetty.websocket.common.Parser.parse(Parser.java:244)
	at org.eclipse.jetty.websocket.common.io.AbstractWebSocketConnection.readParse(AbstractWebSocketConnection.java:559)
	at org.eclipse.jetty.websocket.common.io.AbstractWebSocketConnection.onFillable(AbstractWebSocketConnection.java:390)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.zeppelin.interpreter.InterpreterNotFoundException: No such interpreter: redshift.dep
	at org.apache.zeppelin.interpreter.InterpreterFactory.getInterpreter(InterpreterFactory.java:79)
	at org.apache.zeppelin.notebook.Paragraph.getBindedInterpreter(Paragraph.java:243)
	at org.apache.zeppelin.notebook.Paragraph.execute(Paragraph.java:351)
	... 31 more
 WARN [2020-01-22 23:13:21,627] ({qtp2107447833-36} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: redsdep
 WARN [2020-01-22 23:13:21,946] ({qtp2107447833-9} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: dep
 INFO [2020-01-22 23:13:23,071] ({qtp2107447833-36} VFSNotebookRepo.java[save]:196) - Saving note:2EXSBN97B
ERROR [2020-01-22 23:13:23,081] ({qtp2107447833-36} NotebookServer.java[persistAndExecuteSingleParagraph]:1883) - Exception from run
java.lang.RuntimeException: org.apache.zeppelin.interpreter.InterpreterNotFoundException: Either no interpreter named dep or it is not binded to this note
	at org.apache.zeppelin.notebook.Paragraph.execute(Paragraph.java:375)
	at org.apache.zeppelin.notebook.Note.run(Note.java:683)
	at org.apache.zeppelin.socket.NotebookServer.persistAndExecuteSingleParagraph(NotebookServer.java:1881)
	at org.apache.zeppelin.socket.NotebookServer.runParagraph(NotebookServer.java:1840)
	at org.apache.zeppelin.socket.NotebookServer.onMessage(NotebookServer.java:262)
	at org.apache.zeppelin.socket.NotebookSocket.onWebSocketText(NotebookSocket.java:59)
	at org.eclipse.jetty.websocket.common.events.JettyListenerEventDriver.onTextMessage(JettyListenerEventDriver.java:189)
	at org.eclipse.jetty.websocket.common.message.SimpleTextMessage.messageComplete(SimpleTextMessage.java:69)
	at org.eclipse.jetty.websocket.common.events.AbstractEventDriver.appendMessage(AbstractEventDriver.java:66)
	at org.eclipse.jetty.websocket.common.events.JettyListenerEventDriver.onTextFrame(JettyListenerEventDriver.java:158)
	at org.eclipse.jetty.websocket.common.events.AbstractEventDriver.incomingFrame(AbstractEventDriver.java:162)
	at org.eclipse.jetty.websocket.common.WebSocketSession.incomingFrame(WebSocketSession.java:459)
	at org.eclipse.jetty.websocket.common.extensions.AbstractExtension.nextIncomingFrame(AbstractExtension.java:182)
	at org.eclipse.jetty.websocket.common.extensions.compress.PerMessageDeflateExtension.nextIncomingFrame(PerMessageDeflateExtension.java:105)
	at org.eclipse.jetty.websocket.common.extensions.compress.CompressExtension.forwardIncoming(CompressExtension.java:142)
	at org.eclipse.jetty.websocket.common.extensions.compress.PerMessageDeflateExtension.incomingFrame(PerMessageDeflateExtension.java:85)
	at org.eclipse.jetty.websocket.common.extensions.ExtensionStack.incomingFrame(ExtensionStack.java:220)
	at org.eclipse.jetty.websocket.common.Parser.notifyFrame(Parser.java:219)
	at org.eclipse.jetty.websocket.common.Parser.parse(Parser.java:244)
	at org.eclipse.jetty.websocket.common.io.AbstractWebSocketConnection.readParse(AbstractWebSocketConnection.java:559)
	at org.eclipse.jetty.websocket.common.io.AbstractWebSocketConnection.onFillable(AbstractWebSocketConnection.java:390)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.zeppelin.interpreter.InterpreterNotFoundException: Either no interpreter named dep or it is not binded to this note
	at org.apache.zeppelin.interpreter.InterpreterFactory.getInterpreter(InterpreterFactory.java:101)
	at org.apache.zeppelin.notebook.Paragraph.getBindedInterpreter(Paragraph.java:243)
	at org.apache.zeppelin.notebook.Paragraph.execute(Paragraph.java:351)
	... 31 more
 WARN [2020-01-22 23:13:28,174] ({qtp2107447833-9} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: r
 WARN [2020-01-22 23:13:28,370] ({qtp2107447833-36} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: re
 WARN [2020-01-22 23:13:28,586] ({qtp2107447833-9} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: red
 WARN [2020-01-22 23:13:29,788] ({qtp2107447833-36} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: reds
 WARN [2020-01-22 23:13:29,914] ({qtp2107447833-9} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: redsh
 WARN [2020-01-22 23:13:29,974] ({qtp2107447833-36} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: redshi
 WARN [2020-01-22 23:13:30,162] ({qtp2107447833-9} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: redshif
 INFO [2020-01-22 23:13:30,642] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EXSBN97B
 INFO [2020-01-22 23:13:30,652] ({pool-2-thread-27} SchedulerFactory.java[jobStarted]:114) - Job 20200122-230711_1942802516 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-redshift:shared_process-shared_session
 INFO [2020-01-22 23:13:30,653] ({pool-2-thread-27} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20200122-230711_1942802516, interpreter: redshift, note_id: 2EXSBN97B, user: anonymous]
 INFO [2020-01-22 23:13:30,653] ({pool-2-thread-27} ManagedInterpreterGroup.java[getOrCreateInterpreterProcess]:61) - Create InterpreterProcess for InterpreterGroup: redshift:shared_process
 INFO [2020-01-22 23:13:30,653] ({pool-2-thread-27} ShellScriptLauncher.java[launch]:48) - Launching Interpreter: jdbc
 INFO [2020-01-22 23:13:30,654] ({pool-2-thread-27} RemoteInterpreterManagedProcess.java[start]:115) - Thrift server for callback will start. Port: 38621
 INFO [2020-01-22 23:13:31,156] ({pool-2-thread-27} RemoteInterpreterManagedProcess.java[start]:190) - Run interpreter process [/zeppelin/bin/interpreter.sh, -d, /zeppelin/interpreter/jdbc, -c, 172.19.0.3, -p, 38621, -r, :, -l, /usr/local/local-repo/redshift, -g, redshift]
 INFO [2020-01-22 23:13:32,862] ({pool-12-thread-1} RemoteInterpreterManagedProcess.java[callback]:123) - RemoteInterpreterServer Registered: CallbackInfo(host:172.19.0.3, port:41071)
 INFO [2020-01-22 23:13:32,864] ({pool-2-thread-27} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.jdbc.JDBCInterpreter
 INFO [2020-01-22 23:13:33,030] ({pool-2-thread-27} RemoteInterpreter.java[call]:142) - Open RemoteInterpreter org.apache.zeppelin.jdbc.JDBCInterpreter
 INFO [2020-01-22 23:13:33,031] ({pool-2-thread-27} RemoteInterpreter.java[pushAngularObjectRegistryToRemote]:436) - Push local angular object registry from ZeppelinServer to remote interpreter group redshift:shared_process
 WARN [2020-01-22 23:13:33,433] ({pool-2-thread-27} NotebookServer.java[afterStatusChange]:2316) - Job 20200122-230711_1942802516 is finished, status: ERROR, exception: null, result: %text java.lang.ClassNotFoundException: com.amazon.redshift.jdbc42.Driver
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:264)
	at org.apache.zeppelin.jdbc.JDBCInterpreter.createConnectionPool(JDBCInterpreter.java:412)
	at org.apache.zeppelin.jdbc.JDBCInterpreter.getConnectionFromPool(JDBCInterpreter.java:423)
	at org.apache.zeppelin.jdbc.JDBCInterpreter.getConnection(JDBCInterpreter.java:443)
	at org.apache.zeppelin.jdbc.JDBCInterpreter.executeSql(JDBCInterpreter.java:692)
	at org.apache.zeppelin.jdbc.JDBCInterpreter.interpret(JDBCInterpreter.java:820)
	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:103)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:632)
	at org.apache.zeppelin.scheduler.Job.run(Job.java:188)
	at org.apache.zeppelin.scheduler.ParallelScheduler$JobRunner.run(ParallelScheduler.java:162)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

 INFO [2020-01-22 23:13:33,436] ({pool-2-thread-27} VFSNotebookRepo.java[save]:196) - Saving note:2EXSBN97B
 INFO [2020-01-22 23:13:33,450] ({pool-2-thread-27} SchedulerFactory.java[jobFinished]:120) - Job 20200122-230711_1942802516 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-redshift:shared_process-shared_session
 INFO [2020-01-22 23:38:53,621] ({Thread-13} ZeppelinServer.java[run]:253) - Shutting down Zeppelin Server ... 
 INFO [2020-01-22 23:38:53,636] ({qtp2107447833-9} NotebookServer.java[onClose]:372) - Closed connection to 192.168.99.1 : 21048. (1000) null
 INFO [2020-01-22 23:38:53,637] ({qtp2107447833-11} NotebookServer.java[onClose]:372) - Closed connection to 192.168.99.1 : 16261. (1000) null
 INFO [2020-01-22 23:38:53,638] ({Thread-13} AbstractConnector.java[doStop]:341) - Stopped ServerConnector@49faf066{HTTP/1.1,[http/1.1]}{0.0.0.0:8080}
 INFO [2020-01-22 23:38:53,639] ({Thread-13} HouseKeeper.java[stopScavenging]:167) - node0 Stopped scavenging
 INFO [2020-01-22 23:38:55,336] ({Thread-13} ContextHandler.java[doStop]:1045) - Stopped o.e.j.w.WebAppContext@5af97850{zeppelin-web,/,null,UNAVAILABLE}{/zeppelin/zeppelin-web-0.8.2.war}
 INFO [2020-01-22 23:38:55,338] ({Thread-796} InterpreterSetting.java[close]:483) - Close InterpreterSetting: ignite
 INFO [2020-01-22 23:38:55,338] ({Thread-798} InterpreterSetting.java[close]:483) - Close InterpreterSetting: python
 INFO [2020-01-22 23:38:55,339] ({Thread-799} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sap
 INFO [2020-01-22 23:38:55,339] ({Thread-800} InterpreterSetting.java[close]:483) - Close InterpreterSetting: jdbc
 INFO [2020-01-22 23:38:55,339] ({Thread-801} InterpreterSetting.java[close]:483) - Close InterpreterSetting: lens
 INFO [2020-01-22 23:38:55,339] ({Thread-802} InterpreterSetting.java[close]:483) - Close InterpreterSetting: pig
 INFO [2020-01-22 23:38:55,339] ({Thread-803} InterpreterSetting.java[close]:483) - Close InterpreterSetting: flink
 INFO [2020-01-22 23:38:55,340] ({Thread-804} InterpreterSetting.java[close]:483) - Close InterpreterSetting: angular
 INFO [2020-01-22 23:38:55,340] ({Thread-805} InterpreterSetting.java[close]:483) - Close InterpreterSetting: livy
 INFO [2020-01-22 23:38:55,340] ({Thread-806} InterpreterSetting.java[close]:483) - Close InterpreterSetting: neo4j
 INFO [2020-01-22 23:38:55,340] ({Thread-807} InterpreterSetting.java[close]:483) - Close InterpreterSetting: file
 INFO [2020-01-22 23:38:55,340] ({Thread-808} InterpreterSetting.java[close]:483) - Close InterpreterSetting: groovy
 INFO [2020-01-22 23:38:55,341] ({Thread-809} InterpreterSetting.java[close]:483) - Close InterpreterSetting: elasticsearch
 INFO [2020-01-22 23:38:55,341] ({Thread-810} InterpreterSetting.java[close]:483) - Close InterpreterSetting: redshift
 INFO [2020-01-22 23:38:55,341] ({Thread-810} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: redshift:shared_process
 INFO [2020-01-22 23:38:55,341] ({Thread-811} InterpreterSetting.java[close]:483) - Close InterpreterSetting: cassandra
 INFO [2020-01-22 23:38:55,341] ({Thread-810} ManagedInterpreterGroup.java[close]:100) - Close Session: shared_session for interpreter setting: redshift
 INFO [2020-01-22 23:38:55,341] ({Thread-812} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sh
 INFO [2020-01-22 23:38:55,341] ({Thread-813} InterpreterSetting.java[close]:483) - Close InterpreterSetting: spark
 INFO [2020-01-22 23:38:55,342] ({Thread-813} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: spark:shared_process
 INFO [2020-01-22 23:38:55,342] ({Thread-813} ManagedInterpreterGroup.java[close]:100) - Close Session: shared_session for interpreter setting: spark
 WARN [2020-01-22 23:38:55,342] ({Thread-813} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.SparkInterpreter
 WARN [2020-01-22 23:38:55,343] ({Thread-813} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.SparkSqlInterpreter
 INFO [2020-01-22 23:38:55,342] ({Thread-814} InterpreterSetting.java[close]:483) - Close InterpreterSetting: md
 INFO [2020-01-22 23:38:55,343] ({Thread-816} InterpreterSetting.java[close]:483) - Close InterpreterSetting: bigquery
 INFO [2020-01-22 23:38:55,343] ({Thread-815} InterpreterSetting.java[close]:483) - Close InterpreterSetting: alluxio
 INFO [2020-01-22 23:38:55,343] ({Thread-817} InterpreterSetting.java[close]:483) - Close InterpreterSetting: hbase
 INFO [2020-01-22 23:38:55,348] ({Thread-818} InterpreterSetting.java[close]:483) - Close InterpreterSetting: kylin
 INFO [2020-01-22 23:38:55,349] ({Thread-819} InterpreterSetting.java[close]:483) - Close InterpreterSetting: python
 INFO [2020-01-22 23:38:55,350] ({Thread-820} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sap
 INFO [2020-01-22 23:38:55,350] ({Thread-825} InterpreterSetting.java[close]:483) - Close InterpreterSetting: angular
 INFO [2020-01-22 23:38:55,350] ({Thread-824} InterpreterSetting.java[close]:483) - Close InterpreterSetting: flink
 INFO [2020-01-22 23:38:55,350] ({Thread-823} InterpreterSetting.java[close]:483) - Close InterpreterSetting: pig
 INFO [2020-01-22 23:38:55,350] ({Thread-822} InterpreterSetting.java[close]:483) - Close InterpreterSetting: lens
 INFO [2020-01-22 23:38:55,350] ({Thread-821} InterpreterSetting.java[close]:483) - Close InterpreterSetting: jdbc
 INFO [2020-01-22 23:38:55,351] ({Thread-828} InterpreterSetting.java[close]:483) - Close InterpreterSetting: file
 INFO [2020-01-22 23:38:55,350] ({Thread-827} InterpreterSetting.java[close]:483) - Close InterpreterSetting: neo4j
 INFO [2020-01-22 23:38:55,350] ({Thread-826} InterpreterSetting.java[close]:483) - Close InterpreterSetting: livy
 INFO [2020-01-22 23:38:55,353] ({Thread-797} InterpreterSetting.java[close]:483) - Close InterpreterSetting: ignite
 INFO [2020-01-22 23:38:55,353] ({Thread-829} InterpreterSetting.java[close]:483) - Close InterpreterSetting: groovy
 INFO [2020-01-22 23:38:55,355] ({Thread-830} InterpreterSetting.java[close]:483) - Close InterpreterSetting: elasticsearch
 INFO [2020-01-22 23:38:55,356] ({Thread-831} InterpreterSetting.java[close]:483) - Close InterpreterSetting: redshift
 INFO [2020-01-22 23:38:55,357] ({Thread-810} ManagedInterpreterGroup.java[close]:105) - Remove this InterpreterGroup: redshift:shared_process as all the sessions are closed
 INFO [2020-01-22 23:38:55,357] ({Thread-810} ManagedInterpreterGroup.java[close]:108) - Kill RemoteInterpreterProcess
 INFO [2020-01-22 23:38:55,359] ({Thread-832} InterpreterSetting.java[close]:483) - Close InterpreterSetting: cassandra
 INFO [2020-01-22 23:38:55,359] ({Thread-834} InterpreterSetting.java[close]:483) - Close InterpreterSetting: spark
 INFO [2020-01-22 23:38:55,359] ({Thread-833} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sh
 INFO [2020-01-22 23:38:55,364] ({Thread-835} InterpreterSetting.java[close]:483) - Close InterpreterSetting: md
 INFO [2020-01-22 23:38:55,364] ({Thread-837} InterpreterSetting.java[close]:483) - Close InterpreterSetting: bigquery
 INFO [2020-01-22 23:38:55,366] ({Thread-836} InterpreterSetting.java[close]:483) - Close InterpreterSetting: alluxio
 INFO [2020-01-22 23:38:55,376] ({Thread-810} RemoteInterpreterManagedProcess.java[stop]:220) - Kill interpreter process
 INFO [2020-01-22 23:38:55,376] ({Thread-838} InterpreterSetting.java[close]:483) - Close InterpreterSetting: hbase
 INFO [2020-01-22 23:38:55,378] ({Thread-839} InterpreterSetting.java[close]:483) - Close InterpreterSetting: kylin
ERROR [2020-01-22 23:38:55,433] ({Thread-793} RemoteInterpreterEventPoller.java[run]:257) - Can not get RemoteInterpreterEvent because it is shutdown.
ERROR [2020-01-22 23:38:55,438] ({pool-11-thread-1} AppendOutputRunner.java[run]:68) - Wait for OutputBuffer queue interrupted: null
 WARN [2020-01-22 23:38:55,518] ({Thread-813} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.IPySparkInterpreter
 WARN [2020-01-22 23:38:55,519] ({Thread-813} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.SparkRInterpreter
 INFO [2020-01-22 23:38:55,520] ({Thread-813} ManagedInterpreterGroup.java[close]:105) - Remove this InterpreterGroup: spark:shared_process as all the sessions are closed
 INFO [2020-01-22 23:38:55,521] ({Thread-813} ManagedInterpreterGroup.java[close]:108) - Kill RemoteInterpreterProcess
 INFO [2020-01-22 23:38:55,521] ({Thread-813} RemoteInterpreterManagedProcess.java[stop]:220) - Kill interpreter process
ERROR [2020-01-22 23:38:55,651] ({Thread-199} RemoteInterpreterEventPoller.java[run]:257) - Can not get RemoteInterpreterEvent because it is shutdown.
ERROR [2020-01-22 23:38:55,651] ({pool-8-thread-1} AppendOutputRunner.java[run]:68) - Wait for OutputBuffer queue interrupted: null
 WARN [2020-01-22 23:38:57,864] ({Thread-810} RemoteInterpreterManagedProcess.java[stop]:230) - ignore the exception when shutting down
 INFO [2020-01-22 23:38:57,866] ({Thread-810} RemoteInterpreterManagedProcess.java[stop]:238) - Remote process terminated
 INFO [2020-01-22 23:38:57,867] ({Thread-831} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: redshift:shared_process
 INFO [2020-01-22 23:38:57,870] ({Exec Default Executor} RemoteInterpreterManagedProcess.java[onProcessFailed]:250) - Interpreter process failed {}
org.apache.commons.exec.ExecuteException: Process exited with an error: 143 (Exit value: 143)
	at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:404)
	at org.apache.commons.exec.DefaultExecutor.access$200(DefaultExecutor.java:48)
	at org.apache.commons.exec.DefaultExecutor$1.run(DefaultExecutor.java:200)
	at java.lang.Thread.run(Thread.java:748)
 WARN [2020-01-22 23:38:58,176] ({Thread-813} RemoteInterpreterManagedProcess.java[stop]:230) - ignore the exception when shutting down
 INFO [2020-01-22 23:38:58,177] ({Thread-813} RemoteInterpreterManagedProcess.java[stop]:238) - Remote process terminated
 INFO [2020-01-22 23:38:58,178] ({Thread-834} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: spark:shared_process
 INFO [2020-01-22 23:38:58,178] ({Thread-13} NotebookRepoSync.java[close]:428) - Closing all notebook storages
 INFO [2020-01-22 23:38:58,180] ({Exec Default Executor} RemoteInterpreterManagedProcess.java[onProcessFailed]:250) - Interpreter process failed {}
org.apache.commons.exec.ExecuteException: Process exited with an error: 143 (Exit value: 143)
	at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:404)
	at org.apache.commons.exec.DefaultExecutor.access$200(DefaultExecutor.java:48)
	at org.apache.commons.exec.DefaultExecutor$1.run(DefaultExecutor.java:200)
	at java.lang.Thread.run(Thread.java:748)
 INFO [2020-01-22 23:39:01,202] ({Thread-13} ZeppelinServer.java[run]:264) - Bye
 WARN [2020-01-22 23:57:48,642] ({main} ZeppelinConfiguration.java[create]:117) - Failed to load configuration, proceeding with a default
 INFO [2020-01-22 23:57:49,090] ({main} ZeppelinConfiguration.java[create]:129) - Server Host: 0.0.0.0
 INFO [2020-01-22 23:57:49,090] ({main} ZeppelinConfiguration.java[create]:131) - Server Port: 8080
 INFO [2020-01-22 23:57:49,090] ({main} ZeppelinConfiguration.java[create]:135) - Context Path: /
 INFO [2020-01-22 23:57:49,133] ({main} ZeppelinConfiguration.java[create]:136) - Zeppelin Version: 0.8.2
 INFO [2020-01-22 23:57:49,282] ({main} Log.java[initialized]:193) - Logging initialized @4120ms to org.eclipse.jetty.util.log.Slf4jLog
 WARN [2020-01-22 23:57:50,008] ({main} ServerConnector.java[setSoLingerTime]:458) - Ignoring deprecated socket close linger time
 INFO [2020-01-22 23:57:50,446] ({main} ZeppelinServer.java[setupWebAppContext]:413) - ZeppelinServer Webapp path: /zeppelin/webapps
 INFO [2020-01-22 23:57:51,109] ({main} ZeppelinServer.java[main]:239) - Starting zeppelin server
 INFO [2020-01-22 23:57:51,110] ({main} Server.java[doStart]:370) - jetty-9.4.14.v20181114; built: 2018-11-14T21:20:31.478Z; git: c4550056e785fb5665914545889f21dc136ad9e6; jvm 1.8.0_222-8u222-b10-1ubuntu1~16.04.1-b10
 INFO [2020-01-22 23:58:03,124] ({main} StandardDescriptorProcessor.java[visitServlet]:283) - NO JSP Support for /, did not find org.eclipse.jetty.jsp.JettyJspServlet
 INFO [2020-01-22 23:58:03,201] ({main} DefaultSessionIdManager.java[doStart]:365) - DefaultSessionIdManager workerName=node0
 INFO [2020-01-22 23:58:03,202] ({main} DefaultSessionIdManager.java[doStart]:370) - No SessionScavenger set, using defaults
 INFO [2020-01-22 23:58:03,214] ({main} HouseKeeper.java[startScavenging]:149) - node0 Scavenging every 660000ms
 INFO [2020-01-22 23:58:05,325] ({main} SchedulerFactory.java[<init>]:59) - Scheduler Thread Pool Size: 100
 WARN [2020-01-22 23:58:05,401] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir
 WARN [2020-01-22 23:58:05,421] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir
 INFO [2020-01-22 23:58:05,832] ({main} InterpreterSettingManager.java[<init>]:165) - Using RecoveryStorage: org.apache.zeppelin.interpreter.recovery.NullRecoveryStorage
 INFO [2020-01-22 23:58:05,844] ({main} InterpreterSettingManager.java[<init>]:169) - Using LifecycleManager: org.apache.zeppelin.interpreter.lifecycle.NullLifecycleManager
 INFO [2020-01-22 23:58:06,076] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: cassandra
 INFO [2020-01-22 23:58:06,105] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: md
 INFO [2020-01-22 23:58:06,158] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: sap
 INFO [2020-01-22 23:58:06,192] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: sh
 INFO [2020-01-22 23:58:06,238] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: angular
 WARN [2020-01-22 23:58:06,297] ({main} InterpreterSettingManager.java[init]:331) - No interpreter-setting.json found in /zeppelin/interpreter/lib
 INFO [2020-01-22 23:58:06,315] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: hbase
 INFO [2020-01-22 23:58:06,353] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: elasticsearch
 WARN [2020-01-22 23:58:06,964] ({main} InterpreterSettingManager.java[init]:331) - No interpreter-setting.json found in /zeppelin/interpreter/scio
 INFO [2020-01-22 23:58:06,992] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: alluxio
 INFO [2020-01-22 23:58:06,999] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: jdbc
 INFO [2020-01-22 23:58:07,049] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: neo4j
 INFO [2020-01-22 23:58:07,065] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: python
 INFO [2020-01-22 23:58:07,084] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: pig
 INFO [2020-01-22 23:58:07,111] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: spark
 INFO [2020-01-22 23:58:07,122] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: lens
 INFO [2020-01-22 23:58:07,154] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: bigquery
 INFO [2020-01-22 23:58:07,162] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: file
 INFO [2020-01-22 23:58:07,185] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: ignite
 INFO [2020-01-22 23:58:07,209] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: kylin
 INFO [2020-01-22 23:58:07,237] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: livy
 INFO [2020-01-22 23:58:07,255] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: groovy
 INFO [2020-01-22 23:58:07,272] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: flink
 WARN [2020-01-22 23:58:07,284] ({main} InterpreterSettingManager.java[init]:331) - No interpreter-setting.json found in /zeppelin/interpreter/${interpreter.name}
 INFO [2020-01-22 23:58:07,289] ({main} LocalConfigStorage.java[loadInterpreterSettings]:63) - Load Interpreter Setting from file: /zeppelin/conf/interpreter.json
 INFO [2020-01-22 23:58:07,573] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting ignite from interpreter.json
 INFO [2020-01-22 23:58:07,583] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting python from interpreter.json
 INFO [2020-01-22 23:58:07,587] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting sap from interpreter.json
 INFO [2020-01-22 23:58:07,596] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting jdbc from interpreter.json
 INFO [2020-01-22 23:58:07,600] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting lens from interpreter.json
 INFO [2020-01-22 23:58:07,601] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting pig from interpreter.json
 INFO [2020-01-22 23:58:07,612] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting flink from interpreter.json
 INFO [2020-01-22 23:58:07,613] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting angular from interpreter.json
 INFO [2020-01-22 23:58:07,616] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting livy from interpreter.json
 INFO [2020-01-22 23:58:07,622] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting neo4j from interpreter.json
 INFO [2020-01-22 23:58:07,623] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting file from interpreter.json
 INFO [2020-01-22 23:58:07,627] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting groovy from interpreter.json
 INFO [2020-01-22 23:58:07,629] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting elasticsearch from interpreter.json
 INFO [2020-01-22 23:58:07,633] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting redshift from interpreter.json
 INFO [2020-01-22 23:58:07,635] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting cassandra from interpreter.json
 INFO [2020-01-22 23:58:07,646] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting sh from interpreter.json
 INFO [2020-01-22 23:58:07,652] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting spark from interpreter.json
 INFO [2020-01-22 23:58:07,655] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting md from interpreter.json
 INFO [2020-01-22 23:58:07,660] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting alluxio from interpreter.json
 INFO [2020-01-22 23:58:07,662] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting bigquery from interpreter.json
 INFO [2020-01-22 23:58:07,668] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting hbase from interpreter.json
 INFO [2020-01-22 23:58:07,670] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting kylin from interpreter.json
 INFO [2020-01-22 23:58:07,788] ({main} LocalConfigStorage.java[save]:53) - Save Interpreter Setting to /zeppelin/conf/interpreter.json
 INFO [2020-01-22 23:58:08,294] ({main} VfsLog.java[info]:138) - Using "/tmp/vfs_cache" as temporary files store.
ERROR [2020-01-22 23:58:08,397] ({Thread-22} InterpreterSetting.java[run]:840) - Error while downloading repos for interpreter group : jdbc, go to interpreter setting page click on edit and save it again to make this interpreter work properly. : Cannot fetch dependencies for com.amazonaws:aws-java-sdk-redshift:1.11.51
org.sonatype.aether.RepositoryException: Cannot fetch dependencies for com.amazonaws:aws-java-sdk-redshift:1.11.51
	at org.apache.zeppelin.dep.DependencyResolver.getArtifactsWithDep(DependencyResolver.java:179)
	at org.apache.zeppelin.dep.DependencyResolver.loadFromMvn(DependencyResolver.java:128)
	at org.apache.zeppelin.dep.DependencyResolver.load(DependencyResolver.java:76)
	at org.apache.zeppelin.dep.DependencyResolver.load(DependencyResolver.java:93)
	at org.apache.zeppelin.dep.DependencyResolver.load(DependencyResolver.java:85)
	at org.apache.zeppelin.interpreter.InterpreterSetting$1.run(InterpreterSetting.java:832)
Caused by: java.lang.NullPointerException
	at org.sonatype.aether.impl.internal.DefaultRepositorySystem.resolveDependencies(DefaultRepositorySystem.java:352)
	at org.apache.zeppelin.dep.DependencyResolver.getArtifactsWithDep(DependencyResolver.java:176)
	... 5 more
 INFO [2020-01-22 23:58:08,550] ({main} GitNotebookRepo.java[<init>]:64) - Opening a git repo at '/zeppelin/notebook'
 INFO [2020-01-22 23:58:08,753] ({main} NotebookRepoSync.java[<init>]:77) - Instantiate NotebookRepo: org.apache.zeppelin.notebook.repo.GitNotebookRepo
 WARN [2020-01-22 23:58:09,243] ({main} NotebookAuthorization.java[getInstance]:86) - Notebook authorization module was called without initialization, initializing with default configuration
 WARN [2020-01-22 23:58:09,256] ({main} LocalConfigStorage.java[loadNotebookAuthorization]:77) - NotebookAuthorization file /zeppelin/conf/notebook-authorization.json is not existed
 INFO [2020-01-22 23:58:09,258] ({main} Credentials.java[loadFromFile]:121) - /zeppelin/conf/credentials.json
 INFO [2020-01-22 23:58:09,444] ({main} StdSchedulerFactory.java[instantiate]:1184) - Using default implementation for ThreadExecutor
 INFO [2020-01-22 23:58:09,459] ({main} SimpleThreadPool.java[initialize]:268) - Job execution threads will use class loader of thread: main
 INFO [2020-01-22 23:58:09,583] ({main} SchedulerSignalerImpl.java[<init>]:61) - Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
 INFO [2020-01-22 23:58:09,603] ({main} QuartzScheduler.java[<init>]:240) - Quartz Scheduler v.2.2.1 created.
 INFO [2020-01-22 23:58:09,605] ({main} RAMJobStore.java[initialize]:155) - RAMJobStore initialized.
 INFO [2020-01-22 23:58:09,614] ({main} QuartzScheduler.java[initialize]:305) - Scheduler meta-data: Quartz Scheduler (v2.2.1) 'DefaultQuartzScheduler' with instanceId 'NON_CLUSTERED'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'org.quartz.simpl.SimpleThreadPool' - with 10 threads.
  Using job-store 'org.quartz.simpl.RAMJobStore' - which does not support persistence. and is not clustered.

 INFO [2020-01-22 23:58:09,623] ({main} StdSchedulerFactory.java[instantiate]:1339) - Quartz scheduler 'DefaultQuartzScheduler' initialized from default resource file in Quartz package: 'quartz.properties'
 INFO [2020-01-22 23:58:09,632] ({main} StdSchedulerFactory.java[instantiate]:1343) - Quartz scheduler version: 2.2.1
 INFO [2020-01-22 23:58:09,633] ({main} QuartzScheduler.java[start]:575) - Scheduler DefaultQuartzScheduler_$_NON_CLUSTERED started.
 INFO [2020-01-22 23:58:10,293] ({main} FolderView.java[createFolder]:107) - Create folder /
 INFO [2020-01-22 23:58:10,298] ({main} Folder.java[setParent]:169) - Set parent of / to /
 INFO [2020-01-22 23:58:10,302] ({main} Folder.java[addNote]:185) - Add note 2EXBGWKU1 to folder /
 WARN [2020-01-22 23:58:10,305] ({main} Notebook.java[refreshCron]:981) - execution of the cron job is skipped cron is not enabled from Zeppelin server
 INFO [2020-01-22 23:58:10,463] ({main} Folder.java[addNote]:185) - Add note 2EXQEGYMG to folder /
 WARN [2020-01-22 23:58:10,464] ({main} Notebook.java[refreshCron]:981) - execution of the cron job is skipped cron is not enabled from Zeppelin server
 INFO [2020-01-22 23:58:10,511] ({main} Folder.java[addNote]:185) - Add note 2EXSBN97B to folder /
 WARN [2020-01-22 23:58:10,516] ({main} Notebook.java[refreshCron]:981) - execution of the cron job is skipped cron is not enabled from Zeppelin server
 INFO [2020-01-22 23:58:10,517] ({main} Notebook.java[<init>]:127) - Notebook indexing started...
 INFO [2020-01-22 23:58:11,239] ({main} LuceneSearch.java[addIndexDocs]:305) - Indexing 3 notebooks took 721ms
 INFO [2020-01-22 23:58:11,241] ({main} Notebook.java[<init>]:129) - Notebook indexing finished: 3 indexed in 0s
 INFO [2020-01-22 23:58:11,244] ({main} Helium.java[loadConf]:103) - Add helium local registry /zeppelin/helium
 INFO [2020-01-22 23:58:11,252] ({main} Helium.java[loadConf]:100) - Add helium online registry https://s3.amazonaws.com/helium-package/helium.json
 WARN [2020-01-22 23:58:11,265] ({main} Helium.java[loadConf]:111) - /zeppelin/conf/helium.json does not exists
 INFO [2020-01-22 23:58:18,835] ({main} ContextHandler.java[doStart]:855) - Started o.e.j.w.WebAppContext@1d548a08{zeppelin-web,/,file:///zeppelin/webapps/webapp/,AVAILABLE}{/zeppelin/zeppelin-web-0.8.2.war}
 INFO [2020-01-22 23:58:18,942] ({main} AbstractConnector.java[doStart]:292) - Started ServerConnector@79a41e8{HTTP/1.1,[http/1.1]}{0.0.0.0:8080}
 INFO [2020-01-22 23:58:18,947] ({main} Server.java[doStart]:407) - Started @33795ms
 INFO [2020-01-22 23:58:18,948] ({main} ZeppelinServer.java[main]:249) - Done, zeppelin server started
 WARN [2020-01-22 23:59:23,272] ({qtp395629617-13} SecurityRestApi.java[ticket]:88) - {"status":"OK","message":"","body":{"principal":"anonymous","ticket":"anonymous","roles":"[]"}}
 INFO [2020-01-22 23:59:23,500] ({qtp395629617-9} NotebookServer.java[onOpen]:151) - New connection from 192.168.99.1 : 21937
 WARN [2020-01-22 23:59:34,266] ({qtp395629617-12} SecurityRestApi.java[ticket]:88) - {"status":"OK","message":"","body":{"principal":"anonymous","ticket":"anonymous","roles":"[]"}}
 INFO [2020-01-22 23:59:34,293] ({qtp395629617-15} NotebookServer.java[onOpen]:151) - New connection from 192.168.99.1 : 21938
 WARN [2020-01-22 23:59:37,711] ({qtp395629617-12} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:59:37,715] ({qtp395629617-12} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:59:37,716] ({qtp395629617-12} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:59:37,720] ({qtp395629617-12} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:59:37,721] ({qtp395629617-12} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-22 23:59:37,721] ({qtp395629617-12} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
