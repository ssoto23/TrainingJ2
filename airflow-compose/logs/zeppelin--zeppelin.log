 WARN [2020-01-19 00:01:45,216] ({qtp2107447833-11} SecurityRestApi.java[ticket]:88) - {"status":"OK","message":"","body":{"principal":"anonymous","ticket":"anonymous","roles":"[]"}}
 INFO [2020-01-19 00:01:45,317] ({qtp2107447833-14} NotebookServer.java[onOpen]:151) - New connection from 192.168.99.1 : 9695
 INFO [2020-01-19 00:01:48,853] ({qtp2107447833-13} NotebookServer.java[sendNote]:828) - New operation from 192.168.99.1 : 9695 : anonymous : GET_NOTE : 2EZEECRFJ
 WARN [2020-01-19 00:01:48,956] ({qtp2107447833-13} GitNotebookRepo.java[revisionHistory]:158) - No Head found for 2EZEECRFJ, No HEAD exists and no explicit starting revision was specified
 WARN [2020-01-19 00:01:48,964] ({qtp2107447833-13} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-19 00:01:48,969] ({qtp2107447833-13} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-19 00:01:48,971] ({qtp2107447833-13} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-19 00:01:48,972] ({qtp2107447833-13} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 INFO [2020-01-19 00:01:48,975] ({qtp2107447833-15} InterpreterSetting.java[getOrCreateInterpreterGroup]:419) - Create InterpreterGroup with groupId: spark:shared_process for user: anonymous and note: 2EZEECRFJ
 INFO [2020-01-19 00:01:48,978] ({qtp2107447833-15} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.SparkInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-19 00:01:48,978] ({qtp2107447833-15} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.SparkSqlInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-19 00:01:48,979] ({qtp2107447833-15} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.DepInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-19 00:01:48,980] ({qtp2107447833-15} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.PySparkInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-19 00:01:48,981] ({qtp2107447833-15} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.IPySparkInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-19 00:01:48,982] ({qtp2107447833-15} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.SparkRInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-19 00:01:48,982] ({qtp2107447833-15} ManagedInterpreterGroup.java[getOrCreateSession]:158) - Create Session: shared_session in InterpreterGroup: spark:shared_process for user: anonymous
 WARN [2020-01-19 00:01:48,978] ({qtp2107447833-13} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-19 00:01:48,987] ({qtp2107447833-13} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 INFO [2020-01-19 00:02:16,659] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:02:16,697] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195314_1536993808 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:02:16,699] ({pool-2-thread-2} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195314_1536993808, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:02:16,700] ({pool-2-thread-2} ManagedInterpreterGroup.java[getOrCreateInterpreterProcess]:61) - Create InterpreterProcess for InterpreterGroup: spark:shared_process
 INFO [2020-01-19 00:02:16,701] ({pool-2-thread-2} ShellScriptLauncher.java[launch]:48) - Launching Interpreter: spark
 INFO [2020-01-19 00:02:16,724] ({pool-2-thread-2} SparkInterpreterLauncher.java[buildEnvFromProperties]:108) - Run Spark under non-secure mode as no keytab and principal is specified
 INFO [2020-01-19 00:02:16,727] ({pool-2-thread-2} RemoteInterpreterManagedProcess.java[start]:115) - Thrift server for callback will start. Port: 35275
 INFO [2020-01-19 00:02:17,238] ({pool-2-thread-2} RemoteInterpreterManagedProcess.java[start]:190) - Run interpreter process [/zeppelin/bin/interpreter.sh, -d, /zeppelin/interpreter/spark, -c, 172.18.0.4, -p, 35275, -r, :, -l, /zeppelin/local-repo/spark, -g, spark]
 INFO [2020-01-19 00:02:21,240] ({pool-7-thread-1} RemoteInterpreterManagedProcess.java[callback]:123) - RemoteInterpreterServer Registered: CallbackInfo(host:172.18.0.4, port:35437)
 INFO [2020-01-19 00:02:21,342] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkInterpreter
 INFO [2020-01-19 00:02:21,450] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkSqlInterpreter
 INFO [2020-01-19 00:02:21,453] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.DepInterpreter
 INFO [2020-01-19 00:02:21,460] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.PySparkInterpreter
 INFO [2020-01-19 00:02:21,467] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.IPySparkInterpreter
 INFO [2020-01-19 00:02:21,494] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkRInterpreter
 INFO [2020-01-19 00:02:21,500] ({pool-2-thread-2} RemoteInterpreter.java[call]:142) - Open RemoteInterpreter org.apache.zeppelin.spark.PySparkInterpreter
 INFO [2020-01-19 00:02:21,501] ({pool-2-thread-2} RemoteInterpreter.java[pushAngularObjectRegistryToRemote]:436) - Push local angular object registry from ZeppelinServer to remote interpreter group spark:shared_process
 INFO [2020-01-19 00:02:56,804] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2314) - Job 20200118-195314_1536993808 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:02:56,827] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:02:56,842] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195314_1536993808 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:03:28,216] ({qtp2107447833-11} NotebookServer.java[broadcastNewParagraph]:688) - Broadcasting paragraph on run call instead of note.
 INFO [2020-01-19 00:03:28,235] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:03:28,256] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:03:28,256] ({pool-2-thread-2} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195328_2060007234, interpreter: , note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:03:28,257] ({pool-2-thread-2} RemoteInterpreter.java[call]:142) - Open RemoteInterpreter org.apache.zeppelin.spark.SparkInterpreter
 WARN [2020-01-19 00:03:28,289] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2316) - Job 20200118-195328_2060007234 is finished, status: ERROR, exception: null, result: %text <console>:1: error: illegal start of simple expression
dataFrame = spark.createDataFrame([range(1, 14)],  ['number'])
                                  ^
<console>:1: error: unclosed character literal
dataFrame = spark.createDataFrame([range(1, 14)],  ['number'])
                                                           ^

 INFO [2020-01-19 00:03:28,335] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:03:28,352] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:03:37,888] ({qtp2107447833-57} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:03:37,901] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:03:37,906] ({pool-2-thread-3} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195328_2060007234, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:03:39,603] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2314) - Job 20200118-195328_2060007234 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:03:39,633] ({pool-2-thread-3} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:03:39,653] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:04:33,520] ({qtp2107447833-57} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:05:12,700] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:05:27,064] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:05:57,711] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:05:57,723] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:05:57,724] ({pool-2-thread-2} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195328_2060007234, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:05:57,781] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2316) - Job 20200118-195328_2060007234 is finished, status: ERROR, exception: null, result: %text [0;36m  File [0;32m"<ipython-input-8-af19396184e4>"[0;36m, line [0;32m1[0m
[0;31m    dataFrames = [spark.createDataFrame(i, ['number']) for i in range(1, 15))[0m
[0m                                                                            ^[0m
[0;31mSyntaxError[0m[0;31m:[0m invalid syntax

 INFO [2020-01-19 00:05:57,801] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:05:57,822] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:06:05,835] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:06:05,847] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:06:05,848] ({pool-2-thread-4} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195328_2060007234, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:06:05,909] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2316) - Job 20200118-195328_2060007234 is finished, status: ERROR, exception: null, result: %text [0;36m  File [0;32m"<ipython-input-10-79f10337e62f>"[0;36m, line [0;32m1[0m
[0;31m    dataFrames = [spark.createDataFrame(i, ['number']) for i in range(1, 15))][0m
[0m                                                                            ^[0m
[0;31mSyntaxError[0m[0;31m:[0m invalid syntax

 INFO [2020-01-19 00:06:05,925] ({pool-2-thread-4} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:06:05,939] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:06:10,460] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:06:10,475] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:06:10,475] ({pool-2-thread-3} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195328_2060007234, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:06:10,656] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2316) - Job 20200118-195328_2060007234 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mTypeError[0mTraceback (most recent call last)
[0;32m<ipython-input-12-a3923a121b57>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdataFrames[0m [0;34m=[0m [0;34m[[0m[0mspark[0m[0;34m.[0m[0mcreateDataFrame[0m[0;34m([0m[0mi[0m[0;34m,[0m [0;34m[[0m[0;34m'number'[0m[0;34m][0m[0;34m)[0m [0;32mfor[0m [0mi[0m [0;32min[0m [0mrange[0m[0;34m([0m[0;36m1[0m[0;34m,[0m [0;36m15[0m[0;34m)[0m[0;34m][0m[0;34m[0m[0m
[0m[1;32m      2[0m [0;34m[0m[0m
[1;32m      3[0m [0;34m[0m[0m
[1;32m      4[0m [0mdataFrames[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36mcreateDataFrame[0;34m(self, data, schema, samplingRatio, verifySchema)[0m
[1;32m    746[0m             [0mrdd[0m[0;34m,[0m [0mschema[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_createFromRDD[0m[0;34m([0m[0mdata[0m[0;34m.[0m[0mmap[0m[0;34m([0m[0mprepare[0m[0;34m)[0m[0;34m,[0m [0mschema[0m[0;34m,[0m [0msamplingRatio[0m[0;34m)[0m[0;34m[0m[0m
[1;32m    747[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 748[0;31m             [0mrdd[0m[0;34m,[0m [0mschema[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_createFromLocal[0m[0;34m([0m[0mmap[0m[0;34m([0m[0mprepare[0m[0;34m,[0m [0mdata[0m[0;34m)[0m[0;34m,[0m [0mschema[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    749[0m         [0mjrdd[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_jvm[0m[0;34m.[0m[0mSerDeUtil[0m[0;34m.[0m[0mtoJavaArray[0m[0;34m([0m[0mrdd[0m[0;34m.[0m[0m_to_java_object_rdd[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[1;32m    750[0m         [0mjdf[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_jsparkSession[0m[0;34m.[0m[0mapplySchemaToPythonRDD[0m[0;34m([0m[0mjrdd[0m[0;34m.[0m[0mrdd[0m[0;34m([0m[0;34m)[0m[0;34m,[0m [0mschema[0m[0;34m.[0m[0mjson[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m

[0;31mTypeError[0m: 'int' object is not iterable
 INFO [2020-01-19 00:06:10,670] ({pool-2-thread-3} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:06:10,690] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:06:22,311] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:06:22,334] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:06:22,334] ({pool-2-thread-5} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195328_2060007234, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:06:22,467] ({pool-2-thread-5} NotebookServer.java[afterStatusChange]:2316) - Job 20200118-195328_2060007234 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mTypeError[0mTraceback (most recent call last)
[0;32m<ipython-input-14-7cb2cb148db3>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdataFrames[0m [0;34m=[0m [0;34m[[0m[0mspark[0m[0;34m.[0m[0mcreateDataFrame[0m[0;34m([0m[0mi[0m[0;34m,[0m [0;34m[[0m[0;34m'number'[0m[0;34m][0m[0;34m)[0m [0;32mfor[0m [0mi[0m [0;32min[0m [0;34m[[0m[0mrange[0m[0;34m([0m[0;36m1[0m[0;34m,[0m [0;36m15[0m[0;34m)[0m[0;34m][0m[0;34m][0m[0;34m[0m[0m
[0m[1;32m      2[0m [0;34m[0m[0m
[1;32m      3[0m [0;34m[0m[0m
[1;32m      4[0m [0mdataFrames[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36mcreateDataFrame[0;34m(self, data, schema, samplingRatio, verifySchema)[0m
[1;32m    746[0m             [0mrdd[0m[0;34m,[0m [0mschema[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_createFromRDD[0m[0;34m([0m[0mdata[0m[0;34m.[0m[0mmap[0m[0;34m([0m[0mprepare[0m[0;34m)[0m[0;34m,[0m [0mschema[0m[0;34m,[0m [0msamplingRatio[0m[0;34m)[0m[0;34m[0m[0m
[1;32m    747[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 748[0;31m             [0mrdd[0m[0;34m,[0m [0mschema[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_createFromLocal[0m[0;34m([0m[0mmap[0m[0;34m([0m[0mprepare[0m[0;34m,[0m [0mdata[0m[0;34m)[0m[0;34m,[0m [0mschema[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    749[0m         [0mjrdd[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_jvm[0m[0;34m.[0m[0mSerDeUtil[0m[0;34m.[0m[0mtoJavaArray[0m[0;34m([0m[0mrdd[0m[0;34m.[0m[0m_to_java_object_rdd[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[1;32m    750[0m         [0mjdf[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_jsparkSession[0m[0;34m.[0m[0mapplySchemaToPythonRDD[0m[0;34m([0m[0mjrdd[0m[0;34m.[0m[0mrdd[0m[0;34m([0m[0;34m)[0m[0;34m,[0m [0mschema[0m[0;34m.[0m[0mjson[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36m_createFromLocal[0;34m(self, data, schema)[0m
[1;32m    414[0m [0;34m[0m[0m
[1;32m    415[0m         [0;32mif[0m [0mschema[0m [0;32mis[0m [0mNone[0m [0;32mor[0m [0misinstance[0m[0;34m([0m[0mschema[0m[0;34m,[0m [0;34m([0m[0mlist[0m[0;34m,[0m [0mtuple[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 416[0;31m             [0mstruct[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_inferSchemaFromList[0m[0;34m([0m[0mdata[0m[0;34m,[0m [0mnames[0m[0;34m=[0m[0mschema[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    417[0m             [0mconverter[0m [0;34m=[0m [0m_create_converter[0m[0;34m([0m[0mstruct[0m[0;34m)[0m[0;34m[0m[0m
[1;32m    418[0m             [0mdata[0m [0;34m=[0m [0mmap[0m[0;34m([0m[0mconverter[0m[0;34m,[0m [0mdata[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36m_inferSchemaFromList[0;34m(self, data, names)[0m
[1;32m    346[0m             warnings.warn("inferring schema from dict is deprecated,"
[1;32m    347[0m                           "please use pyspark.sql.Row instead")
[0;32m--> 348[0;31m         [0mschema[0m [0;34m=[0m [0mreduce[0m[0;34m([0m[0m_merge_type[0m[0;34m,[0m [0;34m([0m[0m_infer_schema[0m[0;34m([0m[0mrow[0m[0;34m,[0m [0mnames[0m[0;34m)[0m [0;32mfor[0m [0mrow[0m [0;32min[0m [0mdata[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    349[0m         [0;32mif[0m [0m_has_nulltype[0m[0;34m([0m[0mschema[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    350[0m             [0;32mraise[0m [0mValueError[0m[0;34m([0m[0;34m"Some of types cannot be determined after inferring"[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36m<genexpr>[0;34m((row,))[0m
[1;32m    346[0m             warnings.warn("inferring schema from dict is deprecated,"
[1;32m    347[0m                           "please use pyspark.sql.Row instead")
[0;32m--> 348[0;31m         [0mschema[0m [0;34m=[0m [0mreduce[0m[0;34m([0m[0m_merge_type[0m[0;34m,[0m [0;34m([0m[0m_infer_schema[0m[0;34m([0m[0mrow[0m[0;34m,[0m [0mnames[0m[0;34m)[0m [0;32mfor[0m [0mrow[0m [0;32min[0m [0mdata[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    349[0m         [0;32mif[0m [0m_has_nulltype[0m[0;34m([0m[0mschema[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    350[0m             [0;32mraise[0m [0mValueError[0m[0;34m([0m[0;34m"Some of types cannot be determined after inferring"[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/types.py[0m in [0;36m_infer_schema[0;34m(row, names)[0m
[1;32m   1060[0m [0;34m[0m[0m
[1;32m   1061[0m     [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[0;32m-> 1062[0;31m         [0;32mraise[0m [0mTypeError[0m[0;34m([0m[0;34m"Can not infer schema for type: %s"[0m [0;34m%[0m [0mtype[0m[0;34m([0m[0mrow[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1063[0m [0;34m[0m[0m
[1;32m   1064[0m     [0mfields[0m [0;34m=[0m [0;34m[[0m[0mStructField[0m[0;34m([0m[0mk[0m[0;34m,[0m [0m_infer_type[0m[0;34m([0m[0mv[0m[0;34m)[0m[0;34m,[0m [0mTrue[0m[0;34m)[0m [0;32mfor[0m [0mk[0m[0;34m,[0m [0mv[0m [0;32min[0m [0mitems[0m[0;34m][0m[0;34m[0m[0m

[0;31mTypeError[0m: Can not infer schema for type: <type 'int'>
 INFO [2020-01-19 00:06:22,484] ({pool-2-thread-5} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:06:22,508] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:06:32,975] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:06:33,001] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:06:33,001] ({pool-2-thread-2} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195328_2060007234, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:06:33,120] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2314) - Job 20200118-195328_2060007234 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:06:33,142] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:06:33,159] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:06:51,670] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:06:51,689] ({pool-2-thread-6} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:06:51,694] ({pool-2-thread-6} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195328_2060007234, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:06:51,770] ({pool-2-thread-6} NotebookServer.java[afterStatusChange]:2314) - Job 20200118-195328_2060007234 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:06:51,783] ({pool-2-thread-6} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:06:51,793] ({pool-2-thread-6} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:06:54,694] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:06:54,709] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:06:54,709] ({pool-2-thread-4} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195328_2060007234, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:06:54,759] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2314) - Job 20200118-195328_2060007234 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:06:54,775] ({pool-2-thread-4} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:06:54,791] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:07:02,382] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:07:02,402] ({pool-2-thread-7} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:07:02,402] ({pool-2-thread-7} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195328_2060007234, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:07:02,525] ({pool-2-thread-7} NotebookServer.java[afterStatusChange]:2314) - Job 20200118-195328_2060007234 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:07:02,536] ({pool-2-thread-7} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:07:02,554] ({pool-2-thread-7} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:07:12,763] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:07:12,776] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:07:12,777] ({pool-2-thread-3} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195328_2060007234, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:07:12,840] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2314) - Job 20200118-195328_2060007234 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:07:12,856] ({pool-2-thread-3} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:07:12,867] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:07:24,772] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:07:24,784] ({pool-2-thread-8} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:07:24,785] ({pool-2-thread-8} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195328_2060007234, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:07:24,849] ({pool-2-thread-8} NotebookServer.java[afterStatusChange]:2316) - Job 20200118-195328_2060007234 is finished, status: ERROR, exception: null, result: %text [0;36m  File [0;32m"<ipython-input-26-059baf6e0b7f>"[0;36m, line [0;32m1[0m
[0;31m    dataFrames = i for i in [range(1, 15)][0m
[0m                     ^[0m
[0;31mSyntaxError[0m[0;31m:[0m invalid syntax

 INFO [2020-01-19 00:07:24,858] ({pool-2-thread-8} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:07:24,873] ({pool-2-thread-8} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:07:27,353] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:07:27,372] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:07:27,372] ({pool-2-thread-5} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195328_2060007234, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:07:27,434] ({pool-2-thread-5} NotebookServer.java[afterStatusChange]:2314) - Job 20200118-195328_2060007234 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:07:27,445] ({pool-2-thread-5} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:07:27,464] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:08:18,155] ({qtp2107447833-15} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:09:09,729] ({qtp2107447833-15} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:09:09,757] ({pool-2-thread-9} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:09:09,758] ({pool-2-thread-9} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195328_2060007234, interpreter: , note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:09:09,768] ({pool-2-thread-9} NotebookServer.java[afterStatusChange]:2316) - Job 20200118-195328_2060007234 is finished, status: ERROR, exception: null, result: %text <console>:1: error: illegal start of simple expression
dataFrame = spark.createDataFrame([(1,),(2,),(3,),(4,),(5,),(6,),(7,),(8,),(9,),(10,),(11,),(12,),(13,),(14,)],  ['session_idx'])
                                  ^
<console>:1: error: unclosed character literal
dataFrame = spark.createDataFrame([(1,),(2,),(3,),(4,),(5,),(6,),(7,),(8,),(9,),(10,),(11,),(12,),(13,),(14,)],  ['session_idx'])
                                                                                                                              ^

 INFO [2020-01-19 00:09:09,778] ({pool-2-thread-9} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:09:09,798] ({pool-2-thread-9} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:09:25,623] ({qtp2107447833-57} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:09:25,651] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:09:25,651] ({pool-2-thread-2} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195328_2060007234, interpreter: , note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:09:25,675] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2316) - Job 20200118-195328_2060007234 is finished, status: ERROR, exception: null, result: %text <console>:1: error: illegal start of simple expression
dataFrame = spark.createDataFrame([(1,),(2,),(3,),(4,),(5,),(6,),(7,),(8,),(9,),(10,),(11,),(12,),(13,),(14,)],  ['session_idx'])
                                  ^
<console>:1: error: unclosed character literal
dataFrame = spark.createDataFrame([(1,),(2,),(3,),(4,),(5,),(6,),(7,),(8,),(9,),(10,),(11,),(12,),(13,),(14,)],  ['session_idx'])
                                                                                                                              ^

 INFO [2020-01-19 00:09:25,687] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:09:25,703] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:09:56,517] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:09:56,537] ({pool-2-thread-10} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:09:56,542] ({pool-2-thread-10} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195328_2060007234, interpreter: , note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:09:56,561] ({pool-2-thread-10} NotebookServer.java[afterStatusChange]:2316) - Job 20200118-195328_2060007234 is finished, status: ERROR, exception: null, result: %text <console>:1: error: illegal start of simple expression
dataFrame = spark.createDataFrame([(1,""),(2,""),(3,""),(4,""),(5,""),(6,""),(7,""),(8,""),(9,""),(10,""),(11,""),(12,""),(13,""),(14,"")],  ["nuym",'session_idx'])
                                  ^
<console>:1: error: unclosed character literal
dataFrame = spark.createDataFrame([(1,""),(2,""),(3,""),(4,""),(5,""),(6,""),(7,""),(8,""),(9,""),(10,""),(11,""),(12,""),(13,""),(14,"")],  ["nuym",'session_idx'])
                                                                                                                                                                 ^

 INFO [2020-01-19 00:09:56,568] ({pool-2-thread-10} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:09:56,591] ({pool-2-thread-10} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:09:57,702] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:09:57,725] ({pool-2-thread-6} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:09:57,725] ({pool-2-thread-6} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195328_2060007234, interpreter: , note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:09:57,734] ({pool-2-thread-6} NotebookServer.java[afterStatusChange]:2316) - Job 20200118-195328_2060007234 is finished, status: ERROR, exception: null, result: %text <console>:1: error: illegal start of simple expression
dataFrame = spark.createDataFrame([(1,""),(2,""),(3,""),(4,""),(5,""),(6,""),(7,""),(8,""),(9,""),(10,""),(11,""),(12,""),(13,""),(14,"")],  ["nuym",'session_idx'])
                                  ^
<console>:1: error: unclosed character literal
dataFrame = spark.createDataFrame([(1,""),(2,""),(3,""),(4,""),(5,""),(6,""),(7,""),(8,""),(9,""),(10,""),(11,""),(12,""),(13,""),(14,"")],  ["nuym",'session_idx'])
                                                                                                                                                                 ^

 INFO [2020-01-19 00:09:57,743] ({pool-2-thread-6} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:09:57,760] ({pool-2-thread-6} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:10:07,311] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:10:12,470] ({qtp2107447833-15} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:10:15,836] ({qtp2107447833-15} InterpreterSetting.java[getOrCreateInterpreterGroup]:419) - Create InterpreterGroup with groupId: python:shared_process for user: anonymous and note: 2EZEECRFJ
 INFO [2020-01-19 00:10:15,837] ({qtp2107447833-15} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.python.PythonInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-19 00:10:15,837] ({qtp2107447833-15} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.python.IPythonInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-19 00:10:15,838] ({qtp2107447833-15} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.python.PythonInterpreterPandasSql created for user: anonymous, sessionId: shared_session
 INFO [2020-01-19 00:10:15,838] ({qtp2107447833-15} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.python.PythonCondaInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-19 00:10:15,839] ({qtp2107447833-15} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.python.PythonDockerInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-19 00:10:15,839] ({qtp2107447833-15} ManagedInterpreterGroup.java[getOrCreateSession]:158) - Create Session: shared_session in InterpreterGroup: python:shared_process for user: anonymous
 INFO [2020-01-19 00:10:21,325] ({qtp2107447833-9} NotebookServer.java[broadcastNewParagraph]:688) - Broadcasting paragraph on run call instead of note.
 INFO [2020-01-19 00:10:21,334] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:10:21,351] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-python:shared_process-shared_session
 INFO [2020-01-19 00:10:21,352] ({pool-2-thread-4} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-000328_1025815385, interpreter: python, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:10:21,352] ({pool-2-thread-4} ManagedInterpreterGroup.java[getOrCreateInterpreterProcess]:61) - Create InterpreterProcess for InterpreterGroup: python:shared_process
 INFO [2020-01-19 00:10:21,352] ({pool-2-thread-4} ShellScriptLauncher.java[launch]:48) - Launching Interpreter: python
 INFO [2020-01-19 00:10:21,353] ({pool-2-thread-4} RemoteInterpreterManagedProcess.java[start]:115) - Thrift server for callback will start. Port: 36995
 INFO [2020-01-19 00:10:21,854] ({pool-2-thread-4} RemoteInterpreterManagedProcess.java[start]:190) - Run interpreter process [/zeppelin/bin/interpreter.sh, -d, /zeppelin/interpreter/python, -c, 172.18.0.4, -p, 36995, -r, :, -l, /zeppelin/local-repo/python, -g, python]
 INFO [2020-01-19 00:10:23,273] ({pool-9-thread-1} RemoteInterpreterManagedProcess.java[callback]:123) - RemoteInterpreterServer Registered: CallbackInfo(host:172.18.0.4, port:34985)
 INFO [2020-01-19 00:10:23,280] ({pool-2-thread-4} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.python.PythonInterpreter
 INFO [2020-01-19 00:10:23,394] ({pool-2-thread-4} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.python.IPythonInterpreter
 INFO [2020-01-19 00:10:23,409] ({pool-2-thread-4} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.python.PythonInterpreterPandasSql
 INFO [2020-01-19 00:10:23,411] ({pool-2-thread-4} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.python.PythonCondaInterpreter
 INFO [2020-01-19 00:10:23,414] ({pool-2-thread-4} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.python.PythonDockerInterpreter
 INFO [2020-01-19 00:10:23,417] ({pool-2-thread-4} RemoteInterpreter.java[call]:142) - Open RemoteInterpreter org.apache.zeppelin.python.PythonInterpreter
 INFO [2020-01-19 00:10:23,417] ({pool-2-thread-4} RemoteInterpreter.java[pushAngularObjectRegistryToRemote]:436) - Push local angular object registry from ZeppelinServer to remote interpreter group python:shared_process
 INFO [2020-01-19 00:10:26,587] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2314) - Job 20200119-000328_1025815385 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:10:26,603] ({pool-2-thread-4} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:10:26,615] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-python:shared_process-shared_session
 INFO [2020-01-19 00:10:41,441] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:10:41,459] ({pool-2-thread-12} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:10:41,459] ({pool-2-thread-12} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195328_2060007234, interpreter: , note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:10:41,473] ({pool-2-thread-12} NotebookServer.java[afterStatusChange]:2316) - Job 20200118-195328_2060007234 is finished, status: ERROR, exception: null, result: %text <console>:1: error: illegal start of simple expression
list = [(1,""),(2,""),(3,""),(4,""),(5,""),(6,""),(7,""),(8,""),(9,""),(10,""),(11,""),(12,""),(13,""),(14,"")]
       ^
<console>:2: error: illegal start of simple expression
dataFrame = spark.createDataFrame(list,  ["nuym",'session_idx'])
                                         ^
<console>:2: error: unclosed character literal
dataFrame = spark.createDataFrame(list,  ["nuym",'session_idx'])
                                                             ^

 INFO [2020-01-19 00:10:41,488] ({pool-2-thread-12} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:10:41,518] ({pool-2-thread-12} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:10:53,331] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:10:53,355] ({pool-2-thread-7} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:10:53,355] ({pool-2-thread-7} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195328_2060007234, interpreter: , note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:10:53,368] ({pool-2-thread-7} NotebookServer.java[afterStatusChange]:2316) - Job 20200118-195328_2060007234 is finished, status: ERROR, exception: null, result: %text <console>:1: error: illegal start of simple expression
list = [(1,""),(2,""),(3,""),(4,""),(5,""),(6,""),(7,""),(8,""),(9,""),(10,""),(11,""),(12,""),(13,""),(14,"")]
       ^
<console>:2: error: illegal start of simple expression
dataFrame = spark.createDataFrame(list,['nuym','session_idx'])
                                       ^
<console>:2: error: unclosed character literal
dataFrame = spark.createDataFrame(list,['nuym','session_idx'])
                                                           ^

 INFO [2020-01-19 00:10:53,392] ({pool-2-thread-7} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:10:53,404] ({pool-2-thread-7} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:10:58,015] ({qtp2107447833-15} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:10:58,037] ({pool-2-thread-13} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:10:58,038] ({pool-2-thread-13} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195328_2060007234, interpreter: , note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:10:58,049] ({pool-2-thread-13} NotebookServer.java[afterStatusChange]:2316) - Job 20200118-195328_2060007234 is finished, status: ERROR, exception: null, result: %text <console>:1: error: illegal start of simple expression
list = [(1,""),(2,""),(3,""),(4,""),(5,""),(6,""),(7,""),(8,""),(9,""),(10,""),(11,""),(12,""),(13,""),(14,"")]
       ^
<console>:2: error: illegal start of simple expression
dataFrame = spark.createDataFrame(list,['nuym','session_idx'])
                                       ^
<console>:2: error: unclosed character literal
dataFrame = spark.createDataFrame(list,['nuym','session_idx'])
                                                           ^

 INFO [2020-01-19 00:10:58,069] ({pool-2-thread-13} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:10:58,098] ({pool-2-thread-13} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:11:06,611] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:11:06,631] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:11:06,636] ({pool-2-thread-3} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195328_2060007234, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:11:06,782] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2314) - Job 20200118-195328_2060007234 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:11:06,802] ({pool-2-thread-3} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:11:06,820] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:11:22,162] ({qtp2107447833-15} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:11:25,793] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:11:25,806] ({pool-2-thread-26} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:11:25,807] ({pool-2-thread-26} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-000328_1025815385, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:11:35,507] ({pool-2-thread-26} NotebookServer.java[afterStatusChange]:2314) - Job 20200119-000328_1025815385 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:11:35,528] ({pool-2-thread-26} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:11:35,544] ({pool-2-thread-26} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:12:06,356] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:12:06,386] ({pool-2-thread-8} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:12:06,391] ({pool-2-thread-8} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-000328_1025815385, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:12:06,542] ({pool-2-thread-8} NotebookServer.java[afterStatusChange]:2316) - Job 20200119-000328_1025815385 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mAttributeError[0mTraceback (most recent call last)
[0;32m<ipython-input-34-6ad61e99682e>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdataFrame[0m[0;34m.[0m[0mselect[0m[0;34m([0m[0;34m"nuym"[0m[0;34m)[0m[0;34m.[0m[0msum[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py[0m in [0;36m__getattr__[0;34m(self, name)[0m
[1;32m   1298[0m         [0;32mif[0m [0mname[0m [0;32mnot[0m [0;32min[0m [0mself[0m[0;34m.[0m[0mcolumns[0m[0;34m:[0m[0;34m[0m[0m
[1;32m   1299[0m             raise AttributeError(
[0;32m-> 1300[0;31m                 "'%s' object has no attribute '%s'" % (self.__class__.__name__, name))
[0m[1;32m   1301[0m         [0mjc[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mapply[0m[0;34m([0m[0mname[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1302[0m         [0;32mreturn[0m [0mColumn[0m[0;34m([0m[0mjc[0m[0;34m)[0m[0;34m[0m[0m

[0;31mAttributeError[0m: 'DataFrame' object has no attribute 'sum'
 INFO [2020-01-19 00:12:06,558] ({pool-2-thread-8} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:12:06,582] ({pool-2-thread-8} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:12:12,488] ({qtp2107447833-15} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:12:12,519] ({pool-2-thread-15} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:12:12,519] ({pool-2-thread-15} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-000328_1025815385, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:12:12,609] ({pool-2-thread-15} NotebookServer.java[afterStatusChange]:2314) - Job 20200119-000328_1025815385 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:12:12,619] ({pool-2-thread-15} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:12:12,630] ({pool-2-thread-15} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:12:18,208] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:12:18,231] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:12:18,232] ({pool-2-thread-5} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-000328_1025815385, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:12:18,328] ({pool-2-thread-5} NotebookServer.java[afterStatusChange]:2314) - Job 20200119-000328_1025815385 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:12:18,338] ({pool-2-thread-5} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:12:18,354] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:12:20,411] ({qtp2107447833-15} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:12:20,428] ({pool-2-thread-16} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:12:20,428] ({pool-2-thread-16} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-000328_1025815385, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:12:21,717] ({pool-2-thread-16} NotebookServer.java[afterStatusChange]:2314) - Job 20200119-000328_1025815385 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:12:21,741] ({pool-2-thread-16} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:12:21,779] ({pool-2-thread-16} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:12:26,666] ({qtp2107447833-15} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:12:26,681] ({pool-2-thread-9} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:12:26,682] ({pool-2-thread-9} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-000328_1025815385, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:12:26,791] ({pool-2-thread-9} NotebookServer.java[afterStatusChange]:2316) - Job 20200119-000328_1025815385 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mAttributeError[0mTraceback (most recent call last)
[0;32m<ipython-input-42-6ad61e99682e>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdataFrame[0m[0;34m.[0m[0mselect[0m[0;34m([0m[0;34m"nuym"[0m[0;34m)[0m[0;34m.[0m[0msum[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py[0m in [0;36m__getattr__[0;34m(self, name)[0m
[1;32m   1298[0m         [0;32mif[0m [0mname[0m [0;32mnot[0m [0;32min[0m [0mself[0m[0;34m.[0m[0mcolumns[0m[0;34m:[0m[0;34m[0m[0m
[1;32m   1299[0m             raise AttributeError(
[0;32m-> 1300[0;31m                 "'%s' object has no attribute '%s'" % (self.__class__.__name__, name))
[0m[1;32m   1301[0m         [0mjc[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mapply[0m[0;34m([0m[0mname[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1302[0m         [0;32mreturn[0m [0mColumn[0m[0;34m([0m[0mjc[0m[0;34m)[0m[0;34m[0m[0m

[0;31mAttributeError[0m: 'DataFrame' object has no attribute 'sum'
 INFO [2020-01-19 00:12:26,800] ({pool-2-thread-9} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:12:26,810] ({pool-2-thread-9} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:13:09,402] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:13:09,431] ({pool-2-thread-17} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:13:09,431] ({pool-2-thread-17} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-000328_1025815385, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:13:09,521] ({pool-2-thread-17} NotebookServer.java[afterStatusChange]:2316) - Job 20200119-000328_1025815385 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mNameError[0mTraceback (most recent call last)
[0;32m<ipython-input-44-952567fe42ce>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdataFrame[0m[0;34m.[0m[0mselect[0m[0;34m([0m[0;34m"nuym"[0m[0;34m)[0m[0;34m.[0m[0magg[0m[0;34m([0m[0msum[0m[0;34m([0m[0mf[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;31mNameError[0m: name 'f' is not defined
 INFO [2020-01-19 00:13:09,532] ({pool-2-thread-17} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:13:09,544] ({pool-2-thread-17} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:13:29,155] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:13:29,178] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:13:29,180] ({pool-2-thread-2} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-000328_1025815385, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:13:29,263] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2316) - Job 20200119-000328_1025815385 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mTypeError[0mTraceback (most recent call last)
[0;32m<ipython-input-46-89140275f329>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdataFrame[0m[0;34m.[0m[0mselect[0m[0;34m([0m[0;34m"nuym"[0m[0;34m)[0m[0;34m.[0m[0magg[0m[0;34m([0m[0msum[0m[0;34m([0m[0;34m"nuym"[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;31mTypeError[0m: unsupported operand type(s) for +: 'int' and 'str'
 INFO [2020-01-19 00:13:29,277] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:13:29,293] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:13:37,863] ({qtp2107447833-15} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:13:37,876] ({pool-2-thread-18} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:13:37,878] ({pool-2-thread-18} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-000328_1025815385, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:13:37,971] ({pool-2-thread-18} NotebookServer.java[afterStatusChange]:2316) - Job 20200119-000328_1025815385 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mTypeError[0mTraceback (most recent call last)
[0;32m<ipython-input-48-89140275f329>[0m in [0;36m<module>[0;34m()[0m
[0;32m----> 1[0;31m [0mdataFrame[0m[0;34m.[0m[0mselect[0m[0;34m([0m[0;34m"nuym"[0m[0;34m)[0m[0;34m.[0m[0magg[0m[0;34m([0m[0msum[0m[0;34m([0m[0;34m"nuym"[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;31mTypeError[0m: unsupported operand type(s) for +: 'int' and 'str'
 INFO [2020-01-19 00:13:37,984] ({pool-2-thread-18} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:13:37,997] ({pool-2-thread-18} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:13:58,459] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:15:04,195] ({qtp2107447833-15} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:15:34,288] ({qtp2107447833-9} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:15:34,305] ({pool-2-thread-10} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:15:34,306] ({pool-2-thread-10} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-000328_1025815385, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:15:35,422] ({pool-2-thread-10} NotebookServer.java[afterStatusChange]:2314) - Job 20200119-000328_1025815385 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:15:35,437] ({pool-2-thread-10} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:15:35,458] ({pool-2-thread-10} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:15:48,818] ({qtp2107447833-57} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:15:48,832] ({pool-2-thread-19} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:15:48,839] ({pool-2-thread-19} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-000328_1025815385, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:15:49,559] ({pool-2-thread-19} NotebookServer.java[afterStatusChange]:2314) - Job 20200119-000328_1025815385 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:15:49,576] ({pool-2-thread-19} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:15:49,594] ({pool-2-thread-19} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:18:58,431] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:18:58,444] ({pool-2-thread-6} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:18:58,445] ({pool-2-thread-6} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-000328_1025815385, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:18:59,319] ({pool-2-thread-6} NotebookServer.java[afterStatusChange]:2314) - Job 20200119-000328_1025815385 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:18:59,339] ({pool-2-thread-6} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:18:59,366] ({pool-2-thread-6} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:20:22,531] ({Thread-34} ZeppelinServer.java[run]:253) - Shutting down Zeppelin Server ... 
 INFO [2020-01-19 00:20:22,556] ({qtp2107447833-11} NotebookServer.java[onClose]:372) - Closed connection to 192.168.99.1 : 9695. (1000) null
 INFO [2020-01-19 00:20:22,558] ({Thread-34} AbstractConnector.java[doStop]:341) - Stopped ServerConnector@436adac3{HTTP/1.1,[http/1.1]}{0.0.0.0:8090}
 INFO [2020-01-19 00:20:22,559] ({Thread-34} HouseKeeper.java[stopScavenging]:167) - node0 Stopped scavenging
 INFO [2020-01-19 00:20:24,229] ({Thread-34} ContextHandler.java[doStop]:1045) - Stopped o.e.j.w.WebAppContext@5af97850{zeppelin-web,/,null,UNAVAILABLE}{/zeppelin/zeppelin-web-0.8.1.war}
 INFO [2020-01-19 00:20:24,232] ({Thread-77} InterpreterSetting.java[close]:483) - Close InterpreterSetting: ignite
 INFO [2020-01-19 00:20:24,233] ({Thread-82} InterpreterSetting.java[close]:483) - Close InterpreterSetting: lens
 INFO [2020-01-19 00:20:24,232] ({Thread-80} InterpreterSetting.java[close]:483) - Close InterpreterSetting: jdbc
 INFO [2020-01-19 00:20:24,235] ({Thread-93} InterpreterSetting.java[close]:483) - Close InterpreterSetting: spark
 INFO [2020-01-19 00:20:24,236] ({Thread-93} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: spark:shared_process
 INFO [2020-01-19 00:20:24,236] ({Thread-93} ManagedInterpreterGroup.java[close]:100) - Close Session: shared_session for interpreter setting: spark
 INFO [2020-01-19 00:20:24,232] ({Thread-78} InterpreterSetting.java[close]:483) - Close InterpreterSetting: python
 INFO [2020-01-19 00:20:24,236] ({Thread-78} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: python:shared_process
 INFO [2020-01-19 00:20:24,236] ({Thread-78} ManagedInterpreterGroup.java[close]:100) - Close Session: shared_session for interpreter setting: python
 INFO [2020-01-19 00:20:24,232] ({Thread-79} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sap
 INFO [2020-01-19 00:20:24,236] ({Thread-96} InterpreterSetting.java[close]:483) - Close InterpreterSetting: bigquery
 INFO [2020-01-19 00:20:24,236] ({Thread-94} InterpreterSetting.java[close]:483) - Close InterpreterSetting: md
 INFO [2020-01-19 00:20:24,235] ({Thread-92} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sh
 INFO [2020-01-19 00:20:24,233] ({Thread-88} InterpreterSetting.java[close]:483) - Close InterpreterSetting: file
 INFO [2020-01-19 00:20:24,233] ({Thread-86} InterpreterSetting.java[close]:483) - Close InterpreterSetting: livy
 INFO [2020-01-19 00:20:24,233] ({Thread-87} InterpreterSetting.java[close]:483) - Close InterpreterSetting: neo4j
 INFO [2020-01-19 00:20:24,233] ({Thread-85} InterpreterSetting.java[close]:483) - Close InterpreterSetting: angular
 INFO [2020-01-19 00:20:24,233] ({Thread-84} InterpreterSetting.java[close]:483) - Close InterpreterSetting: flink
 INFO [2020-01-19 00:20:24,233] ({Thread-83} InterpreterSetting.java[close]:483) - Close InterpreterSetting: pig
 INFO [2020-01-19 00:20:24,241] ({Thread-95} InterpreterSetting.java[close]:483) - Close InterpreterSetting: alluxio
 INFO [2020-01-19 00:20:24,249] ({Thread-89} InterpreterSetting.java[close]:483) - Close InterpreterSetting: groovy
 INFO [2020-01-19 00:20:24,250] ({Thread-91} InterpreterSetting.java[close]:483) - Close InterpreterSetting: cassandra
 INFO [2020-01-19 00:20:24,250] ({Thread-90} InterpreterSetting.java[close]:483) - Close InterpreterSetting: elasticsearch
 INFO [2020-01-19 00:20:24,252] ({Thread-97} InterpreterSetting.java[close]:483) - Close InterpreterSetting: hbase
 INFO [2020-01-19 00:20:24,256] ({Thread-98} InterpreterSetting.java[close]:483) - Close InterpreterSetting: kylin
 INFO [2020-01-19 00:20:24,257] ({Thread-81} InterpreterSetting.java[close]:483) - Close InterpreterSetting: ignite
 INFO [2020-01-19 00:20:24,257] ({Thread-99} InterpreterSetting.java[close]:483) - Close InterpreterSetting: python
 INFO [2020-01-19 00:20:24,258] ({Thread-102} InterpreterSetting.java[close]:483) - Close InterpreterSetting: lens
 INFO [2020-01-19 00:20:24,257] ({Thread-101} InterpreterSetting.java[close]:483) - Close InterpreterSetting: jdbc
 INFO [2020-01-19 00:20:24,257] ({Thread-100} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sap
 INFO [2020-01-19 00:20:24,259] ({Thread-105} InterpreterSetting.java[close]:483) - Close InterpreterSetting: angular
 INFO [2020-01-19 00:20:24,258] ({Thread-104} InterpreterSetting.java[close]:483) - Close InterpreterSetting: flink
 INFO [2020-01-19 00:20:24,258] ({Thread-103} InterpreterSetting.java[close]:483) - Close InterpreterSetting: pig
 INFO [2020-01-19 00:20:24,259] ({Thread-106} InterpreterSetting.java[close]:483) - Close InterpreterSetting: livy
 INFO [2020-01-19 00:20:24,262] ({Thread-107} InterpreterSetting.java[close]:483) - Close InterpreterSetting: neo4j
 INFO [2020-01-19 00:20:24,263] ({Thread-108} InterpreterSetting.java[close]:483) - Close InterpreterSetting: file
 INFO [2020-01-19 00:20:24,270] ({Thread-109} InterpreterSetting.java[close]:483) - Close InterpreterSetting: groovy
 INFO [2020-01-19 00:20:24,270] ({Thread-110} InterpreterSetting.java[close]:483) - Close InterpreterSetting: elasticsearch
 INFO [2020-01-19 00:20:24,271] ({Thread-111} InterpreterSetting.java[close]:483) - Close InterpreterSetting: cassandra
 INFO [2020-01-19 00:20:24,276] ({Thread-112} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sh
 INFO [2020-01-19 00:20:24,277] ({Thread-113} InterpreterSetting.java[close]:483) - Close InterpreterSetting: spark
 INFO [2020-01-19 00:20:24,277] ({Thread-114} InterpreterSetting.java[close]:483) - Close InterpreterSetting: md
 INFO [2020-01-19 00:20:24,278] ({Thread-117} InterpreterSetting.java[close]:483) - Close InterpreterSetting: hbase
 INFO [2020-01-19 00:20:24,277] ({Thread-116} InterpreterSetting.java[close]:483) - Close InterpreterSetting: bigquery
 INFO [2020-01-19 00:20:24,277] ({Thread-115} InterpreterSetting.java[close]:483) - Close InterpreterSetting: alluxio
 INFO [2020-01-19 00:20:24,290] ({Thread-118} InterpreterSetting.java[close]:483) - Close InterpreterSetting: kylin
 WARN [2020-01-19 00:20:24,292] ({Thread-78} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.python.IPythonInterpreter
 WARN [2020-01-19 00:20:24,332] ({Thread-78} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.python.PythonInterpreterPandasSql
 WARN [2020-01-19 00:20:24,336] ({Thread-78} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.python.PythonCondaInterpreter
 WARN [2020-01-19 00:20:24,340] ({Thread-78} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.python.PythonDockerInterpreter
 INFO [2020-01-19 00:20:24,356] ({Thread-78} ManagedInterpreterGroup.java[close]:105) - Remove this InterpreterGroup: python:shared_process as all the sessions are closed
 INFO [2020-01-19 00:20:24,359] ({Thread-78} ManagedInterpreterGroup.java[close]:108) - Kill RemoteInterpreterProcess
 INFO [2020-01-19 00:20:24,359] ({Thread-78} RemoteInterpreterManagedProcess.java[stop]:220) - Kill interpreter process
 WARN [2020-01-19 00:20:24,454] ({Thread-93} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.SparkSqlInterpreter
 WARN [2020-01-19 00:20:24,458] ({Thread-93} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.DepInterpreter
 WARN [2020-01-19 00:20:24,476] ({Thread-93} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.IPySparkInterpreter
 WARN [2020-01-19 00:20:24,477] ({Thread-93} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.SparkRInterpreter
 INFO [2020-01-19 00:20:24,477] ({Thread-93} ManagedInterpreterGroup.java[close]:105) - Remove this InterpreterGroup: spark:shared_process as all the sessions are closed
 INFO [2020-01-19 00:20:24,477] ({Thread-93} ManagedInterpreterGroup.java[close]:108) - Kill RemoteInterpreterProcess
 INFO [2020-01-19 00:20:24,478] ({Thread-93} RemoteInterpreterManagedProcess.java[stop]:220) - Kill interpreter process
ERROR [2020-01-19 00:20:25,132] ({Thread-58} RemoteInterpreterEventPoller.java[run]:257) - Can not get RemoteInterpreterEvent because it is shutdown.
ERROR [2020-01-19 00:20:25,132] ({pool-8-thread-1} AppendOutputRunner.java[run]:68) - Wait for OutputBuffer queue interrupted: null
ERROR [2020-01-19 00:20:25,379] ({Thread-37} RemoteInterpreterEventPoller.java[run]:257) - Can not get RemoteInterpreterEvent because it is shutdown.
ERROR [2020-01-19 00:20:25,380] ({pool-6-thread-1} AppendOutputRunner.java[run]:68) - Wait for OutputBuffer queue interrupted: null
 WARN [2020-01-19 00:20:26,857] ({Thread-78} RemoteInterpreterManagedProcess.java[stop]:230) - ignore the exception when shutting down
 INFO [2020-01-19 00:20:26,859] ({Thread-78} RemoteInterpreterManagedProcess.java[stop]:238) - Remote process terminated
 INFO [2020-01-19 00:20:26,859] ({Thread-99} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: python:shared_process
 INFO [2020-01-19 00:20:26,859] ({Exec Default Executor} RemoteInterpreterManagedProcess.java[onProcessComplete]:243) - Interpreter process exited 0
 WARN [2020-01-19 00:20:26,985] ({Thread-93} RemoteInterpreterManagedProcess.java[stop]:230) - ignore the exception when shutting down
 INFO [2020-01-19 00:20:26,986] ({Thread-93} RemoteInterpreterManagedProcess.java[stop]:238) - Remote process terminated
 INFO [2020-01-19 00:20:26,986] ({Thread-34} NotebookRepoSync.java[close]:428) - Closing all notebook storages
 INFO [2020-01-19 00:20:26,986] ({Thread-113} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: spark:shared_process
 INFO [2020-01-19 00:20:26,993] ({Exec Default Executor} RemoteInterpreterManagedProcess.java[onProcessFailed]:250) - Interpreter process failed {}
org.apache.commons.exec.ExecuteException: Process exited with an error: 143 (Exit value: 143)
	at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:404)
	at org.apache.commons.exec.DefaultExecutor.access$200(DefaultExecutor.java:48)
	at org.apache.commons.exec.DefaultExecutor$1.run(DefaultExecutor.java:200)
	at java.lang.Thread.run(Thread.java:748)
 INFO [2020-01-19 00:20:29,992] ({Thread-34} ZeppelinServer.java[run]:264) - Bye
 WARN [2020-01-19 00:20:48,414] ({main} ZeppelinConfiguration.java[create]:117) - Failed to load configuration, proceeding with a default
 INFO [2020-01-19 00:20:48,600] ({main} ZeppelinConfiguration.java[create]:129) - Server Host: 0.0.0.0
 INFO [2020-01-19 00:20:48,600] ({main} ZeppelinConfiguration.java[create]:131) - Server Port: 8090
 INFO [2020-01-19 00:20:48,601] ({main} ZeppelinConfiguration.java[create]:135) - Context Path: /
 INFO [2020-01-19 00:20:48,606] ({main} ZeppelinConfiguration.java[create]:136) - Zeppelin Version: 0.8.1
 INFO [2020-01-19 00:20:48,647] ({main} Log.java[initialized]:193) - Logging initialized @847ms to org.eclipse.jetty.util.log.Slf4jLog
 WARN [2020-01-19 00:20:48,993] ({main} ServerConnector.java[setSoLingerTime]:458) - Ignoring deprecated socket close linger time
 INFO [2020-01-19 00:20:49,232] ({main} ZeppelinServer.java[setupWebAppContext]:413) - ZeppelinServer Webapp path: /zeppelin/webapps
 INFO [2020-01-19 00:20:49,616] ({main} ZeppelinServer.java[main]:239) - Starting zeppelin server
 INFO [2020-01-19 00:20:49,624] ({main} Server.java[doStart]:370) - jetty-9.4.14.v20181114; built: 2018-11-14T21:20:31.478Z; git: c4550056e785fb5665914545889f21dc136ad9e6; jvm 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12
 INFO [2020-01-19 00:20:58,671] ({main} StandardDescriptorProcessor.java[visitServlet]:283) - NO JSP Support for /, did not find org.eclipse.jetty.jsp.JettyJspServlet
 INFO [2020-01-19 00:20:58,775] ({main} DefaultSessionIdManager.java[doStart]:365) - DefaultSessionIdManager workerName=node0
 INFO [2020-01-19 00:20:58,780] ({main} DefaultSessionIdManager.java[doStart]:370) - No SessionScavenger set, using defaults
 INFO [2020-01-19 00:20:58,784] ({main} HouseKeeper.java[startScavenging]:149) - node0 Scavenging every 660000ms
 INFO [2020-01-19 00:21:00,405] ({main} SchedulerFactory.java[<init>]:59) - Scheduler Thread Pool Size: 100
 WARN [2020-01-19 00:21:00,468] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir
 WARN [2020-01-19 00:21:00,485] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir
 INFO [2020-01-19 00:21:00,779] ({main} InterpreterSettingManager.java[<init>]:165) - Using RecoveryStorage: org.apache.zeppelin.interpreter.recovery.NullRecoveryStorage
 INFO [2020-01-19 00:21:00,784] ({main} InterpreterSettingManager.java[<init>]:169) - Using LifecycleManager: org.apache.zeppelin.interpreter.lifecycle.NullLifecycleManager
 INFO [2020-01-19 00:21:00,923] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: cassandra
 INFO [2020-01-19 00:21:00,945] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: md
 INFO [2020-01-19 00:21:00,964] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: sap
 INFO [2020-01-19 00:21:00,990] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: sh
 INFO [2020-01-19 00:21:00,998] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: angular
 WARN [2020-01-19 00:21:01,013] ({main} InterpreterSettingManager.java[init]:331) - No interpreter-setting.json found in /zeppelin/interpreter/lib
 INFO [2020-01-19 00:21:01,027] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: hbase
 INFO [2020-01-19 00:21:01,038] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: elasticsearch
 WARN [2020-01-19 00:21:01,289] ({main} InterpreterSettingManager.java[init]:331) - No interpreter-setting.json found in /zeppelin/interpreter/scio
 INFO [2020-01-19 00:21:01,303] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: alluxio
 INFO [2020-01-19 00:21:01,309] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: jdbc
 INFO [2020-01-19 00:21:01,323] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: neo4j
 INFO [2020-01-19 00:21:01,327] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: python
 INFO [2020-01-19 00:21:01,339] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: pig
 INFO [2020-01-19 00:21:01,345] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: spark
 INFO [2020-01-19 00:21:01,350] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: lens
 INFO [2020-01-19 00:21:01,358] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: bigquery
 INFO [2020-01-19 00:21:01,362] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: file
 INFO [2020-01-19 00:21:01,366] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: ignite
 INFO [2020-01-19 00:21:01,372] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: kylin
 INFO [2020-01-19 00:21:01,379] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: livy
 INFO [2020-01-19 00:21:01,381] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: groovy
 INFO [2020-01-19 00:21:01,383] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: flink
 WARN [2020-01-19 00:21:01,401] ({main} InterpreterSettingManager.java[init]:331) - No interpreter-setting.json found in /zeppelin/interpreter/${interpreter.name}
 INFO [2020-01-19 00:21:01,406] ({main} LocalConfigStorage.java[loadInterpreterSettings]:63) - Load Interpreter Setting from file: /zeppelin/conf/interpreter.json
 INFO [2020-01-19 00:21:01,643] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting ignite from interpreter.json
 INFO [2020-01-19 00:21:01,646] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting python from interpreter.json
 INFO [2020-01-19 00:21:01,647] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting sap from interpreter.json
 INFO [2020-01-19 00:21:01,652] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting jdbc from interpreter.json
 INFO [2020-01-19 00:21:01,654] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting lens from interpreter.json
 INFO [2020-01-19 00:21:01,657] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting pig from interpreter.json
 INFO [2020-01-19 00:21:01,659] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting flink from interpreter.json
 INFO [2020-01-19 00:21:01,661] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting angular from interpreter.json
 INFO [2020-01-19 00:21:01,663] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting livy from interpreter.json
 INFO [2020-01-19 00:21:01,664] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting neo4j from interpreter.json
 INFO [2020-01-19 00:21:01,668] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting file from interpreter.json
 INFO [2020-01-19 00:21:01,670] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting groovy from interpreter.json
 INFO [2020-01-19 00:21:01,671] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting elasticsearch from interpreter.json
 INFO [2020-01-19 00:21:01,679] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting cassandra from interpreter.json
 INFO [2020-01-19 00:21:01,680] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting sh from interpreter.json
 INFO [2020-01-19 00:21:01,685] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting spark from interpreter.json
 INFO [2020-01-19 00:21:01,687] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting md from interpreter.json
 INFO [2020-01-19 00:21:01,688] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting alluxio from interpreter.json
 INFO [2020-01-19 00:21:01,690] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting bigquery from interpreter.json
 INFO [2020-01-19 00:21:01,692] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting hbase from interpreter.json
 INFO [2020-01-19 00:21:01,693] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting kylin from interpreter.json
 INFO [2020-01-19 00:21:01,790] ({main} LocalConfigStorage.java[save]:53) - Save Interpreter Setting to /zeppelin/conf/interpreter.json
 INFO [2020-01-19 00:21:02,154] ({main} VfsLog.java[info]:138) - Using "/tmp/vfs_cache" as temporary files store.
 INFO [2020-01-19 00:21:02,261] ({main} GitNotebookRepo.java[<init>]:64) - Opening a git repo at '/zeppelin/notebook'
 INFO [2020-01-19 00:21:02,540] ({main} NotebookRepoSync.java[<init>]:77) - Instantiate NotebookRepo: org.apache.zeppelin.notebook.repo.GitNotebookRepo
 WARN [2020-01-19 00:21:02,835] ({main} NotebookAuthorization.java[getInstance]:86) - Notebook authorization module was called without initialization, initializing with default configuration
 WARN [2020-01-19 00:21:02,836] ({main} LocalConfigStorage.java[loadNotebookAuthorization]:77) - NotebookAuthorization file /zeppelin/conf/notebook-authorization.json is not existed
 INFO [2020-01-19 00:21:02,838] ({main} Credentials.java[loadFromFile]:121) - /zeppelin/conf/credentials.json
 INFO [2020-01-19 00:21:02,914] ({main} StdSchedulerFactory.java[instantiate]:1184) - Using default implementation for ThreadExecutor
 INFO [2020-01-19 00:21:02,921] ({main} SimpleThreadPool.java[initialize]:268) - Job execution threads will use class loader of thread: main
 INFO [2020-01-19 00:21:02,959] ({main} SchedulerSignalerImpl.java[<init>]:61) - Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
 INFO [2020-01-19 00:21:02,969] ({main} QuartzScheduler.java[<init>]:240) - Quartz Scheduler v.2.2.1 created.
 INFO [2020-01-19 00:21:02,970] ({main} RAMJobStore.java[initialize]:155) - RAMJobStore initialized.
 INFO [2020-01-19 00:21:02,973] ({main} QuartzScheduler.java[initialize]:305) - Scheduler meta-data: Quartz Scheduler (v2.2.1) 'DefaultQuartzScheduler' with instanceId 'NON_CLUSTERED'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'org.quartz.simpl.SimpleThreadPool' - with 10 threads.
  Using job-store 'org.quartz.simpl.RAMJobStore' - which does not support persistence. and is not clustered.

 INFO [2020-01-19 00:21:02,974] ({main} StdSchedulerFactory.java[instantiate]:1339) - Quartz scheduler 'DefaultQuartzScheduler' initialized from default resource file in Quartz package: 'quartz.properties'
 INFO [2020-01-19 00:21:02,975] ({main} StdSchedulerFactory.java[instantiate]:1343) - Quartz scheduler version: 2.2.1
 INFO [2020-01-19 00:21:02,977] ({main} QuartzScheduler.java[start]:575) - Scheduler DefaultQuartzScheduler_$_NON_CLUSTERED started.
 INFO [2020-01-19 00:21:03,142] ({main} FolderView.java[createFolder]:107) - Create folder /
 INFO [2020-01-19 00:21:03,148] ({main} Folder.java[setParent]:169) - Set parent of / to /
 INFO [2020-01-19 00:21:03,149] ({main} Folder.java[addNote]:185) - Add note 2EZEECRFJ to folder /
 WARN [2020-01-19 00:21:03,154] ({main} Notebook.java[refreshCron]:981) - execution of the cron job is skipped cron is not enabled from Zeppelin server
 INFO [2020-01-19 00:21:03,155] ({main} Notebook.java[<init>]:127) - Notebook indexing started...
 INFO [2020-01-19 00:21:03,318] ({main} LuceneSearch.java[addIndexDocs]:305) - Indexing 1 notebooks took 161ms
 INFO [2020-01-19 00:21:03,318] ({main} Notebook.java[<init>]:129) - Notebook indexing finished: 1 indexed in 0s
 INFO [2020-01-19 00:21:03,323] ({main} Helium.java[loadConf]:103) - Add helium local registry /zeppelin/helium
 INFO [2020-01-19 00:21:03,324] ({main} Helium.java[loadConf]:100) - Add helium online registry https://s3.amazonaws.com/helium-package/helium.json
 WARN [2020-01-19 00:21:03,326] ({main} Helium.java[loadConf]:111) - /zeppelin/conf/helium.json does not exists
 INFO [2020-01-19 00:21:06,720] ({main} ContextHandler.java[doStart]:855) - Started o.e.j.w.WebAppContext@5af97850{zeppelin-web,/,file:///zeppelin/webapps/webapp/,AVAILABLE}{/zeppelin/zeppelin-web-0.8.1.war}
 INFO [2020-01-19 00:21:06,750] ({main} AbstractConnector.java[doStart]:292) - Started ServerConnector@15400fff{HTTP/1.1,[http/1.1]}{0.0.0.0:8090}
 INFO [2020-01-19 00:21:06,751] ({main} Server.java[doStart]:407) - Started @18954ms
 INFO [2020-01-19 00:21:06,751] ({main} ZeppelinServer.java[main]:249) - Done, zeppelin server started
 WARN [2020-01-19 00:21:08,079] ({qtp2107447833-9} SecurityRestApi.java[ticket]:88) - {"status":"OK","message":"","body":{"principal":"anonymous","ticket":"anonymous","roles":"[]"}}
 INFO [2020-01-19 00:21:08,321] ({qtp2107447833-14} NotebookServer.java[onOpen]:151) - New connection from 192.168.99.1 : 9813
 INFO [2020-01-19 00:21:08,383] ({qtp2107447833-15} NotebookServer.java[sendNote]:828) - New operation from 192.168.99.1 : 9813 : anonymous : GET_NOTE : 2EZEECRFJ
 INFO [2020-01-19 00:21:08,481] ({qtp2107447833-12} InterpreterSetting.java[getOrCreateInterpreterGroup]:419) - Create InterpreterGroup with groupId: spark:shared_process for user: anonymous and note: 2EZEECRFJ
 INFO [2020-01-19 00:21:08,484] ({qtp2107447833-12} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.SparkInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-19 00:21:08,484] ({qtp2107447833-12} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.SparkSqlInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-19 00:21:08,484] ({qtp2107447833-12} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.DepInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-19 00:21:08,485] ({qtp2107447833-12} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.PySparkInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-19 00:21:08,485] ({qtp2107447833-12} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.IPySparkInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-19 00:21:08,486] ({qtp2107447833-12} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.SparkRInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-19 00:21:08,487] ({qtp2107447833-12} ManagedInterpreterGroup.java[getOrCreateSession]:158) - Create Session: shared_session in InterpreterGroup: spark:shared_process for user: anonymous
 WARN [2020-01-19 00:21:08,515] ({qtp2107447833-15} GitNotebookRepo.java[revisionHistory]:158) - No Head found for 2EZEECRFJ, No HEAD exists and no explicit starting revision was specified
 WARN [2020-01-19 00:21:08,518] ({qtp2107447833-15} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-19 00:21:08,518] ({qtp2107447833-15} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-19 00:21:08,521] ({qtp2107447833-15} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-19 00:21:08,522] ({qtp2107447833-15} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-19 00:21:08,522] ({qtp2107447833-15} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-19 00:21:08,522] ({qtp2107447833-15} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 INFO [2020-01-19 00:21:08,688] ({qtp2107447833-12} InterpreterSetting.java[getOrCreateInterpreterGroup]:419) - Create InterpreterGroup with groupId: python:shared_process for user: anonymous and note: 2EZEECRFJ
 INFO [2020-01-19 00:21:08,688] ({qtp2107447833-12} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.python.PythonInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-19 00:21:08,688] ({qtp2107447833-12} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.python.IPythonInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-19 00:21:08,688] ({qtp2107447833-12} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.python.PythonInterpreterPandasSql created for user: anonymous, sessionId: shared_session
 INFO [2020-01-19 00:21:08,689] ({qtp2107447833-12} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.python.PythonCondaInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-19 00:21:08,689] ({qtp2107447833-12} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.python.PythonDockerInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-01-19 00:21:08,689] ({qtp2107447833-12} ManagedInterpreterGroup.java[getOrCreateSession]:158) - Create Session: shared_session in InterpreterGroup: python:shared_process for user: anonymous
 INFO [2020-01-19 00:21:12,834] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:21:35,177] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:22:15,952] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:22:21,429] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:23:24,745] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:23:25,191] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:23:25,211] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195314_1536993808 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:23:25,213] ({pool-2-thread-2} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195314_1536993808, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:23:25,213] ({pool-2-thread-2} ManagedInterpreterGroup.java[getOrCreateInterpreterProcess]:61) - Create InterpreterProcess for InterpreterGroup: spark:shared_process
 INFO [2020-01-19 00:23:25,214] ({pool-2-thread-2} ShellScriptLauncher.java[launch]:48) - Launching Interpreter: spark
 INFO [2020-01-19 00:23:25,222] ({pool-2-thread-2} SparkInterpreterLauncher.java[buildEnvFromProperties]:108) - Run Spark under non-secure mode as no keytab and principal is specified
 INFO [2020-01-19 00:23:25,226] ({pool-2-thread-2} RemoteInterpreterManagedProcess.java[start]:115) - Thrift server for callback will start. Port: 36335
 INFO [2020-01-19 00:23:25,235] ({pool-2-thread-2} RemoteInterpreterManagedProcess.java[start]:190) - Run interpreter process [/zeppelin/bin/interpreter.sh, -d, /zeppelin/interpreter/spark, -c, 172.18.0.2, -p, 36335, -r, :, -l, /zeppelin/local-repo/spark, -g, spark]
 INFO [2020-01-19 00:23:27,771] ({pool-7-thread-1} RemoteInterpreterManagedProcess.java[callback]:123) - RemoteInterpreterServer Registered: CallbackInfo(host:172.18.0.2, port:45335)
 INFO [2020-01-19 00:23:27,820] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkInterpreter
 INFO [2020-01-19 00:23:27,912] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkSqlInterpreter
 INFO [2020-01-19 00:23:27,918] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.DepInterpreter
 INFO [2020-01-19 00:23:27,923] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.PySparkInterpreter
 INFO [2020-01-19 00:23:27,927] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.IPySparkInterpreter
 INFO [2020-01-19 00:23:27,935] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkRInterpreter
 INFO [2020-01-19 00:23:27,938] ({pool-2-thread-2} RemoteInterpreter.java[call]:142) - Open RemoteInterpreter org.apache.zeppelin.spark.PySparkInterpreter
 INFO [2020-01-19 00:23:27,942] ({pool-2-thread-2} RemoteInterpreter.java[pushAngularObjectRegistryToRemote]:436) - Push local angular object registry from ZeppelinServer to remote interpreter group spark:shared_process
 INFO [2020-01-19 00:23:50,914] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2314) - Job 20200118-195314_1536993808 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:23:50,964] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:23:50,975] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195314_1536993808 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:23:54,324] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:23:55,128] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:23:55,150] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:23:55,151] ({pool-2-thread-2} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195328_2060007234, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:23:56,629] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2314) - Job 20200118-195328_2060007234 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:23:56,654] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:23:56,670] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:24:00,666] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:24:00,679] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:24:00,680] ({pool-2-thread-3} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-000328_1025815385, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:24:08,818] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2314) - Job 20200119-000328_1025815385 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:24:08,860] ({pool-2-thread-3} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:24:08,901] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:24:21,187] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:24:21,204] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:24:21,209] ({pool-2-thread-5} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-000328_1025815385, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:24:21,924] ({pool-2-thread-5} NotebookServer.java[afterStatusChange]:2314) - Job 20200119-000328_1025815385 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:24:21,945] ({pool-2-thread-5} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:24:21,975] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:24:29,539] ({qtp2107447833-13} NotebookServer.java[broadcastNewParagraph]:688) - Broadcasting paragraph on run call instead of note.
 INFO [2020-01-19 00:24:29,555] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:24:29,577] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200119-001021_531760765 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:24:29,578] ({pool-2-thread-4} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-001021_531760765, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:24:29,641] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2316) - Job 20200119-001021_531760765 is finished, status: ERROR, exception: null, result: %text [0;36m  File [0;32m"<ipython-input-12-447b8e9ace42>"[0;36m, line [0;32m3[0m
[0;31m    for row in partition: yield suma += row[0m
[0m                                      ^[0m
[0;31mSyntaxError[0m[0;31m:[0m invalid syntax

 INFO [2020-01-19 00:24:29,666] ({pool-2-thread-4} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:24:29,689] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200119-001021_531760765 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:24:39,481] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:24:39,496] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20200119-001021_531760765 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:24:39,500] ({pool-2-thread-3} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-001021_531760765, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:24:39,556] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2316) - Job 20200119-001021_531760765 is finished, status: ERROR, exception: null, result: %text [0;36m  File [0;32m"<ipython-input-14-167fc8eb58a9>"[0;36m, line [0;32m3[0m
[0;31m    suma = for row in partition: yield suma + row[0m
[0m             ^[0m
[0;31mSyntaxError[0m[0;31m:[0m invalid syntax

 INFO [2020-01-19 00:24:39,580] ({pool-2-thread-3} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:24:39,596] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20200119-001021_531760765 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:24:45,472] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:24:45,487] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200119-001021_531760765 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:24:45,488] ({pool-2-thread-2} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-001021_531760765, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:24:46,597] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2316) - Job 20200119-001021_531760765 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mPy4JJavaError[0mTraceback (most recent call last)
[0;32m<ipython-input-16-d877e8200fea>[0m in [0;36m<module>[0;34m()[0m
[1;32m      3[0m     [0;32mfor[0m [0mrow[0m [0;32min[0m [0mpartition[0m[0;34m:[0m [0;32myield[0m [0msuma[0m [0;34m+[0m [0mrow[0m[0;34m[0m[0m
[1;32m      4[0m [0;34m[0m[0m
[0;32m----> 5[0;31m [0mdataFrame[0m[0;34m.[0m[0mrdd[0m[0;34m.[0m[0mmapPartitions[0m[0;34m([0m[0msumar[0m[0;34m)[0m[0;34m.[0m[0mtoDF[0m[0;34m([0m[0;34m[[0m[0;34m"result"[0m[0;34m][0m[0;34m)[0m[0;34m.[0m[0mshow[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36mtoDF[0;34m(self, schema, sampleRatio)[0m
[1;32m     56[0m         [0;34m[[0m[0mRow[0m[0;34m([0m[0mname[0m[0;34m=[0m[0;34mu'Alice'[0m[0;34m,[0m [0mage[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m[0;34m][0m[0;34m[0m[0m
[1;32m     57[0m         """
[0;32m---> 58[0;31m         [0;32mreturn[0m [0msparkSession[0m[0;34m.[0m[0mcreateDataFrame[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mschema[0m[0;34m,[0m [0msampleRatio[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     59[0m [0;34m[0m[0m
[1;32m     60[0m     [0mRDD[0m[0;34m.[0m[0mtoDF[0m [0;34m=[0m [0mtoDF[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36mcreateDataFrame[0;34m(self, data, schema, samplingRatio, verifySchema)[0m
[1;32m    744[0m [0;34m[0m[0m
[1;32m    745[0m         [0;32mif[0m [0misinstance[0m[0;34m([0m[0mdata[0m[0;34m,[0m [0mRDD[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 746[0;31m             [0mrdd[0m[0;34m,[0m [0mschema[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_createFromRDD[0m[0;34m([0m[0mdata[0m[0;34m.[0m[0mmap[0m[0;34m([0m[0mprepare[0m[0;34m)[0m[0;34m,[0m [0mschema[0m[0;34m,[0m [0msamplingRatio[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    747[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    748[0m             [0mrdd[0m[0;34m,[0m [0mschema[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_createFromLocal[0m[0;34m([0m[0mmap[0m[0;34m([0m[0mprepare[0m[0;34m,[0m [0mdata[0m[0;34m)[0m[0;34m,[0m [0mschema[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36m_createFromRDD[0;34m(self, rdd, schema, samplingRatio)[0m
[1;32m    388[0m         """
[1;32m    389[0m         [0;32mif[0m [0mschema[0m [0;32mis[0m [0mNone[0m [0;32mor[0m [0misinstance[0m[0;34m([0m[0mschema[0m[0;34m,[0m [0;34m([0m[0mlist[0m[0;34m,[0m [0mtuple[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 390[0;31m             [0mstruct[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_inferSchema[0m[0;34m([0m[0mrdd[0m[0;34m,[0m [0msamplingRatio[0m[0;34m,[0m [0mnames[0m[0;34m=[0m[0mschema[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    391[0m             [0mconverter[0m [0;34m=[0m [0m_create_converter[0m[0;34m([0m[0mstruct[0m[0;34m)[0m[0;34m[0m[0m
[1;32m    392[0m             [0mrdd[0m [0;34m=[0m [0mrdd[0m[0;34m.[0m[0mmap[0m[0;34m([0m[0mconverter[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36m_inferSchema[0;34m(self, rdd, samplingRatio, names)[0m
[1;32m    359[0m         [0;34m:[0m[0;32mreturn[0m[0;34m:[0m [0;34m:[0m[0;32mclass[0m[0;34m:[0m[0;34m`[0m[0mpyspark[0m[0;34m.[0m[0msql[0m[0;34m.[0m[0mtypes[0m[0;34m.[0m[0mStructType[0m[0;34m`[0m[0;34m[0m[0m
[1;32m    360[0m         """
[0;32m--> 361[0;31m         [0mfirst[0m [0;34m=[0m [0mrdd[0m[0;34m.[0m[0mfirst[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    362[0m         [0;32mif[0m [0;32mnot[0m [0mfirst[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    363[0m             raise ValueError("The first row in RDD is empty, "

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py[0m in [0;36mfirst[0;34m(self)[0m
[1;32m   1376[0m         [0mValueError[0m[0;34m:[0m [0mRDD[0m [0;32mis[0m [0mempty[0m[0;34m[0m[0m
[1;32m   1377[0m         """
[0;32m-> 1378[0;31m         [0mrs[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mtake[0m[0;34m([0m[0;36m1[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1379[0m         [0;32mif[0m [0mrs[0m[0;34m:[0m[0;34m[0m[0m
[1;32m   1380[0m             [0;32mreturn[0m [0mrs[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py[0m in [0;36mtake[0;34m(self, num)[0m
[1;32m   1358[0m [0;34m[0m[0m
[1;32m   1359[0m             [0mp[0m [0;34m=[0m [0mrange[0m[0;34m([0m[0mpartsScanned[0m[0;34m,[0m [0mmin[0m[0;34m([0m[0mpartsScanned[0m [0;34m+[0m [0mnumPartsToTry[0m[0;34m,[0m [0mtotalParts[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0;32m-> 1360[0;31m             [0mres[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mcontext[0m[0;34m.[0m[0mrunJob[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mtakeUpToNumLeft[0m[0;34m,[0m [0mp[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1361[0m [0;34m[0m[0m
[1;32m   1362[0m             [0mitems[0m [0;34m+=[0m [0mres[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/context.py[0m in [0;36mrunJob[0;34m(self, rdd, partitionFunc, partitions, allowLocal)[0m
[1;32m   1067[0m         [0;31m# SparkContext#runJob.[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1068[0m         [0mmappedRDD[0m [0;34m=[0m [0mrdd[0m[0;34m.[0m[0mmapPartitions[0m[0;34m([0m[0mpartitionFunc[0m[0;34m)[0m[0;34m[0m[0m
[0;32m-> 1069[0;31m         [0msock_info[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_jvm[0m[0;34m.[0m[0mPythonRDD[0m[0;34m.[0m[0mrunJob[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jsc[0m[0;34m.[0m[0msc[0m[0;34m([0m[0;34m)[0m[0;34m,[0m [0mmappedRDD[0m[0;34m.[0m[0m_jrdd[0m[0;34m,[0m [0mpartitions[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1070[0m         [0;32mreturn[0m [0mlist[0m[0;34m([0m[0m_load_from_socket[0m[0;34m([0m[0msock_info[0m[0;34m,[0m [0mmappedRDD[0m[0;34m.[0m[0m_jrdd_deserializer[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1071[0m [0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     61[0m     [0;32mdef[0m [0mdeco[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     62[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 63[0;31m             [0;32mreturn[0m [0mf[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     64[0m         [0;32mexcept[0m [0mpy4j[0m[0;34m.[0m[0mprotocol[0m[0;34m.[0m[0mPy4JJavaError[0m [0;32mas[0m [0me[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     65[0m             [0ms[0m [0;34m=[0m [0me[0m[0;34m.[0m[0mjava_exception[0m[0;34m.[0m[0mtoString[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py[0m in [0;36mget_return_value[0;34m(answer, gateway_client, target_id, name)[0m
[1;32m    326[0m                 raise Py4JJavaError(
[1;32m    327[0m                     [0;34m"An error occurred while calling {0}{1}{2}.\n"[0m[0;34m.[0m[0;34m[0m[0m
[0;32m--> 328[0;31m                     format(target_id, ".", name), value)
[0m[1;32m    329[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    330[0m                 raise Py4JError(

[0;31mPy4JJavaError[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 17, 172.18.0.8, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main
    process()
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 393, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 1354, in takeUpToNumLeft
  File "<ipython-input-16-d877e8200fea>", line 3, in sumar
TypeError: unsupported operand type(s) for +: 'int' and 'Row'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)
	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)
	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main
    process()
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 393, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 1354, in takeUpToNumLeft
  File "<ipython-input-16-d877e8200fea>", line 3, in sumar
TypeError: unsupported operand type(s) for +: 'int' and 'Row'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)
	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

 INFO [2020-01-19 00:24:46,631] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:24:46,658] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200119-001021_531760765 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:25:16,194] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:25:19,927] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:25:19,948] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20200119-001021_531760765 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:25:19,949] ({pool-2-thread-5} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-001021_531760765, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:25:20,993] ({pool-2-thread-5} NotebookServer.java[afterStatusChange]:2316) - Job 20200119-001021_531760765 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mPy4JJavaError[0mTraceback (most recent call last)
[0;32m<ipython-input-18-5589a9fc2c75>[0m in [0;36m<module>[0;34m()[0m
[1;32m      3[0m     [0;32mfor[0m [0mrow[0m [0;32min[0m [0mpartition[0m[0;34m:[0m [0;32myield[0m [0msuma[0m [0;34m+[0m [0mrow[0m[0;34m([0m[0;34m"nuym"[0m[0;34m)[0m[0;34m[0m[0m
[1;32m      4[0m [0;34m[0m[0m
[0;32m----> 5[0;31m [0mdataFrame[0m[0;34m.[0m[0mrdd[0m[0;34m.[0m[0mmapPartitions[0m[0;34m([0m[0msumar[0m[0;34m)[0m[0;34m.[0m[0mtoDF[0m[0;34m([0m[0;34m[[0m[0;34m"result"[0m[0;34m][0m[0;34m)[0m[0;34m.[0m[0mshow[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36mtoDF[0;34m(self, schema, sampleRatio)[0m
[1;32m     56[0m         [0;34m[[0m[0mRow[0m[0;34m([0m[0mname[0m[0;34m=[0m[0;34mu'Alice'[0m[0;34m,[0m [0mage[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m[0;34m][0m[0;34m[0m[0m
[1;32m     57[0m         """
[0;32m---> 58[0;31m         [0;32mreturn[0m [0msparkSession[0m[0;34m.[0m[0mcreateDataFrame[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mschema[0m[0;34m,[0m [0msampleRatio[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     59[0m [0;34m[0m[0m
[1;32m     60[0m     [0mRDD[0m[0;34m.[0m[0mtoDF[0m [0;34m=[0m [0mtoDF[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36mcreateDataFrame[0;34m(self, data, schema, samplingRatio, verifySchema)[0m
[1;32m    744[0m [0;34m[0m[0m
[1;32m    745[0m         [0;32mif[0m [0misinstance[0m[0;34m([0m[0mdata[0m[0;34m,[0m [0mRDD[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 746[0;31m             [0mrdd[0m[0;34m,[0m [0mschema[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_createFromRDD[0m[0;34m([0m[0mdata[0m[0;34m.[0m[0mmap[0m[0;34m([0m[0mprepare[0m[0;34m)[0m[0;34m,[0m [0mschema[0m[0;34m,[0m [0msamplingRatio[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    747[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    748[0m             [0mrdd[0m[0;34m,[0m [0mschema[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_createFromLocal[0m[0;34m([0m[0mmap[0m[0;34m([0m[0mprepare[0m[0;34m,[0m [0mdata[0m[0;34m)[0m[0;34m,[0m [0mschema[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36m_createFromRDD[0;34m(self, rdd, schema, samplingRatio)[0m
[1;32m    388[0m         """
[1;32m    389[0m         [0;32mif[0m [0mschema[0m [0;32mis[0m [0mNone[0m [0;32mor[0m [0misinstance[0m[0;34m([0m[0mschema[0m[0;34m,[0m [0;34m([0m[0mlist[0m[0;34m,[0m [0mtuple[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 390[0;31m             [0mstruct[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_inferSchema[0m[0;34m([0m[0mrdd[0m[0;34m,[0m [0msamplingRatio[0m[0;34m,[0m [0mnames[0m[0;34m=[0m[0mschema[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    391[0m             [0mconverter[0m [0;34m=[0m [0m_create_converter[0m[0;34m([0m[0mstruct[0m[0;34m)[0m[0;34m[0m[0m
[1;32m    392[0m             [0mrdd[0m [0;34m=[0m [0mrdd[0m[0;34m.[0m[0mmap[0m[0;34m([0m[0mconverter[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36m_inferSchema[0;34m(self, rdd, samplingRatio, names)[0m
[1;32m    359[0m         [0;34m:[0m[0;32mreturn[0m[0;34m:[0m [0;34m:[0m[0;32mclass[0m[0;34m:[0m[0;34m`[0m[0mpyspark[0m[0;34m.[0m[0msql[0m[0;34m.[0m[0mtypes[0m[0;34m.[0m[0mStructType[0m[0;34m`[0m[0;34m[0m[0m
[1;32m    360[0m         """
[0;32m--> 361[0;31m         [0mfirst[0m [0;34m=[0m [0mrdd[0m[0;34m.[0m[0mfirst[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    362[0m         [0;32mif[0m [0;32mnot[0m [0mfirst[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    363[0m             raise ValueError("The first row in RDD is empty, "

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py[0m in [0;36mfirst[0;34m(self)[0m
[1;32m   1376[0m         [0mValueError[0m[0;34m:[0m [0mRDD[0m [0;32mis[0m [0mempty[0m[0;34m[0m[0m
[1;32m   1377[0m         """
[0;32m-> 1378[0;31m         [0mrs[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mtake[0m[0;34m([0m[0;36m1[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1379[0m         [0;32mif[0m [0mrs[0m[0;34m:[0m[0;34m[0m[0m
[1;32m   1380[0m             [0;32mreturn[0m [0mrs[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py[0m in [0;36mtake[0;34m(self, num)[0m
[1;32m   1358[0m [0;34m[0m[0m
[1;32m   1359[0m             [0mp[0m [0;34m=[0m [0mrange[0m[0;34m([0m[0mpartsScanned[0m[0;34m,[0m [0mmin[0m[0;34m([0m[0mpartsScanned[0m [0;34m+[0m [0mnumPartsToTry[0m[0;34m,[0m [0mtotalParts[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0;32m-> 1360[0;31m             [0mres[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mcontext[0m[0;34m.[0m[0mrunJob[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mtakeUpToNumLeft[0m[0;34m,[0m [0mp[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1361[0m [0;34m[0m[0m
[1;32m   1362[0m             [0mitems[0m [0;34m+=[0m [0mres[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/context.py[0m in [0;36mrunJob[0;34m(self, rdd, partitionFunc, partitions, allowLocal)[0m
[1;32m   1067[0m         [0;31m# SparkContext#runJob.[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1068[0m         [0mmappedRDD[0m [0;34m=[0m [0mrdd[0m[0;34m.[0m[0mmapPartitions[0m[0;34m([0m[0mpartitionFunc[0m[0;34m)[0m[0;34m[0m[0m
[0;32m-> 1069[0;31m         [0msock_info[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_jvm[0m[0;34m.[0m[0mPythonRDD[0m[0;34m.[0m[0mrunJob[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jsc[0m[0;34m.[0m[0msc[0m[0;34m([0m[0;34m)[0m[0;34m,[0m [0mmappedRDD[0m[0;34m.[0m[0m_jrdd[0m[0;34m,[0m [0mpartitions[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1070[0m         [0;32mreturn[0m [0mlist[0m[0;34m([0m[0m_load_from_socket[0m[0;34m([0m[0msock_info[0m[0;34m,[0m [0mmappedRDD[0m[0;34m.[0m[0m_jrdd_deserializer[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1071[0m [0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1255[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1256[0m         return_value = get_return_value(
[0;32m-> 1257[0;31m             answer, self.gateway_client, self.target_id, self.name)
[0m[1;32m   1258[0m [0;34m[0m[0m
[1;32m   1259[0m         [0;32mfor[0m [0mtemp_arg[0m [0;32min[0m [0mtemp_args[0m[0;34m:[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m     61[0m     [0;32mdef[0m [0mdeco[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     62[0m         [0;32mtry[0m[0;34m:[0m[0;34m[0m[0m
[0;32m---> 63[0;31m             [0;32mreturn[0m [0mf[0m[0;34m([0m[0;34m*[0m[0ma[0m[0;34m,[0m [0;34m**[0m[0mkw[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     64[0m         [0;32mexcept[0m [0mpy4j[0m[0;34m.[0m[0mprotocol[0m[0;34m.[0m[0mPy4JJavaError[0m [0;32mas[0m [0me[0m[0;34m:[0m[0;34m[0m[0m
[1;32m     65[0m             [0ms[0m [0;34m=[0m [0me[0m[0;34m.[0m[0mjava_exception[0m[0;34m.[0m[0mtoString[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py[0m in [0;36mget_return_value[0;34m(answer, gateway_client, target_id, name)[0m
[1;32m    326[0m                 raise Py4JJavaError(
[1;32m    327[0m                     [0;34m"An error occurred while calling {0}{1}{2}.\n"[0m[0;34m.[0m[0;34m[0m[0m
[0;32m--> 328[0;31m                     format(target_id, ".", name), value)
[0m[1;32m    329[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    330[0m                 raise Py4JError(

[0;31mPy4JJavaError[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 21, 172.18.0.7, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main
    process()
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 393, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 1354, in takeUpToNumLeft
  File "<ipython-input-18-5589a9fc2c75>", line 3, in sumar
TypeError: unsupported operand type(s) for +: 'int' and 'Row'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)
	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)
	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main
    process()
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 393, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 1354, in takeUpToNumLeft
  File "<ipython-input-18-5589a9fc2c75>", line 3, in sumar
TypeError: unsupported operand type(s) for +: 'int' and 'Row'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)
	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

 INFO [2020-01-19 00:25:21,036] ({pool-2-thread-5} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:25:21,067] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20200119-001021_531760765 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:26:06,792] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:26:06,816] ({pool-2-thread-6} SchedulerFactory.java[jobStarted]:114) - Job 20200119-001021_531760765 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:26:06,817] ({pool-2-thread-6} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-001021_531760765, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:26:06,865] ({pool-2-thread-6} NotebookServer.java[afterStatusChange]:2316) - Job 20200119-001021_531760765 is finished, status: ERROR, exception: null, result: %text [0;36m  File [0;32m"<ipython-input-20-69bac7a3f180>"[0;36m, line [0;32m3[0m
[0;31m    for row in partition: yield print row[0m
[0m                                    ^[0m
[0;31mSyntaxError[0m[0;31m:[0m invalid syntax

 INFO [2020-01-19 00:26:06,898] ({pool-2-thread-6} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:26:06,913] ({pool-2-thread-6} SchedulerFactory.java[jobFinished]:120) - Job 20200119-001021_531760765 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:26:15,796] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:26:15,811] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200119-001021_531760765 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:26:15,813] ({pool-2-thread-4} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-001021_531760765, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:26:16,527] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2316) - Job 20200119-001021_531760765 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mValueError[0mTraceback (most recent call last)
[0;32m<ipython-input-22-2183ce449584>[0m in [0;36m<module>[0;34m()[0m
[1;32m      5[0m     [0;32mreturn[0m [0mpartition[0m[0;34m[0m[0m
[1;32m      6[0m [0;34m[0m[0m
[0;32m----> 7[0;31m [0mdataFrame[0m[0;34m.[0m[0mrdd[0m[0;34m.[0m[0mmapPartitions[0m[0;34m([0m[0msumar[0m[0;34m)[0m[0;34m.[0m[0mtoDF[0m[0;34m([0m[0;34m[[0m[0;34m"result"[0m[0;34m][0m[0;34m)[0m[0;34m.[0m[0mshow[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36mtoDF[0;34m(self, schema, sampleRatio)[0m
[1;32m     56[0m         [0;34m[[0m[0mRow[0m[0;34m([0m[0mname[0m[0;34m=[0m[0;34mu'Alice'[0m[0;34m,[0m [0mage[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m[0;34m][0m[0;34m[0m[0m
[1;32m     57[0m         """
[0;32m---> 58[0;31m         [0;32mreturn[0m [0msparkSession[0m[0;34m.[0m[0mcreateDataFrame[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mschema[0m[0;34m,[0m [0msampleRatio[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     59[0m [0;34m[0m[0m
[1;32m     60[0m     [0mRDD[0m[0;34m.[0m[0mtoDF[0m [0;34m=[0m [0mtoDF[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36mcreateDataFrame[0;34m(self, data, schema, samplingRatio, verifySchema)[0m
[1;32m    744[0m [0;34m[0m[0m
[1;32m    745[0m         [0;32mif[0m [0misinstance[0m[0;34m([0m[0mdata[0m[0;34m,[0m [0mRDD[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 746[0;31m             [0mrdd[0m[0;34m,[0m [0mschema[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_createFromRDD[0m[0;34m([0m[0mdata[0m[0;34m.[0m[0mmap[0m[0;34m([0m[0mprepare[0m[0;34m)[0m[0;34m,[0m [0mschema[0m[0;34m,[0m [0msamplingRatio[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    747[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    748[0m             [0mrdd[0m[0;34m,[0m [0mschema[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_createFromLocal[0m[0;34m([0m[0mmap[0m[0;34m([0m[0mprepare[0m[0;34m,[0m [0mdata[0m[0;34m)[0m[0;34m,[0m [0mschema[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36m_createFromRDD[0;34m(self, rdd, schema, samplingRatio)[0m
[1;32m    388[0m         """
[1;32m    389[0m         [0;32mif[0m [0mschema[0m [0;32mis[0m [0mNone[0m [0;32mor[0m [0misinstance[0m[0;34m([0m[0mschema[0m[0;34m,[0m [0;34m([0m[0mlist[0m[0;34m,[0m [0mtuple[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 390[0;31m             [0mstruct[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_inferSchema[0m[0;34m([0m[0mrdd[0m[0;34m,[0m [0msamplingRatio[0m[0;34m,[0m [0mnames[0m[0;34m=[0m[0mschema[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    391[0m             [0mconverter[0m [0;34m=[0m [0m_create_converter[0m[0;34m([0m[0mstruct[0m[0;34m)[0m[0;34m[0m[0m
[1;32m    392[0m             [0mrdd[0m [0;34m=[0m [0mrdd[0m[0;34m.[0m[0mmap[0m[0;34m([0m[0mconverter[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36m_inferSchema[0;34m(self, rdd, samplingRatio, names)[0m
[1;32m    359[0m         [0;34m:[0m[0;32mreturn[0m[0;34m:[0m [0;34m:[0m[0;32mclass[0m[0;34m:[0m[0;34m`[0m[0mpyspark[0m[0;34m.[0m[0msql[0m[0;34m.[0m[0mtypes[0m[0;34m.[0m[0mStructType[0m[0;34m`[0m[0;34m[0m[0m
[1;32m    360[0m         """
[0;32m--> 361[0;31m         [0mfirst[0m [0;34m=[0m [0mrdd[0m[0;34m.[0m[0mfirst[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    362[0m         [0;32mif[0m [0;32mnot[0m [0mfirst[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    363[0m             raise ValueError("The first row in RDD is empty, "

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py[0m in [0;36mfirst[0;34m(self)[0m
[1;32m   1379[0m         [0;32mif[0m [0mrs[0m[0;34m:[0m[0;34m[0m[0m
[1;32m   1380[0m             [0;32mreturn[0m [0mrs[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m[0m[0m
[0;32m-> 1381[0;31m         [0;32mraise[0m [0mValueError[0m[0;34m([0m[0;34m"RDD is empty"[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1382[0m [0;34m[0m[0m
[1;32m   1383[0m     [0;32mdef[0m [0misEmpty[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m

[0;31mValueError[0m: RDD is empty
 INFO [2020-01-19 00:26:16,543] ({pool-2-thread-4} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:26:16,558] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200119-001021_531760765 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:26:26,510] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:26:26,524] ({pool-2-thread-7} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:26:26,528] ({pool-2-thread-7} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195328_2060007234, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:26:26,736] ({pool-2-thread-7} NotebookServer.java[afterStatusChange]:2314) - Job 20200118-195328_2060007234 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:26:26,762] ({pool-2-thread-7} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:26:26,802] ({pool-2-thread-7} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:26:29,735] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:26:29,757] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20200119-001021_531760765 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:26:29,760] ({pool-2-thread-3} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-001021_531760765, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:26:30,441] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2316) - Job 20200119-001021_531760765 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mValueError[0mTraceback (most recent call last)
[0;32m<ipython-input-26-2183ce449584>[0m in [0;36m<module>[0;34m()[0m
[1;32m      5[0m     [0;32mreturn[0m [0mpartition[0m[0;34m[0m[0m
[1;32m      6[0m [0;34m[0m[0m
[0;32m----> 7[0;31m [0mdataFrame[0m[0;34m.[0m[0mrdd[0m[0;34m.[0m[0mmapPartitions[0m[0;34m([0m[0msumar[0m[0;34m)[0m[0;34m.[0m[0mtoDF[0m[0;34m([0m[0;34m[[0m[0;34m"result"[0m[0;34m][0m[0;34m)[0m[0;34m.[0m[0mshow[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36mtoDF[0;34m(self, schema, sampleRatio)[0m
[1;32m     56[0m         [0;34m[[0m[0mRow[0m[0;34m([0m[0mname[0m[0;34m=[0m[0;34mu'Alice'[0m[0;34m,[0m [0mage[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m[0;34m][0m[0;34m[0m[0m
[1;32m     57[0m         """
[0;32m---> 58[0;31m         [0;32mreturn[0m [0msparkSession[0m[0;34m.[0m[0mcreateDataFrame[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mschema[0m[0;34m,[0m [0msampleRatio[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     59[0m [0;34m[0m[0m
[1;32m     60[0m     [0mRDD[0m[0;34m.[0m[0mtoDF[0m [0;34m=[0m [0mtoDF[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36mcreateDataFrame[0;34m(self, data, schema, samplingRatio, verifySchema)[0m
[1;32m    744[0m [0;34m[0m[0m
[1;32m    745[0m         [0;32mif[0m [0misinstance[0m[0;34m([0m[0mdata[0m[0;34m,[0m [0mRDD[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 746[0;31m             [0mrdd[0m[0;34m,[0m [0mschema[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_createFromRDD[0m[0;34m([0m[0mdata[0m[0;34m.[0m[0mmap[0m[0;34m([0m[0mprepare[0m[0;34m)[0m[0;34m,[0m [0mschema[0m[0;34m,[0m [0msamplingRatio[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    747[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    748[0m             [0mrdd[0m[0;34m,[0m [0mschema[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_createFromLocal[0m[0;34m([0m[0mmap[0m[0;34m([0m[0mprepare[0m[0;34m,[0m [0mdata[0m[0;34m)[0m[0;34m,[0m [0mschema[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36m_createFromRDD[0;34m(self, rdd, schema, samplingRatio)[0m
[1;32m    388[0m         """
[1;32m    389[0m         [0;32mif[0m [0mschema[0m [0;32mis[0m [0mNone[0m [0;32mor[0m [0misinstance[0m[0;34m([0m[0mschema[0m[0;34m,[0m [0;34m([0m[0mlist[0m[0;34m,[0m [0mtuple[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 390[0;31m             [0mstruct[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_inferSchema[0m[0;34m([0m[0mrdd[0m[0;34m,[0m [0msamplingRatio[0m[0;34m,[0m [0mnames[0m[0;34m=[0m[0mschema[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    391[0m             [0mconverter[0m [0;34m=[0m [0m_create_converter[0m[0;34m([0m[0mstruct[0m[0;34m)[0m[0;34m[0m[0m
[1;32m    392[0m             [0mrdd[0m [0;34m=[0m [0mrdd[0m[0;34m.[0m[0mmap[0m[0;34m([0m[0mconverter[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36m_inferSchema[0;34m(self, rdd, samplingRatio, names)[0m
[1;32m    359[0m         [0;34m:[0m[0;32mreturn[0m[0;34m:[0m [0;34m:[0m[0;32mclass[0m[0;34m:[0m[0;34m`[0m[0mpyspark[0m[0;34m.[0m[0msql[0m[0;34m.[0m[0mtypes[0m[0;34m.[0m[0mStructType[0m[0;34m`[0m[0;34m[0m[0m
[1;32m    360[0m         """
[0;32m--> 361[0;31m         [0mfirst[0m [0;34m=[0m [0mrdd[0m[0;34m.[0m[0mfirst[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    362[0m         [0;32mif[0m [0;32mnot[0m [0mfirst[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    363[0m             raise ValueError("The first row in RDD is empty, "

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py[0m in [0;36mfirst[0;34m(self)[0m
[1;32m   1379[0m         [0;32mif[0m [0mrs[0m[0;34m:[0m[0;34m[0m[0m
[1;32m   1380[0m             [0;32mreturn[0m [0mrs[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m[0m[0m
[0;32m-> 1381[0;31m         [0;32mraise[0m [0mValueError[0m[0;34m([0m[0;34m"RDD is empty"[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1382[0m [0;34m[0m[0m
[1;32m   1383[0m     [0;32mdef[0m [0misEmpty[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m

[0;31mValueError[0m: RDD is empty
 INFO [2020-01-19 00:26:30,477] ({pool-2-thread-3} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:26:30,496] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20200119-001021_531760765 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:27:05,739] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:27:05,758] ({pool-2-thread-8} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:27:05,763] ({pool-2-thread-8} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195328_2060007234, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:27:05,858] ({pool-2-thread-8} NotebookServer.java[afterStatusChange]:2314) - Job 20200118-195328_2060007234 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:27:05,881] ({pool-2-thread-8} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:27:05,892] ({pool-2-thread-8} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:27:06,528] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:27:06,550] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:27:06,551] ({pool-2-thread-2} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195328_2060007234, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:27:06,678] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2314) - Job 20200118-195328_2060007234 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:27:06,693] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:27:06,709] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:27:07,938] ({qtp2107447833-15} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:27:07,956] ({pool-2-thread-9} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:27:07,957] ({pool-2-thread-9} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-000328_1025815385, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:27:08,442] ({pool-2-thread-9} NotebookServer.java[afterStatusChange]:2314) - Job 20200119-000328_1025815385 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:27:08,465] ({pool-2-thread-9} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:27:08,492] ({pool-2-thread-9} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:27:11,234] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:27:11,255] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20200119-001021_531760765 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:27:11,265] ({pool-2-thread-5} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-001021_531760765, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:27:12,113] ({pool-2-thread-5} NotebookServer.java[afterStatusChange]:2316) - Job 20200119-001021_531760765 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mValueError[0mTraceback (most recent call last)
[0;32m<ipython-input-34-2183ce449584>[0m in [0;36m<module>[0;34m()[0m
[1;32m      5[0m     [0;32mreturn[0m [0mpartition[0m[0;34m[0m[0m
[1;32m      6[0m [0;34m[0m[0m
[0;32m----> 7[0;31m [0mdataFrame[0m[0;34m.[0m[0mrdd[0m[0;34m.[0m[0mmapPartitions[0m[0;34m([0m[0msumar[0m[0;34m)[0m[0;34m.[0m[0mtoDF[0m[0;34m([0m[0;34m[[0m[0;34m"result"[0m[0;34m][0m[0;34m)[0m[0;34m.[0m[0mshow[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36mtoDF[0;34m(self, schema, sampleRatio)[0m
[1;32m     56[0m         [0;34m[[0m[0mRow[0m[0;34m([0m[0mname[0m[0;34m=[0m[0;34mu'Alice'[0m[0;34m,[0m [0mage[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m[0;34m][0m[0;34m[0m[0m
[1;32m     57[0m         """
[0;32m---> 58[0;31m         [0;32mreturn[0m [0msparkSession[0m[0;34m.[0m[0mcreateDataFrame[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mschema[0m[0;34m,[0m [0msampleRatio[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     59[0m [0;34m[0m[0m
[1;32m     60[0m     [0mRDD[0m[0;34m.[0m[0mtoDF[0m [0;34m=[0m [0mtoDF[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36mcreateDataFrame[0;34m(self, data, schema, samplingRatio, verifySchema)[0m
[1;32m    744[0m [0;34m[0m[0m
[1;32m    745[0m         [0;32mif[0m [0misinstance[0m[0;34m([0m[0mdata[0m[0;34m,[0m [0mRDD[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 746[0;31m             [0mrdd[0m[0;34m,[0m [0mschema[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_createFromRDD[0m[0;34m([0m[0mdata[0m[0;34m.[0m[0mmap[0m[0;34m([0m[0mprepare[0m[0;34m)[0m[0;34m,[0m [0mschema[0m[0;34m,[0m [0msamplingRatio[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    747[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    748[0m             [0mrdd[0m[0;34m,[0m [0mschema[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_createFromLocal[0m[0;34m([0m[0mmap[0m[0;34m([0m[0mprepare[0m[0;34m,[0m [0mdata[0m[0;34m)[0m[0;34m,[0m [0mschema[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36m_createFromRDD[0;34m(self, rdd, schema, samplingRatio)[0m
[1;32m    388[0m         """
[1;32m    389[0m         [0;32mif[0m [0mschema[0m [0;32mis[0m [0mNone[0m [0;32mor[0m [0misinstance[0m[0;34m([0m[0mschema[0m[0;34m,[0m [0;34m([0m[0mlist[0m[0;34m,[0m [0mtuple[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 390[0;31m             [0mstruct[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_inferSchema[0m[0;34m([0m[0mrdd[0m[0;34m,[0m [0msamplingRatio[0m[0;34m,[0m [0mnames[0m[0;34m=[0m[0mschema[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    391[0m             [0mconverter[0m [0;34m=[0m [0m_create_converter[0m[0;34m([0m[0mstruct[0m[0;34m)[0m[0;34m[0m[0m
[1;32m    392[0m             [0mrdd[0m [0;34m=[0m [0mrdd[0m[0;34m.[0m[0mmap[0m[0;34m([0m[0mconverter[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36m_inferSchema[0;34m(self, rdd, samplingRatio, names)[0m
[1;32m    359[0m         [0;34m:[0m[0;32mreturn[0m[0;34m:[0m [0;34m:[0m[0;32mclass[0m[0;34m:[0m[0;34m`[0m[0mpyspark[0m[0;34m.[0m[0msql[0m[0;34m.[0m[0mtypes[0m[0;34m.[0m[0mStructType[0m[0;34m`[0m[0;34m[0m[0m
[1;32m    360[0m         """
[0;32m--> 361[0;31m         [0mfirst[0m [0;34m=[0m [0mrdd[0m[0;34m.[0m[0mfirst[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    362[0m         [0;32mif[0m [0;32mnot[0m [0mfirst[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    363[0m             raise ValueError("The first row in RDD is empty, "

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py[0m in [0;36mfirst[0;34m(self)[0m
[1;32m   1379[0m         [0;32mif[0m [0mrs[0m[0;34m:[0m[0;34m[0m[0m
[1;32m   1380[0m             [0;32mreturn[0m [0mrs[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m[0m[0m
[0;32m-> 1381[0;31m         [0;32mraise[0m [0mValueError[0m[0;34m([0m[0;34m"RDD is empty"[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1382[0m [0;34m[0m[0m
[1;32m   1383[0m     [0;32mdef[0m [0misEmpty[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m

[0;31mValueError[0m: RDD is empty
 INFO [2020-01-19 00:27:12,123] ({pool-2-thread-5} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:27:12,150] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20200119-001021_531760765 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:27:22,282] ({qtp2107447833-13} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:27:22,295] ({pool-2-thread-10} SchedulerFactory.java[jobStarted]:114) - Job 20200119-001021_531760765 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:27:22,299] ({pool-2-thread-10} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-001021_531760765, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:27:22,551] ({pool-2-thread-10} NotebookServer.java[afterStatusChange]:2316) - Job 20200119-001021_531760765 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 590, in dumps
    return cloudpickle.dumps(obj, 2)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 863, in dumps
    cp.dump(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 260, in dump
    return Pickler.dump(self, obj)
  File "/opt/conda/lib/python2.7/pickle.py", line 224, in dump
    self.save(obj)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 568, in save_tuple
    save(element)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 406, in save_function
    self.save_function_tuple(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 549, in save_function_tuple
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 606, in save_list
    self._batch_appends(iter(obj))
  File "/opt/conda/lib/python2.7/pickle.py", line 639, in _batch_appends
    save(x)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 406, in save_function
    self.save_function_tuple(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 549, in save_function_tuple
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 606, in save_list
    self._batch_appends(iter(obj))
  File "/opt/conda/lib/python2.7/pickle.py", line 639, in _batch_appends
    save(x)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 406, in save_function
    self.save_function_tuple(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 549, in save_function_tuple
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 606, in save_list
    self._batch_appends(iter(obj))
  File "/opt/conda/lib/python2.7/pickle.py", line 642, in _batch_appends
    save(tmp[0])
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 400, in save_function
    self.save_function_tuple(obj)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 549, in save_function_tuple
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 692, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 331, in save
    self.save_reduce(obj=obj, *rv)
  File "/opt/conda/lib/python2.7/pickle.py", line 425, in save_reduce
    save(state)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 655, in save_dict
    self._batch_setitems(obj.iteritems())
  File "/opt/conda/lib/python2.7/pickle.py", line 687, in _batch_setitems
    save(v)
  File "/opt/conda/lib/python2.7/pickle.py", line 306, in save
    rv = reduce(self.proto)
  File "/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
Py4JError: An error occurred while calling o300.__getnewargs__. Trace:
py4j.Py4JException: Method __getnewargs__([]) does not exist
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)
	at py4j.Gateway.invoke(Gateway.java:274)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)


[0;31m[0m
[0;31mPicklingError[0mTraceback (most recent call last)
[0;32m<ipython-input-36-8503e0df3c52>[0m in [0;36m<module>[0;34m()[0m
[1;32m      7[0m     [0;32mprint[0m [0mdataFrame[0m[0;34m.[0m[0mrdd[0m[0;34m[0m[0m
[1;32m      8[0m [0;34m[0m[0m
[0;32m----> 9[0;31m [0mdataFrame[0m[0;34m.[0m[0mrdd[0m[0;34m.[0m[0mmapPartitions[0m[0;34m([0m[0msumar[0m[0;34m)[0m[0;34m.[0m[0mtoDF[0m[0;34m([0m[0;34m[[0m[0;34m"result"[0m[0;34m][0m[0;34m)[0m[0;34m.[0m[0mshow[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36mtoDF[0;34m(self, schema, sampleRatio)[0m
[1;32m     56[0m         [0;34m[[0m[0mRow[0m[0;34m([0m[0mname[0m[0;34m=[0m[0;34mu'Alice'[0m[0;34m,[0m [0mage[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m[0;34m][0m[0;34m[0m[0m
[1;32m     57[0m         """
[0;32m---> 58[0;31m         [0;32mreturn[0m [0msparkSession[0m[0;34m.[0m[0mcreateDataFrame[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mschema[0m[0;34m,[0m [0msampleRatio[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     59[0m [0;34m[0m[0m
[1;32m     60[0m     [0mRDD[0m[0;34m.[0m[0mtoDF[0m [0;34m=[0m [0mtoDF[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36mcreateDataFrame[0;34m(self, data, schema, samplingRatio, verifySchema)[0m
[1;32m    744[0m [0;34m[0m[0m
[1;32m    745[0m         [0;32mif[0m [0misinstance[0m[0;34m([0m[0mdata[0m[0;34m,[0m [0mRDD[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 746[0;31m             [0mrdd[0m[0;34m,[0m [0mschema[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_createFromRDD[0m[0;34m([0m[0mdata[0m[0;34m.[0m[0mmap[0m[0;34m([0m[0mprepare[0m[0;34m)[0m[0;34m,[0m [0mschema[0m[0;34m,[0m [0msamplingRatio[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    747[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    748[0m             [0mrdd[0m[0;34m,[0m [0mschema[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_createFromLocal[0m[0;34m([0m[0mmap[0m[0;34m([0m[0mprepare[0m[0;34m,[0m [0mdata[0m[0;34m)[0m[0;34m,[0m [0mschema[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36m_createFromRDD[0;34m(self, rdd, schema, samplingRatio)[0m
[1;32m    388[0m         """
[1;32m    389[0m         [0;32mif[0m [0mschema[0m [0;32mis[0m [0mNone[0m [0;32mor[0m [0misinstance[0m[0;34m([0m[0mschema[0m[0;34m,[0m [0;34m([0m[0mlist[0m[0;34m,[0m [0mtuple[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 390[0;31m             [0mstruct[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_inferSchema[0m[0;34m([0m[0mrdd[0m[0;34m,[0m [0msamplingRatio[0m[0;34m,[0m [0mnames[0m[0;34m=[0m[0mschema[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    391[0m             [0mconverter[0m [0;34m=[0m [0m_create_converter[0m[0;34m([0m[0mstruct[0m[0;34m)[0m[0;34m[0m[0m
[1;32m    392[0m             [0mrdd[0m [0;34m=[0m [0mrdd[0m[0;34m.[0m[0mmap[0m[0;34m([0m[0mconverter[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36m_inferSchema[0;34m(self, rdd, samplingRatio, names)[0m
[1;32m    359[0m         [0;34m:[0m[0;32mreturn[0m[0;34m:[0m [0;34m:[0m[0;32mclass[0m[0;34m:[0m[0;34m`[0m[0mpyspark[0m[0;34m.[0m[0msql[0m[0;34m.[0m[0mtypes[0m[0;34m.[0m[0mStructType[0m[0;34m`[0m[0;34m[0m[0m
[1;32m    360[0m         """
[0;32m--> 361[0;31m         [0mfirst[0m [0;34m=[0m [0mrdd[0m[0;34m.[0m[0mfirst[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    362[0m         [0;32mif[0m [0;32mnot[0m [0mfirst[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    363[0m             raise ValueError("The first row in RDD is empty, "

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py[0m in [0;36mfirst[0;34m(self)[0m
[1;32m   1376[0m         [0mValueError[0m[0;34m:[0m [0mRDD[0m [0;32mis[0m [0mempty[0m[0;34m[0m[0m
[1;32m   1377[0m         """
[0;32m-> 1378[0;31m         [0mrs[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mtake[0m[0;34m([0m[0;36m1[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1379[0m         [0;32mif[0m [0mrs[0m[0;34m:[0m[0;34m[0m[0m
[1;32m   1380[0m             [0;32mreturn[0m [0mrs[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py[0m in [0;36mtake[0;34m(self, num)[0m
[1;32m   1358[0m [0;34m[0m[0m
[1;32m   1359[0m             [0mp[0m [0;34m=[0m [0mrange[0m[0;34m([0m[0mpartsScanned[0m[0;34m,[0m [0mmin[0m[0;34m([0m[0mpartsScanned[0m [0;34m+[0m [0mnumPartsToTry[0m[0;34m,[0m [0mtotalParts[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[0;32m-> 1360[0;31m             [0mres[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mcontext[0m[0;34m.[0m[0mrunJob[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mtakeUpToNumLeft[0m[0;34m,[0m [0mp[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1361[0m [0;34m[0m[0m
[1;32m   1362[0m             [0mitems[0m [0;34m+=[0m [0mres[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/context.py[0m in [0;36mrunJob[0;34m(self, rdd, partitionFunc, partitions, allowLocal)[0m
[1;32m   1067[0m         [0;31m# SparkContext#runJob.[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1068[0m         [0mmappedRDD[0m [0;34m=[0m [0mrdd[0m[0;34m.[0m[0mmapPartitions[0m[0;34m([0m[0mpartitionFunc[0m[0;34m)[0m[0;34m[0m[0m
[0;32m-> 1069[0;31m         [0msock_info[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_jvm[0m[0;34m.[0m[0mPythonRDD[0m[0;34m.[0m[0mrunJob[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jsc[0m[0;34m.[0m[0msc[0m[0;34m([0m[0;34m)[0m[0;34m,[0m [0mmappedRDD[0m[0;34m.[0m[0m_jrdd[0m[0;34m,[0m [0mpartitions[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1070[0m         [0;32mreturn[0m [0mlist[0m[0;34m([0m[0m_load_from_socket[0m[0;34m([0m[0msock_info[0m[0;34m,[0m [0mmappedRDD[0m[0;34m.[0m[0m_jrdd_deserializer[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1071[0m [0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py[0m in [0;36m_jrdd[0;34m(self)[0m
[1;32m   2530[0m [0;34m[0m[0m
[1;32m   2531[0m         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,
[0;32m-> 2532[0;31m                                       self._jrdd_deserializer, profiler)
[0m[1;32m   2533[0m         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,
[1;32m   2534[0m                                              self.preservesPartitioning, self.is_barrier)

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py[0m in [0;36m_wrap_function[0;34m(sc, func, deserializer, serializer, profiler)[0m
[1;32m   2432[0m     [0;32massert[0m [0mserializer[0m[0;34m,[0m [0;34m"serializer should not be empty"[0m[0;34m[0m[0m
[1;32m   2433[0m     [0mcommand[0m [0;34m=[0m [0;34m([0m[0mfunc[0m[0;34m,[0m [0mprofiler[0m[0;34m,[0m [0mdeserializer[0m[0;34m,[0m [0mserializer[0m[0;34m)[0m[0;34m[0m[0m
[0;32m-> 2434[0;31m     [0mpickled_command[0m[0;34m,[0m [0mbroadcast_vars[0m[0;34m,[0m [0menv[0m[0;34m,[0m [0mincludes[0m [0;34m=[0m [0m_prepare_for_python_RDD[0m[0;34m([0m[0msc[0m[0;34m,[0m [0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   2435[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,
[1;32m   2436[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py[0m in [0;36m_prepare_for_python_RDD[0;34m(sc, command)[0m
[1;32m   2418[0m     [0;31m# the serialized command will be compressed by broadcast[0m[0;34m[0m[0;34m[0m[0m
[1;32m   2419[0m     [0mser[0m [0;34m=[0m [0mCloudPickleSerializer[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0;32m-> 2420[0;31m     [0mpickled_command[0m [0;34m=[0m [0mser[0m[0;34m.[0m[0mdumps[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   2421[0m     [0;32mif[0m [0mlen[0m[0;34m([0m[0mpickled_command[0m[0;34m)[0m [0;34m>[0m [0;34m([0m[0;36m1[0m [0;34m<<[0m [0;36m20[0m[0;34m)[0m[0;34m:[0m  [0;31m# 1M[0m[0;34m[0m[0m
[1;32m   2422[0m         [0;31m# The broadcast will have same life cycle as created PythonRDD[0m[0;34m[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py[0m in [0;36mdumps[0;34m(self, obj)[0m
[1;32m    598[0m                 [0mmsg[0m [0;34m=[0m [0;34m"Could not serialize object: %s: %s"[0m [0;34m%[0m [0;34m([0m[0me[0m[0;34m.[0m[0m__class__[0m[0;34m.[0m[0m__name__[0m[0;34m,[0m [0memsg[0m[0;34m)[0m[0;34m[0m[0m
[1;32m    599[0m             [0mcloudpickle[0m[0;34m.[0m[0mprint_exec[0m[0;34m([0m[0msys[0m[0;34m.[0m[0mstderr[0m[0;34m)[0m[0;34m[0m[0m
[0;32m--> 600[0;31m             [0;32mraise[0m [0mpickle[0m[0;34m.[0m[0mPicklingError[0m[0;34m([0m[0mmsg[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    601[0m [0;34m[0m[0m
[1;32m    602[0m [0;34m[0m[0m

[0;31mPicklingError[0m: Could not serialize object: Py4JError: An error occurred while calling o300.__getnewargs__. Trace:
py4j.Py4JException: Method __getnewargs__([]) does not exist
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)
	at py4j.Gateway.invoke(Gateway.java:274)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)


 INFO [2020-01-19 00:27:22,566] ({pool-2-thread-10} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:27:22,577] ({pool-2-thread-10} SchedulerFactory.java[jobFinished]:120) - Job 20200119-001021_531760765 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:27:29,818] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:27:32,858] ({qtp2107447833-15} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:27:32,891] ({pool-2-thread-6} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:27:32,892] ({pool-2-thread-6} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-000328_1025815385, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:27:33,348] ({pool-2-thread-6} NotebookServer.java[afterStatusChange]:2314) - Job 20200119-000328_1025815385 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:27:33,361] ({pool-2-thread-6} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:27:33,374] ({pool-2-thread-6} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:27:36,859] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:27:36,878] ({pool-2-thread-11} SchedulerFactory.java[jobStarted]:114) - Job 20200119-001021_531760765 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:27:36,881] ({pool-2-thread-11} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-001021_531760765, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:27:37,696] ({pool-2-thread-11} NotebookServer.java[afterStatusChange]:2316) - Job 20200119-001021_531760765 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mValueError[0mTraceback (most recent call last)
[0;32m<ipython-input-40-9834b0d3ce15>[0m in [0;36m<module>[0;34m()[0m
[1;32m      7[0m [0;34m[0m[0m
[1;32m      8[0m [0;34m[0m[0m
[0;32m----> 9[0;31m [0mdataFrame[0m[0;34m.[0m[0mrdd[0m[0;34m.[0m[0mmapPartitions[0m[0;34m([0m[0msumar[0m[0;34m)[0m[0;34m.[0m[0mtoDF[0m[0;34m([0m[0;34m[[0m[0;34m"result"[0m[0;34m][0m[0;34m)[0m[0;34m.[0m[0mshow[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36mtoDF[0;34m(self, schema, sampleRatio)[0m
[1;32m     56[0m         [0;34m[[0m[0mRow[0m[0;34m([0m[0mname[0m[0;34m=[0m[0;34mu'Alice'[0m[0;34m,[0m [0mage[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m[0;34m][0m[0;34m[0m[0m
[1;32m     57[0m         """
[0;32m---> 58[0;31m         [0;32mreturn[0m [0msparkSession[0m[0;34m.[0m[0mcreateDataFrame[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mschema[0m[0;34m,[0m [0msampleRatio[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     59[0m [0;34m[0m[0m
[1;32m     60[0m     [0mRDD[0m[0;34m.[0m[0mtoDF[0m [0;34m=[0m [0mtoDF[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36mcreateDataFrame[0;34m(self, data, schema, samplingRatio, verifySchema)[0m
[1;32m    744[0m [0;34m[0m[0m
[1;32m    745[0m         [0;32mif[0m [0misinstance[0m[0;34m([0m[0mdata[0m[0;34m,[0m [0mRDD[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 746[0;31m             [0mrdd[0m[0;34m,[0m [0mschema[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_createFromRDD[0m[0;34m([0m[0mdata[0m[0;34m.[0m[0mmap[0m[0;34m([0m[0mprepare[0m[0;34m)[0m[0;34m,[0m [0mschema[0m[0;34m,[0m [0msamplingRatio[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    747[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    748[0m             [0mrdd[0m[0;34m,[0m [0mschema[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_createFromLocal[0m[0;34m([0m[0mmap[0m[0;34m([0m[0mprepare[0m[0;34m,[0m [0mdata[0m[0;34m)[0m[0;34m,[0m [0mschema[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36m_createFromRDD[0;34m(self, rdd, schema, samplingRatio)[0m
[1;32m    388[0m         """
[1;32m    389[0m         [0;32mif[0m [0mschema[0m [0;32mis[0m [0mNone[0m [0;32mor[0m [0misinstance[0m[0;34m([0m[0mschema[0m[0;34m,[0m [0;34m([0m[0mlist[0m[0;34m,[0m [0mtuple[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 390[0;31m             [0mstruct[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_inferSchema[0m[0;34m([0m[0mrdd[0m[0;34m,[0m [0msamplingRatio[0m[0;34m,[0m [0mnames[0m[0;34m=[0m[0mschema[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    391[0m             [0mconverter[0m [0;34m=[0m [0m_create_converter[0m[0;34m([0m[0mstruct[0m[0;34m)[0m[0;34m[0m[0m
[1;32m    392[0m             [0mrdd[0m [0;34m=[0m [0mrdd[0m[0;34m.[0m[0mmap[0m[0;34m([0m[0mconverter[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36m_inferSchema[0;34m(self, rdd, samplingRatio, names)[0m
[1;32m    359[0m         [0;34m:[0m[0;32mreturn[0m[0;34m:[0m [0;34m:[0m[0;32mclass[0m[0;34m:[0m[0;34m`[0m[0mpyspark[0m[0;34m.[0m[0msql[0m[0;34m.[0m[0mtypes[0m[0;34m.[0m[0mStructType[0m[0;34m`[0m[0;34m[0m[0m
[1;32m    360[0m         """
[0;32m--> 361[0;31m         [0mfirst[0m [0;34m=[0m [0mrdd[0m[0;34m.[0m[0mfirst[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    362[0m         [0;32mif[0m [0;32mnot[0m [0mfirst[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    363[0m             raise ValueError("The first row in RDD is empty, "

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py[0m in [0;36mfirst[0;34m(self)[0m
[1;32m   1379[0m         [0;32mif[0m [0mrs[0m[0;34m:[0m[0;34m[0m[0m
[1;32m   1380[0m             [0;32mreturn[0m [0mrs[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m[0m[0m
[0;32m-> 1381[0;31m         [0;32mraise[0m [0mValueError[0m[0;34m([0m[0;34m"RDD is empty"[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1382[0m [0;34m[0m[0m
[1;32m   1383[0m     [0;32mdef[0m [0misEmpty[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m

[0;31mValueError[0m: RDD is empty
 INFO [2020-01-19 00:27:37,759] ({pool-2-thread-11} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:27:37,806] ({pool-2-thread-11} SchedulerFactory.java[jobFinished]:120) - Job 20200119-001021_531760765 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:27:47,597] ({qtp2107447833-15} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:27:47,623] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:27:47,623] ({pool-2-thread-4} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-000328_1025815385, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:27:48,050] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2316) - Job 20200119-000328_1025815385 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mAttributeError[0mTraceback (most recent call last)
[0;32m<ipython-input-42-3fb512590a39>[0m in [0;36m<module>[0;34m()[0m
[1;32m      4[0m [0mdataFrame[0m[0;34m.[0m[0magg[0m[0;34m([0m[0mF[0m[0;34m.[0m[0msum[0m[0;34m([0m[0;34m"nuym"[0m[0;34m)[0m[0;34m)[0m[0;34m.[0m[0mcollect[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[1;32m      5[0m [0;34m[0m[0m
[0;32m----> 6[0;31m [0;32mprint[0m [0mdataFrame[0m[0;34m.[0m[0mrdd[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;31mAttributeError[0m: 'RDD' object has no attribute 'size'
 INFO [2020-01-19 00:27:48,064] ({pool-2-thread-4} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:27:48,076] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:27:52,141] ({qtp2107447833-15} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:27:52,161] ({pool-2-thread-12} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:27:52,166] ({pool-2-thread-12} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-000328_1025815385, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:27:53,280] ({pool-2-thread-12} NotebookServer.java[afterStatusChange]:2314) - Job 20200119-000328_1025815385 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:27:53,303] ({pool-2-thread-12} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:27:53,323] ({pool-2-thread-12} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:28:22,303] ({qtp2107447833-15} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:28:22,328] ({pool-2-thread-7} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:28:22,328] ({pool-2-thread-7} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-000328_1025815385, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:28:22,880] ({pool-2-thread-7} NotebookServer.java[afterStatusChange]:2314) - Job 20200119-000328_1025815385 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:28:22,911] ({pool-2-thread-7} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:28:22,948] ({pool-2-thread-7} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:28:43,425] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:28:43,444] ({pool-2-thread-13} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:28:43,445] ({pool-2-thread-13} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195328_2060007234, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:28:43,535] ({pool-2-thread-13} NotebookServer.java[afterStatusChange]:2314) - Job 20200118-195328_2060007234 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:28:43,554] ({pool-2-thread-13} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:28:43,580] ({pool-2-thread-13} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:28:45,653] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:28:45,670] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:28:45,671] ({pool-2-thread-3} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200118-195328_2060007234, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:28:45,766] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2314) - Job 20200118-195328_2060007234 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:28:45,777] ({pool-2-thread-3} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:28:45,788] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:28:48,368] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:28:48,380] ({pool-2-thread-14} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:28:48,381] ({pool-2-thread-14} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-000328_1025815385, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:28:48,848] ({pool-2-thread-14} NotebookServer.java[afterStatusChange]:2314) - Job 20200119-000328_1025815385 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:28:48,861] ({pool-2-thread-14} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:28:48,873] ({pool-2-thread-14} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:28:51,373] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:28:51,391] ({pool-2-thread-8} SchedulerFactory.java[jobStarted]:114) - Job 20200119-001021_531760765 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:28:51,391] ({pool-2-thread-8} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-001021_531760765, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:28:52,382] ({pool-2-thread-8} NotebookServer.java[afterStatusChange]:2316) - Job 20200119-001021_531760765 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mValueError[0mTraceback (most recent call last)
[0;32m<ipython-input-54-9834b0d3ce15>[0m in [0;36m<module>[0;34m()[0m
[1;32m      7[0m [0;34m[0m[0m
[1;32m      8[0m [0;34m[0m[0m
[0;32m----> 9[0;31m [0mdataFrame[0m[0;34m.[0m[0mrdd[0m[0;34m.[0m[0mmapPartitions[0m[0;34m([0m[0msumar[0m[0;34m)[0m[0;34m.[0m[0mtoDF[0m[0;34m([0m[0;34m[[0m[0;34m"result"[0m[0;34m][0m[0;34m)[0m[0;34m.[0m[0mshow[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36mtoDF[0;34m(self, schema, sampleRatio)[0m
[1;32m     56[0m         [0;34m[[0m[0mRow[0m[0;34m([0m[0mname[0m[0;34m=[0m[0;34mu'Alice'[0m[0;34m,[0m [0mage[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m[0;34m][0m[0;34m[0m[0m
[1;32m     57[0m         """
[0;32m---> 58[0;31m         [0;32mreturn[0m [0msparkSession[0m[0;34m.[0m[0mcreateDataFrame[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mschema[0m[0;34m,[0m [0msampleRatio[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     59[0m [0;34m[0m[0m
[1;32m     60[0m     [0mRDD[0m[0;34m.[0m[0mtoDF[0m [0;34m=[0m [0mtoDF[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36mcreateDataFrame[0;34m(self, data, schema, samplingRatio, verifySchema)[0m
[1;32m    744[0m [0;34m[0m[0m
[1;32m    745[0m         [0;32mif[0m [0misinstance[0m[0;34m([0m[0mdata[0m[0;34m,[0m [0mRDD[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 746[0;31m             [0mrdd[0m[0;34m,[0m [0mschema[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_createFromRDD[0m[0;34m([0m[0mdata[0m[0;34m.[0m[0mmap[0m[0;34m([0m[0mprepare[0m[0;34m)[0m[0;34m,[0m [0mschema[0m[0;34m,[0m [0msamplingRatio[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    747[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    748[0m             [0mrdd[0m[0;34m,[0m [0mschema[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_createFromLocal[0m[0;34m([0m[0mmap[0m[0;34m([0m[0mprepare[0m[0;34m,[0m [0mdata[0m[0;34m)[0m[0;34m,[0m [0mschema[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36m_createFromRDD[0;34m(self, rdd, schema, samplingRatio)[0m
[1;32m    388[0m         """
[1;32m    389[0m         [0;32mif[0m [0mschema[0m [0;32mis[0m [0mNone[0m [0;32mor[0m [0misinstance[0m[0;34m([0m[0mschema[0m[0;34m,[0m [0;34m([0m[0mlist[0m[0;34m,[0m [0mtuple[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 390[0;31m             [0mstruct[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_inferSchema[0m[0;34m([0m[0mrdd[0m[0;34m,[0m [0msamplingRatio[0m[0;34m,[0m [0mnames[0m[0;34m=[0m[0mschema[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    391[0m             [0mconverter[0m [0;34m=[0m [0m_create_converter[0m[0;34m([0m[0mstruct[0m[0;34m)[0m[0;34m[0m[0m
[1;32m    392[0m             [0mrdd[0m [0;34m=[0m [0mrdd[0m[0;34m.[0m[0mmap[0m[0;34m([0m[0mconverter[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36m_inferSchema[0;34m(self, rdd, samplingRatio, names)[0m
[1;32m    359[0m         [0;34m:[0m[0;32mreturn[0m[0;34m:[0m [0;34m:[0m[0;32mclass[0m[0;34m:[0m[0;34m`[0m[0mpyspark[0m[0;34m.[0m[0msql[0m[0;34m.[0m[0mtypes[0m[0;34m.[0m[0mStructType[0m[0;34m`[0m[0;34m[0m[0m
[1;32m    360[0m         """
[0;32m--> 361[0;31m         [0mfirst[0m [0;34m=[0m [0mrdd[0m[0;34m.[0m[0mfirst[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    362[0m         [0;32mif[0m [0;32mnot[0m [0mfirst[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    363[0m             raise ValueError("The first row in RDD is empty, "

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py[0m in [0;36mfirst[0;34m(self)[0m
[1;32m   1379[0m         [0;32mif[0m [0mrs[0m[0;34m:[0m[0;34m[0m[0m
[1;32m   1380[0m             [0;32mreturn[0m [0mrs[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m[0m[0m
[0;32m-> 1381[0;31m         [0;32mraise[0m [0mValueError[0m[0;34m([0m[0;34m"RDD is empty"[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1382[0m [0;34m[0m[0m
[1;32m   1383[0m     [0;32mdef[0m [0misEmpty[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m

[0;31mValueError[0m: RDD is empty
 INFO [2020-01-19 00:28:52,410] ({pool-2-thread-8} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:28:52,443] ({pool-2-thread-8} SchedulerFactory.java[jobFinished]:120) - Job 20200119-001021_531760765 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:29:52,341] ({qtp2107447833-12} NotebookServer.java[broadcastNewParagraph]:688) - Broadcasting paragraph on run call instead of note.
 INFO [2020-01-19 00:29:52,352] ({qtp2107447833-12} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:29:52,366] ({pool-2-thread-15} SchedulerFactory.java[jobStarted]:114) - Job 20200119-002429_1902509145 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:29:52,366] ({pool-2-thread-15} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-002429_1902509145, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:29:52,482] ({pool-2-thread-15} NotebookServer.java[afterStatusChange]:2316) - Job 20200119-002429_1902509145 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mAttributeError[0mTraceback (most recent call last)
[0;32m<ipython-input-56-2291ac08d758>[0m in [0;36m<module>[0;34m()[0m
[1;32m      2[0m     [0;32mprint[0m [0mx[0m[0;34m[0m[0m
[1;32m      3[0m     [0;32mreturn[0m [0mx[0m[0;34m[0m[0m
[0;32m----> 4[0;31m [0mdataFrame[0m[0;34m.[0m[0mmap[0m[0;34m([0m[0maa[0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py[0m in [0;36m__getattr__[0;34m(self, name)[0m
[1;32m   1298[0m         [0;32mif[0m [0mname[0m [0;32mnot[0m [0;32min[0m [0mself[0m[0;34m.[0m[0mcolumns[0m[0;34m:[0m[0;34m[0m[0m
[1;32m   1299[0m             raise AttributeError(
[0;32m-> 1300[0;31m                 "'%s' object has no attribute '%s'" % (self.__class__.__name__, name))
[0m[1;32m   1301[0m         [0mjc[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mapply[0m[0;34m([0m[0mname[0m[0;34m)[0m[0;34m[0m[0m
[1;32m   1302[0m         [0;32mreturn[0m [0mColumn[0m[0;34m([0m[0mjc[0m[0;34m)[0m[0;34m[0m[0m

[0;31mAttributeError[0m: 'DataFrame' object has no attribute 'map'
 INFO [2020-01-19 00:29:52,495] ({pool-2-thread-15} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:29:52,509] ({pool-2-thread-15} SchedulerFactory.java[jobFinished]:120) - Job 20200119-002429_1902509145 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:30:26,781] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:30:26,798] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200119-002429_1902509145 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:30:26,799] ({pool-2-thread-2} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-002429_1902509145, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:30:26,906] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2314) - Job 20200119-002429_1902509145 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:30:26,925] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:30:26,941] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200119-002429_1902509145 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:32:00,338] ({qtp2107447833-14} NotebookServer.java[broadcastNewParagraph]:688) - Broadcasting paragraph on run call instead of note.
 INFO [2020-01-19 00:32:00,354] ({qtp2107447833-14} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:32:00,373] ({pool-2-thread-16} SchedulerFactory.java[jobStarted]:114) - Job 20200119-002952_815014234 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:32:00,373] ({pool-2-thread-16} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-002952_815014234, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:32:00,429] ({pool-2-thread-16} NotebookServer.java[afterStatusChange]:2314) - Job 20200119-002952_815014234 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:32:00,461] ({pool-2-thread-16} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:32:00,473] ({pool-2-thread-16} SchedulerFactory.java[jobFinished]:120) - Job 20200119-002952_815014234 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:32:08,746] ({qtp2107447833-57} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:32:08,769] ({pool-2-thread-9} SchedulerFactory.java[jobStarted]:114) - Job 20200119-002429_1902509145 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:32:08,770] ({pool-2-thread-9} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-002429_1902509145, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:32:08,843] ({pool-2-thread-9} NotebookServer.java[afterStatusChange]:2314) - Job 20200119-002429_1902509145 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:32:08,867] ({pool-2-thread-9} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:32:08,882] ({pool-2-thread-9} SchedulerFactory.java[jobFinished]:120) - Job 20200119-002429_1902509145 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:33:09,247] ({qtp2107447833-16} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:33:09,270] ({pool-2-thread-17} SchedulerFactory.java[jobStarted]:114) - Job 20200119-001021_531760765 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:33:09,274] ({pool-2-thread-17} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-001021_531760765, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 WARN [2020-01-19 00:33:10,289] ({pool-2-thread-17} NotebookServer.java[afterStatusChange]:2316) - Job 20200119-001021_531760765 is finished, status: ERROR, exception: null, result: %text [0;31m[0m
[0;31mValueError[0mTraceback (most recent call last)
[0;32m<ipython-input-64-9834b0d3ce15>[0m in [0;36m<module>[0;34m()[0m
[1;32m      7[0m [0;34m[0m[0m
[1;32m      8[0m [0;34m[0m[0m
[0;32m----> 9[0;31m [0mdataFrame[0m[0;34m.[0m[0mrdd[0m[0;34m.[0m[0mmapPartitions[0m[0;34m([0m[0msumar[0m[0;34m)[0m[0;34m.[0m[0mtoDF[0m[0;34m([0m[0;34m[[0m[0;34m"result"[0m[0;34m][0m[0;34m)[0m[0;34m.[0m[0mshow[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m
[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36mtoDF[0;34m(self, schema, sampleRatio)[0m
[1;32m     56[0m         [0;34m[[0m[0mRow[0m[0;34m([0m[0mname[0m[0;34m=[0m[0;34mu'Alice'[0m[0;34m,[0m [0mage[0m[0;34m=[0m[0;36m1[0m[0;34m)[0m[0;34m][0m[0;34m[0m[0m
[1;32m     57[0m         """
[0;32m---> 58[0;31m         [0;32mreturn[0m [0msparkSession[0m[0;34m.[0m[0mcreateDataFrame[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mschema[0m[0;34m,[0m [0msampleRatio[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m     59[0m [0;34m[0m[0m
[1;32m     60[0m     [0mRDD[0m[0;34m.[0m[0mtoDF[0m [0;34m=[0m [0mtoDF[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36mcreateDataFrame[0;34m(self, data, schema, samplingRatio, verifySchema)[0m
[1;32m    744[0m [0;34m[0m[0m
[1;32m    745[0m         [0;32mif[0m [0misinstance[0m[0;34m([0m[0mdata[0m[0;34m,[0m [0mRDD[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 746[0;31m             [0mrdd[0m[0;34m,[0m [0mschema[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_createFromRDD[0m[0;34m([0m[0mdata[0m[0;34m.[0m[0mmap[0m[0;34m([0m[0mprepare[0m[0;34m)[0m[0;34m,[0m [0mschema[0m[0;34m,[0m [0msamplingRatio[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    747[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    748[0m             [0mrdd[0m[0;34m,[0m [0mschema[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_createFromLocal[0m[0;34m([0m[0mmap[0m[0;34m([0m[0mprepare[0m[0;34m,[0m [0mdata[0m[0;34m)[0m[0;34m,[0m [0mschema[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36m_createFromRDD[0;34m(self, rdd, schema, samplingRatio)[0m
[1;32m    388[0m         """
[1;32m    389[0m         [0;32mif[0m [0mschema[0m [0;32mis[0m [0mNone[0m [0;32mor[0m [0misinstance[0m[0;34m([0m[0mschema[0m[0;34m,[0m [0;34m([0m[0mlist[0m[0;34m,[0m [0mtuple[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m
[0;32m--> 390[0;31m             [0mstruct[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_inferSchema[0m[0;34m([0m[0mrdd[0m[0;34m,[0m [0msamplingRatio[0m[0;34m,[0m [0mnames[0m[0;34m=[0m[0mschema[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    391[0m             [0mconverter[0m [0;34m=[0m [0m_create_converter[0m[0;34m([0m[0mstruct[0m[0;34m)[0m[0;34m[0m[0m
[1;32m    392[0m             [0mrdd[0m [0;34m=[0m [0mrdd[0m[0;34m.[0m[0mmap[0m[0;34m([0m[0mconverter[0m[0;34m)[0m[0;34m[0m[0m

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/session.py[0m in [0;36m_inferSchema[0;34m(self, rdd, samplingRatio, names)[0m
[1;32m    359[0m         [0;34m:[0m[0;32mreturn[0m[0;34m:[0m [0;34m:[0m[0;32mclass[0m[0;34m:[0m[0;34m`[0m[0mpyspark[0m[0;34m.[0m[0msql[0m[0;34m.[0m[0mtypes[0m[0;34m.[0m[0mStructType[0m[0;34m`[0m[0;34m[0m[0m
[1;32m    360[0m         """
[0;32m--> 361[0;31m         [0mfirst[0m [0;34m=[0m [0mrdd[0m[0;34m.[0m[0mfirst[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    362[0m         [0;32mif[0m [0;32mnot[0m [0mfirst[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    363[0m             raise ValueError("The first row in RDD is empty, "

[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py[0m in [0;36mfirst[0;34m(self)[0m
[1;32m   1379[0m         [0;32mif[0m [0mrs[0m[0;34m:[0m[0;34m[0m[0m
[1;32m   1380[0m             [0;32mreturn[0m [0mrs[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m[0m[0m
[0;32m-> 1381[0;31m         [0;32mraise[0m [0mValueError[0m[0;34m([0m[0;34m"RDD is empty"[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m   1382[0m [0;34m[0m[0m
[1;32m   1383[0m     [0;32mdef[0m [0misEmpty[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0m

[0;31mValueError[0m: RDD is empty
 INFO [2020-01-19 00:33:10,312] ({JobProgressPoller, jobId=20200119-001021_531760765} NotebookServer.java[onClose]:372) - Closed connection to 192.168.99.1 : 9813. (1006) WebSocket Write EOF
 INFO [2020-01-19 00:33:10,362] ({pool-2-thread-17} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:33:10,414] ({pool-2-thread-17} SchedulerFactory.java[jobFinished]:120) - Job 20200119-001021_531760765 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:33:11,640] ({qtp2107447833-16} NotebookServer.java[onOpen]:151) - New connection from 192.168.99.1 : 9874
 INFO [2020-01-19 00:33:11,643] ({qtp2107447833-57} NotebookServer.java[sendNote]:828) - New operation from 192.168.99.1 : 9874 : anonymous : GET_NOTE : 2EZEECRFJ
 WARN [2020-01-19 00:33:11,667] ({qtp2107447833-57} GitNotebookRepo.java[revisionHistory]:158) - No Head found for 2EZEECRFJ, No HEAD exists and no explicit starting revision was specified
 WARN [2020-01-19 00:33:11,668] ({qtp2107447833-57} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-19 00:33:11,668] ({qtp2107447833-57} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-19 00:33:11,668] ({qtp2107447833-57} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-19 00:33:11,669] ({qtp2107447833-57} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-19 00:33:11,669] ({qtp2107447833-57} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-01-19 00:33:11,669] ({qtp2107447833-57} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 INFO [2020-01-19 00:33:45,864] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:33:45,887] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:33:45,888] ({pool-2-thread-5} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-000328_1025815385, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:33:46,352] ({pool-2-thread-5} NotebookServer.java[afterStatusChange]:2314) - Job 20200119-000328_1025815385 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:33:46,396] ({pool-2-thread-5} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:33:46,417] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:33:53,335] ({qtp2107447833-11} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:33:53,352] ({pool-2-thread-18} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-01-19 00:33:53,353] ({pool-2-thread-18} Paragraph.java[jobRun]:380) - Run paragraph [paragraph_id: 20200119-000328_1025815385, interpreter: spark.pyspark, note_id: 2EZEECRFJ, user: anonymous]
 INFO [2020-01-19 00:33:53,750] ({pool-2-thread-18} NotebookServer.java[afterStatusChange]:2314) - Job 20200119-000328_1025815385 is finished successfully, status: FINISHED
 INFO [2020-01-19 00:33:53,779] ({pool-2-thread-18} VFSNotebookRepo.java[save]:196) - Saving note:2EZEECRFJ
 INFO [2020-01-19 00:33:53,818] ({pool-2-thread-18} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
