 WARN [2020-01-19 00:02:19,913] ({main} NativeCodeLoader.java[<clinit>]:62) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
 INFO [2020-01-19 00:02:20,183] ({main} RemoteInterpreterServer.java[main]:261) - URL:jar:file:/zeppelin/interpreter/spark/spark-interpreter-0.8.1.jar!/org/apache/zeppelin/interpreter/remote/RemoteInterpreterServer.class
 INFO [2020-01-19 00:02:20,214] ({main} RemoteInterpreterServer.java[<init>]:162) - Launching ThriftServer at 172.18.0.4:35437
 INFO [2020-01-19 00:02:20,217] ({main} RemoteInterpreterServer.java[<init>]:166) - Starting remote interpreter server on port 35437
 INFO [2020-01-19 00:02:20,218] ({Thread-3} RemoteInterpreterServer.java[run]:203) - Starting remote interpreter server on port 35437
 INFO [2020-01-19 00:02:21,223] ({Thread-4} RemoteInterpreterUtils.java[registerInterpreter]:165) - callbackHost: 172.18.0.4, callbackPort: 35275, callbackInfo: CallbackInfo(host:172.18.0.4, port:35437)
 INFO [2020-01-19 00:02:21,449] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.SparkInterpreter
 INFO [2020-01-19 00:02:21,452] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.SparkSqlInterpreter
 INFO [2020-01-19 00:02:21,460] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.DepInterpreter
 INFO [2020-01-19 00:02:21,466] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.PySparkInterpreter
 INFO [2020-01-19 00:02:21,494] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.IPySparkInterpreter
 INFO [2020-01-19 00:02:21,499] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.SparkRInterpreter
 WARN [2020-01-19 00:02:21,656] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:117) - Failed to load configuration, proceeding with a default
 INFO [2020-01-19 00:02:21,693] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:129) - Server Host: 0.0.0.0
 INFO [2020-01-19 00:02:21,693] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:131) - Server Port: 8090
 INFO [2020-01-19 00:02:21,693] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:135) - Context Path: /
 INFO [2020-01-19 00:02:21,700] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:136) - Zeppelin Version: 0.8.1
 INFO [2020-01-19 00:02:21,700] ({pool-1-thread-1} SchedulerFactory.java[<init>]:59) - Scheduler Thread Pool Size: 100
 INFO [2020-01-19 00:02:21,729] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195314_1536993808 started by scheduler interpreter_1122984007
 INFO [2020-01-19 00:02:22,704] ({pool-2-thread-2} IPythonInterpreter.java[checkIPythonPrerequisite]:198) - IPython prerequisite is met
 INFO [2020-01-19 00:02:22,708] ({pool-2-thread-2} NewSparkInterpreter.java[open]:83) - Using Scala Version: 2.11
 INFO [2020-01-19 00:02:31,560] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Running Spark version 2.4.3
 INFO [2020-01-19 00:02:32,061] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Submitted application: Zeppelin
 INFO [2020-01-19 00:02:32,225] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Changing view acls to: root
 INFO [2020-01-19 00:02:32,225] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Changing modify acls to: root
 INFO [2020-01-19 00:02:32,226] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Changing view acls groups to: 
 INFO [2020-01-19 00:02:32,227] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Changing modify acls groups to: 
 INFO [2020-01-19 00:02:32,227] ({pool-2-thread-2} Logging.scala[logInfo]:54) - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
 INFO [2020-01-19 00:02:33,115] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Successfully started service 'sparkDriver' on port 43707.
 INFO [2020-01-19 00:02:33,169] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registering MapOutputTracker
 INFO [2020-01-19 00:02:33,196] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registering BlockManagerMaster
 INFO [2020-01-19 00:02:33,221] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
 INFO [2020-01-19 00:02:33,222] ({pool-2-thread-2} Logging.scala[logInfo]:54) - BlockManagerMasterEndpoint up
 INFO [2020-01-19 00:02:33,239] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Created local directory at /tmp/blockmgr-16138310-3606-47d6-8509-f5e122ceabed
 INFO [2020-01-19 00:02:33,252] ({pool-2-thread-2} Logging.scala[logInfo]:54) - MemoryStore started with capacity 366.3 MB
 INFO [2020-01-19 00:02:33,270] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registering OutputCommitCoordinator
 INFO [2020-01-19 00:02:33,400] ({pool-2-thread-2} Log.java[initialized]:192) - Logging initialized @15871ms
 INFO [2020-01-19 00:02:33,506] ({pool-2-thread-2} Server.java[doStart]:351) - jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
 INFO [2020-01-19 00:02:33,555] ({pool-2-thread-2} Server.java[doStart]:419) - Started @16025ms
 INFO [2020-01-19 00:02:33,581] ({pool-2-thread-2} AbstractConnector.java[doStart]:278) - Started ServerConnector@1fe55f09{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
 INFO [2020-01-19 00:02:33,582] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Successfully started service 'SparkUI' on port 4040.
 INFO [2020-01-19 00:02:33,611] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@3bc7c5e9{/jobs,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:33,613] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@63eea843{/jobs/json,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:33,615] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@795e299a{/jobs/job,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:33,617] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@7b8504f6{/jobs/job/json,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:33,618] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@6d3f1775{/stages,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:33,618] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@65a5f1b1{/stages/json,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:33,619] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@b411fcc{/stages/stage,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:33,622] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@7436046{/stages/stage/json,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:33,625] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@47fb01d{/stages/pool,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:33,628] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@5c50ef5c{/stages/pool/json,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:33,629] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@18746c04{/storage,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:33,630] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@4a97e1c{/storage/json,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:33,631] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@7dafc9cc{/storage/rdd,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:33,632] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@265ee32a{/storage/rdd/json,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:33,633] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@1df1afa9{/environment,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:33,634] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@52676b95{/environment/json,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:33,635] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@518539bc{/executors,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:33,638] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@93d715{/executors/json,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:33,640] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@108fac49{/executors/threadDump,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:33,641] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@795d560e{/executors/threadDump/json,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:33,648] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@1edd840{/static,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:33,649] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@56eddacb{/,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:33,651] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@45489b3a{/api,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:33,653] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@21446f8d{/jobs/job/kill,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:33,655] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@35c0983e{/stages/stage/kill,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:33,658] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Bound SparkUI to 0.0.0.0, and started at http://zeppelin:4040
 INFO [2020-01-19 00:02:33,687] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Added JAR file:/zeppelin/interpreter/spark/spark-interpreter-0.8.1.jar at spark://zeppelin:43707/jars/spark-interpreter-0.8.1.jar with timestamp 1579392153686
 WARN [2020-01-19 00:02:33,687] ({pool-2-thread-2} Logging.scala[logWarning]:66) - The jar /zeppelin/interpreter/spark/spark-interpreter-0.8.1.jar has been added already. Overwriting of added jars is not supported in the current version.
 INFO [2020-01-19 00:02:33,879] ({appclient-register-master-threadpool-0} Logging.scala[logInfo]:54) - Connecting to master spark://spark-master:7077...
 INFO [2020-01-19 00:02:34,049] ({netty-rpc-connection-0} TransportClientFactory.java[createClient]:267) - Successfully created connection to spark-master/172.18.0.3:7077 after 35 ms (0 ms spent in bootstraps)
 INFO [2020-01-19 00:02:34,218] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Connected to Spark cluster with app ID app-20200119000234-0000
 INFO [2020-01-19 00:02:34,229] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34711.
 INFO [2020-01-19 00:02:34,230] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Server created on zeppelin:34711
 INFO [2020-01-19 00:02:34,240] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
 INFO [2020-01-19 00:02:34,299] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Executor added: app-20200119000234-0000/0 on worker-20200118235932-172.18.0.6-35037 (172.18.0.6:35037) with 2 core(s)
 INFO [2020-01-19 00:02:34,311] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Granted executor ID app-20200119000234-0000/0 on hostPort 172.18.0.6:35037 with 2 core(s), 2.0 GB RAM
 INFO [2020-01-19 00:02:34,314] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Executor added: app-20200119000234-0000/1 on worker-20200118235932-172.18.0.7-43649 (172.18.0.7:43649) with 2 core(s)
 INFO [2020-01-19 00:02:34,317] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Granted executor ID app-20200119000234-0000/1 on hostPort 172.18.0.7:43649 with 2 core(s), 2.0 GB RAM
 INFO [2020-01-19 00:02:34,318] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Executor added: app-20200119000234-0000/2 on worker-20200118235932-172.18.0.8-44607 (172.18.0.8:44607) with 2 core(s)
 INFO [2020-01-19 00:02:34,341] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Granted executor ID app-20200119000234-0000/2 on hostPort 172.18.0.8:44607 with 2 core(s), 2.0 GB RAM
 INFO [2020-01-19 00:02:34,356] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registering BlockManager BlockManagerId(driver, zeppelin, 34711, None)
 INFO [2020-01-19 00:02:34,384] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Registering block manager zeppelin:34711 with 366.3 MB RAM, BlockManagerId(driver, zeppelin, 34711, None)
 INFO [2020-01-19 00:02:34,389] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registered BlockManager BlockManagerId(driver, zeppelin, 34711, None)
 INFO [2020-01-19 00:02:34,390] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Initialized BlockManager: BlockManagerId(driver, zeppelin, 34711, None)
 INFO [2020-01-19 00:02:34,615] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Executor updated: app-20200119000234-0000/0 is now RUNNING
 INFO [2020-01-19 00:02:34,799] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Executor updated: app-20200119000234-0000/1 is now RUNNING
 INFO [2020-01-19 00:02:34,804] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Executor updated: app-20200119000234-0000/2 is now RUNNING
 INFO [2020-01-19 00:02:35,463] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@78f3a7ec{/metrics/json,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:35,527] ({pool-2-thread-2} Logging.scala[logInfo]:54) - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
 INFO [2020-01-19 00:02:48,201] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.8:53782) with ID 2
 INFO [2020-01-19 00:02:48,416] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.7:53826) with ID 1
 INFO [2020-01-19 00:02:48,586] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:32928) with ID 0
 INFO [2020-01-19 00:02:48,839] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Registering block manager 172.18.0.8:45595 with 912.3 MB RAM, BlockManagerId(2, 172.18.0.8, 45595, None)
 INFO [2020-01-19 00:02:49,053] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Registering block manager 172.18.0.7:43081 with 912.3 MB RAM, BlockManagerId(1, 172.18.0.7, 43081, None)
 INFO [2020-01-19 00:02:49,235] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Registering block manager 172.18.0.6:40877 with 912.3 MB RAM, BlockManagerId(0, 172.18.0.6, 40877, None)
 INFO [2020-01-19 00:02:51,891] ({pool-2-thread-2} SparkShims.java[loadShims]:62) - Initializing shims for Spark 2.x
 INFO [2020-01-19 00:02:52,205] ({pool-2-thread-2} IPythonInterpreter.java[setAdditionalPythonPath]:103) - setAdditionalPythonPath: /usr/local/spark/python/lib/pyspark.zip:/usr/local/spark/python/lib/py4j-0.10.7-src.zip:/zeppelin/interpreter/lib/python
 INFO [2020-01-19 00:02:52,207] ({pool-2-thread-2} IPythonInterpreter.java[open]:135) - Python Exec: python
 INFO [2020-01-19 00:02:52,630] ({pool-2-thread-2} IPythonInterpreter.java[checkIPythonPrerequisite]:198) - IPython prerequisite is met
 INFO [2020-01-19 00:02:52,632] ({pool-2-thread-2} IPythonInterpreter.java[open]:146) - Launching IPython Kernel at port: 33953
 INFO [2020-01-19 00:02:52,632] ({pool-2-thread-2} IPythonInterpreter.java[open]:147) - Launching JVM Gateway at port: 37855
 INFO [2020-01-19 00:02:52,805] ({pool-2-thread-2} IPythonInterpreter.java[setupIPythonEnv]:319) - PYTHONPATH:/usr/local/spark/python/lib/pyspark.zip:/usr/local/spark/python/lib/py4j-0.10.7-src.zip:/zeppelin/interpreter/lib/python:/usr/local/spark//python/lib/py4j-0.10.7-src.zip:/usr/local/spark//python/:
 INFO [2020-01-19 00:02:53,204] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:297) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:02:53,306] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:297) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:02:53,407] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:297) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:02:53,508] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:297) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:02:53,609] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:297) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:02:53,710] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:297) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:02:53,810] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:297) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:02:53,911] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:297) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:02:54,012] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:297) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:02:54,113] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:297) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:02:54,252] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:293) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:02:54,357] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:293) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:02:54,463] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:293) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:02:54,571] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:293) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:02:54,677] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:293) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:02:54,784] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:293) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:02:54,889] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:293) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:02:55,000] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:290) - IPython Kernel is Running
 INFO [2020-01-19 00:02:55,000] ({pool-2-thread-2} Py4JUtils.java[createGatewayServer]:44) - Launching GatewayServer at 127.0.0.1:37855
 INFO [2020-01-19 00:02:56,058] ({pool-2-thread-2} PySparkInterpreter.java[open]:130) - IPython is available, Use IPySparkInterpreter to replace PySparkInterpreter
 INFO [2020-01-19 00:02:56,159] ({Thread-20} Logging.scala[logInfo]:54) - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/zeppelin/spark-warehouse').
 INFO [2020-01-19 00:02:56,160] ({Thread-20} Logging.scala[logInfo]:54) - Warehouse path is 'file:/zeppelin/spark-warehouse'.
 INFO [2020-01-19 00:02:56,172] ({Thread-20} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@721c9b35{/SQL,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:56,173] ({Thread-20} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@24a0a853{/SQL/json,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:56,175] ({Thread-20} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@5406bdf8{/SQL/execution,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:56,176] ({Thread-20} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@196516f7{/SQL/execution/json,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:56,178] ({Thread-20} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@7d91136a{/static/sql,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:02:56,759] ({Thread-20} Logging.scala[logInfo]:54) - Registered StateStoreCoordinator endpoint
 INFO [2020-01-19 00:02:56,796] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195314_1536993808 finished by scheduler interpreter_1122984007
 INFO [2020-01-19 00:03:28,259] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler interpreter_1593196443
 INFO [2020-01-19 00:03:28,285] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler interpreter_1593196443
 INFO [2020-01-19 00:03:37,909] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler interpreter_1122984007
 INFO [2020-01-19 00:03:39,602] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler interpreter_1122984007
 INFO [2020-01-19 00:05:57,726] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler interpreter_1122984007
 INFO [2020-01-19 00:05:57,780] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler interpreter_1122984007
 INFO [2020-01-19 00:06:05,849] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler interpreter_1122984007
 INFO [2020-01-19 00:06:05,908] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler interpreter_1122984007
 INFO [2020-01-19 00:06:10,477] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler interpreter_1122984007
 INFO [2020-01-19 00:06:10,654] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler interpreter_1122984007
 INFO [2020-01-19 00:06:22,335] ({pool-2-thread-6} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler interpreter_1122984007
 INFO [2020-01-19 00:06:22,466] ({pool-2-thread-6} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler interpreter_1122984007
 INFO [2020-01-19 00:06:33,003] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler interpreter_1122984007
 INFO [2020-01-19 00:06:33,120] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler interpreter_1122984007
 INFO [2020-01-19 00:06:51,695] ({pool-2-thread-7} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler interpreter_1122984007
 INFO [2020-01-19 00:06:51,766] ({pool-2-thread-7} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler interpreter_1122984007
 INFO [2020-01-19 00:06:54,711] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler interpreter_1122984007
 INFO [2020-01-19 00:06:54,758] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler interpreter_1122984007
 INFO [2020-01-19 00:07:02,408] ({pool-2-thread-8} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler interpreter_1122984007
 INFO [2020-01-19 00:07:02,524] ({pool-2-thread-8} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler interpreter_1122984007
 INFO [2020-01-19 00:07:12,778] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler interpreter_1122984007
 INFO [2020-01-19 00:07:12,838] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler interpreter_1122984007
 INFO [2020-01-19 00:07:24,786] ({pool-2-thread-9} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler interpreter_1122984007
 INFO [2020-01-19 00:07:24,847] ({pool-2-thread-9} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler interpreter_1122984007
 INFO [2020-01-19 00:07:27,373] ({pool-2-thread-6} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler interpreter_1122984007
 INFO [2020-01-19 00:07:27,430] ({pool-2-thread-6} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler interpreter_1122984007
 INFO [2020-01-19 00:09:09,760] ({pool-2-thread-10} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler interpreter_1593196443
 INFO [2020-01-19 00:09:09,767] ({pool-2-thread-10} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler interpreter_1593196443
 INFO [2020-01-19 00:09:25,653] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler interpreter_1593196443
 INFO [2020-01-19 00:09:25,674] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler interpreter_1593196443
 INFO [2020-01-19 00:09:56,544] ({pool-2-thread-11} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler interpreter_1593196443
 INFO [2020-01-19 00:09:56,560] ({pool-2-thread-11} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler interpreter_1593196443
 INFO [2020-01-19 00:09:57,727] ({pool-2-thread-7} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler interpreter_1593196443
 INFO [2020-01-19 00:09:57,734] ({pool-2-thread-7} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler interpreter_1593196443
 INFO [2020-01-19 00:10:41,462] ({pool-2-thread-12} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler interpreter_1593196443
 INFO [2020-01-19 00:10:41,473] ({pool-2-thread-12} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler interpreter_1593196443
 INFO [2020-01-19 00:10:53,358] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler interpreter_1593196443
 INFO [2020-01-19 00:10:53,366] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler interpreter_1593196443
 INFO [2020-01-19 00:10:58,040] ({pool-2-thread-13} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler interpreter_1593196443
 INFO [2020-01-19 00:10:58,048] ({pool-2-thread-13} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler interpreter_1593196443
 INFO [2020-01-19 00:11:06,637] ({pool-2-thread-8} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler interpreter_1122984007
 INFO [2020-01-19 00:11:06,767] ({pool-2-thread-8} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler interpreter_1122984007
 INFO [2020-01-19 00:11:25,809] ({pool-2-thread-14} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler interpreter_1122984007
 INFO [2020-01-19 00:11:26,782] ({Thread-20} Logging.scala[logInfo]:54) - Code generated in 371.296279 ms
 INFO [2020-01-19 00:11:26,909] ({Thread-20} Logging.scala[logInfo]:54) - Code generated in 50.384099 ms
 INFO [2020-01-19 00:11:26,979] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: showString at NativeMethodAccessorImpl.java:0
 INFO [2020-01-19 00:11:27,002] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
 INFO [2020-01-19 00:11:27,003] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)
 INFO [2020-01-19 00:11:27,003] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-01-19 00:11:27,005] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-01-19 00:11:27,011] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 0 (MapPartitionsRDD[23] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
 INFO [2020-01-19 00:11:27,180] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_0 stored as values in memory (estimated size 11.1 KB, free 366.3 MB)
 INFO [2020-01-19 00:11:27,205] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.9 KB, free 366.3 MB)
 INFO [2020-01-19 00:11:27,207] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_0_piece0 in memory on zeppelin:34711 (size: 5.9 KB, free: 366.3 MB)
 INFO [2020-01-19 00:11:27,212] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 0 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:11:27,223] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[23] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
 INFO [2020-01-19 00:11:27,224] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 0.0 with 1 tasks
 INFO [2020-01-19 00:11:27,266] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 0.0 (TID 0, 172.18.0.8, executor 2, partition 0, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:11:28,081] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_0_piece0 in memory on 172.18.0.8:45595 (size: 5.9 KB, free: 912.3 MB)
 INFO [2020-01-19 00:11:30,436] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 0.0 (TID 0) in 3195 ms on 172.18.0.8 (executor 2) (1/1)
 INFO [2020-01-19 00:11:30,441] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Connected to AccumulatorServer at host: 127.0.0.1 port: 49013
 INFO [2020-01-19 00:11:30,439] ({task-result-getter-0} Logging.scala[logInfo]:54) - Removed TaskSet 0.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:11:30,460] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 3.410 s
 INFO [2020-01-19 00:11:30,472] ({Thread-20} Logging.scala[logInfo]:54) - Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 3.486511 s
 INFO [2020-01-19 00:11:30,487] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: showString at NativeMethodAccessorImpl.java:0
 INFO [2020-01-19 00:11:30,488] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 4 output partitions
 INFO [2020-01-19 00:11:30,488] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
 INFO [2020-01-19 00:11:30,488] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-01-19 00:11:30,489] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-01-19 00:11:30,490] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 1 (MapPartitionsRDD[23] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
 INFO [2020-01-19 00:11:30,498] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_1 stored as values in memory (estimated size 11.1 KB, free 366.3 MB)
 INFO [2020-01-19 00:11:30,500] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.0 KB, free 366.3 MB)
 INFO [2020-01-19 00:11:30,503] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_1_piece0 in memory on zeppelin:34711 (size: 6.0 KB, free: 366.3 MB)
 INFO [2020-01-19 00:11:30,504] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 1 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:11:30,505] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[23] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1, 2, 3, 4))
 INFO [2020-01-19 00:11:30,505] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 1.0 with 4 tasks
 INFO [2020-01-19 00:11:30,506] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 1.0 (TID 1, 172.18.0.7, executor 1, partition 1, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:11:30,506] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 1.0 in stage 1.0 (TID 2, 172.18.0.6, executor 0, partition 2, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:11:30,507] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 2.0 in stage 1.0 (TID 3, 172.18.0.8, executor 2, partition 3, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:11:30,508] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 3.0 in stage 1.0 (TID 4, 172.18.0.7, executor 1, partition 4, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:11:30,592] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_1_piece0 in memory on 172.18.0.8:45595 (size: 6.0 KB, free: 912.3 MB)
 INFO [2020-01-19 00:11:30,715] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 2.0 in stage 1.0 (TID 3) in 208 ms on 172.18.0.8 (executor 2) (1/4)
 INFO [2020-01-19 00:11:31,829] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_1_piece0 in memory on 172.18.0.6:40877 (size: 6.0 KB, free: 912.3 MB)
 INFO [2020-01-19 00:11:31,912] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_1_piece0 in memory on 172.18.0.7:43081 (size: 6.0 KB, free: 912.3 MB)
 INFO [2020-01-19 00:11:35,172] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 3.0 in stage 1.0 (TID 4) in 4664 ms on 172.18.0.7 (executor 1) (2/4)
 INFO [2020-01-19 00:11:35,175] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 1.0 (TID 1) in 4669 ms on 172.18.0.7 (executor 1) (3/4)
 INFO [2020-01-19 00:11:35,267] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 1.0 in stage 1.0 (TID 2) in 4761 ms on 172.18.0.6 (executor 0) (4/4)
 INFO [2020-01-19 00:11:35,267] ({task-result-getter-0} Logging.scala[logInfo]:54) - Removed TaskSet 1.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:11:35,268] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 4.776 s
 INFO [2020-01-19 00:11:35,271] ({Thread-20} Logging.scala[logInfo]:54) - Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 4.783976 s
 INFO [2020-01-19 00:11:35,273] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: showString at NativeMethodAccessorImpl.java:0
 INFO [2020-01-19 00:11:35,274] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
 INFO [2020-01-19 00:11:35,275] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)
 INFO [2020-01-19 00:11:35,275] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-01-19 00:11:35,276] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-01-19 00:11:35,276] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 2 (MapPartitionsRDD[23] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
 INFO [2020-01-19 00:11:35,279] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_2 stored as values in memory (estimated size 11.1 KB, free 366.3 MB)
 INFO [2020-01-19 00:11:35,285] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.0 KB, free 366.2 MB)
 INFO [2020-01-19 00:11:35,291] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_2_piece0 in memory on zeppelin:34711 (size: 6.0 KB, free: 366.3 MB)
 INFO [2020-01-19 00:11:35,291] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 2 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:11:35,292] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[23] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(5))
 INFO [2020-01-19 00:11:35,292] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 2.0 with 1 tasks
 INFO [2020-01-19 00:11:35,293] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 2.0 (TID 5, 172.18.0.8, executor 2, partition 5, PROCESS_LOCAL, 7920 bytes)
 INFO [2020-01-19 00:11:35,328] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_2_piece0 in memory on 172.18.0.8:45595 (size: 6.0 KB, free: 912.3 MB)
 INFO [2020-01-19 00:11:35,398] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 2.0 (TID 5) in 105 ms on 172.18.0.8 (executor 2) (1/1)
 INFO [2020-01-19 00:11:35,398] ({task-result-getter-1} Logging.scala[logInfo]:54) - Removed TaskSet 2.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:11:35,399] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.121 s
 INFO [2020-01-19 00:11:35,423] ({Thread-20} Logging.scala[logInfo]:54) - Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.149195 s
 INFO [2020-01-19 00:11:35,506] ({pool-2-thread-14} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler interpreter_1122984007
 INFO [2020-01-19 00:12:06,392] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler interpreter_1122984007
 INFO [2020-01-19 00:12:06,541] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler interpreter_1122984007
 INFO [2020-01-19 00:12:12,526] ({pool-2-thread-15} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler interpreter_1122984007
 INFO [2020-01-19 00:12:12,608] ({pool-2-thread-15} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler interpreter_1122984007
 INFO [2020-01-19 00:12:18,234] ({pool-2-thread-9} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler interpreter_1122984007
 INFO [2020-01-19 00:12:18,323] ({pool-2-thread-9} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler interpreter_1122984007
 INFO [2020-01-19 00:12:20,429] ({pool-2-thread-16} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler interpreter_1122984007
 INFO [2020-01-19 00:12:20,666] ({Thread-20} Logging.scala[logInfo]:54) - Code generated in 43.864304 ms
 INFO [2020-01-19 00:12:20,694] ({Thread-20} Logging.scala[logInfo]:54) - Code generated in 20.355774 ms
 INFO [2020-01-19 00:12:20,732] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: count at NativeMethodAccessorImpl.java:0
 INFO [2020-01-19 00:12:20,734] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Registering RDD 26 (count at NativeMethodAccessorImpl.java:0)
 INFO [2020-01-19 00:12:20,736] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 3 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
 INFO [2020-01-19 00:12:20,736] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 4 (count at NativeMethodAccessorImpl.java:0)
 INFO [2020-01-19 00:12:20,736] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List(ShuffleMapStage 3)
 INFO [2020-01-19 00:12:20,737] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List(ShuffleMapStage 3)
 INFO [2020-01-19 00:12:20,743] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ShuffleMapStage 3 (MapPartitionsRDD[26] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
 INFO [2020-01-19 00:12:20,757] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_3 stored as values in memory (estimated size 12.2 KB, free 366.2 MB)
 INFO [2020-01-19 00:12:20,758] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.6 KB, free 366.2 MB)
 INFO [2020-01-19 00:12:20,759] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_3_piece0 in memory on zeppelin:34711 (size: 6.6 KB, free: 366.3 MB)
 INFO [2020-01-19 00:12:20,760] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 3 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:12:20,763] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 6 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[26] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
 INFO [2020-01-19 00:12:20,764] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 3.0 with 6 tasks
 INFO [2020-01-19 00:12:20,765] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 3.0 (TID 6, 172.18.0.8, executor 2, partition 0, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:12:20,766] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 1.0 in stage 3.0 (TID 7, 172.18.0.7, executor 1, partition 1, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:12:20,766] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 2.0 in stage 3.0 (TID 8, 172.18.0.6, executor 0, partition 2, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:12:20,767] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 3.0 in stage 3.0 (TID 9, 172.18.0.8, executor 2, partition 3, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:12:20,768] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 4.0 in stage 3.0 (TID 10, 172.18.0.7, executor 1, partition 4, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:12:20,768] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 5.0 in stage 3.0 (TID 11, 172.18.0.6, executor 0, partition 5, PROCESS_LOCAL, 7909 bytes)
 INFO [2020-01-19 00:12:20,817] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_3_piece0 in memory on 172.18.0.6:40877 (size: 6.6 KB, free: 912.3 MB)
 INFO [2020-01-19 00:12:20,832] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_3_piece0 in memory on 172.18.0.7:43081 (size: 6.6 KB, free: 912.3 MB)
 INFO [2020-01-19 00:12:20,844] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_3_piece0 in memory on 172.18.0.8:45595 (size: 6.6 KB, free: 912.3 MB)
 INFO [2020-01-19 00:12:21,172] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 1.0 in stage 3.0 (TID 7) in 406 ms on 172.18.0.7 (executor 1) (1/6)
 INFO [2020-01-19 00:12:21,207] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 4.0 in stage 3.0 (TID 10) in 440 ms on 172.18.0.7 (executor 1) (2/6)
 INFO [2020-01-19 00:12:21,270] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 2.0 in stage 3.0 (TID 8) in 504 ms on 172.18.0.6 (executor 0) (3/6)
 INFO [2020-01-19 00:12:21,275] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 5.0 in stage 3.0 (TID 11) in 507 ms on 172.18.0.6 (executor 0) (4/6)
 INFO [2020-01-19 00:12:21,281] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 3.0 (TID 6) in 516 ms on 172.18.0.8 (executor 2) (5/6)
 INFO [2020-01-19 00:12:21,304] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 3.0 in stage 3.0 (TID 9) in 537 ms on 172.18.0.8 (executor 2) (6/6)
 INFO [2020-01-19 00:12:21,305] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 3.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:12:21,306] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ShuffleMapStage 3 (count at NativeMethodAccessorImpl.java:0) finished in 0.557 s
 INFO [2020-01-19 00:12:21,308] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - looking for newly runnable stages
 INFO [2020-01-19 00:12:21,312] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - running: Set()
 INFO [2020-01-19 00:12:21,313] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - waiting: Set(ResultStage 4)
 INFO [2020-01-19 00:12:21,315] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - failed: Set()
 INFO [2020-01-19 00:12:21,321] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 4 (MapPartitionsRDD[29] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
 INFO [2020-01-19 00:12:21,337] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_4 stored as values in memory (estimated size 7.1 KB, free 366.2 MB)
 INFO [2020-01-19 00:12:21,344] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 366.2 MB)
 INFO [2020-01-19 00:12:21,347] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_4_piece0 in memory on zeppelin:34711 (size: 3.8 KB, free: 366.3 MB)
 INFO [2020-01-19 00:12:21,350] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 4 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:12:21,356] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[29] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
 INFO [2020-01-19 00:12:21,356] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 4.0 with 1 tasks
 INFO [2020-01-19 00:12:21,361] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 4.0 (TID 12, 172.18.0.7, executor 1, partition 0, NODE_LOCAL, 7771 bytes)
 INFO [2020-01-19 00:12:21,402] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_4_piece0 in memory on 172.18.0.7:43081 (size: 3.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:12:21,455] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Asked to send map output locations for shuffle 0 to 172.18.0.7:53826
 INFO [2020-01-19 00:12:21,664] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 4.0 (TID 12) in 305 ms on 172.18.0.7 (executor 1) (1/1)
 INFO [2020-01-19 00:12:21,665] ({task-result-getter-0} Logging.scala[logInfo]:54) - Removed TaskSet 4.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:12:21,666] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 4 (count at NativeMethodAccessorImpl.java:0) finished in 0.332 s
 INFO [2020-01-19 00:12:21,669] ({Thread-20} Logging.scala[logInfo]:54) - Job 3 finished: count at NativeMethodAccessorImpl.java:0, took 0.936310 s
 INFO [2020-01-19 00:12:21,714] ({pool-2-thread-16} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler interpreter_1122984007
 INFO [2020-01-19 00:12:26,684] ({pool-2-thread-6} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler interpreter_1122984007
 INFO [2020-01-19 00:12:26,790] ({pool-2-thread-6} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler interpreter_1122984007
 INFO [2020-01-19 00:13:09,432] ({pool-2-thread-17} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler interpreter_1122984007
 INFO [2020-01-19 00:13:09,518] ({pool-2-thread-17} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler interpreter_1122984007
 INFO [2020-01-19 00:13:29,182] ({pool-2-thread-10} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler interpreter_1122984007
 INFO [2020-01-19 00:13:29,261] ({pool-2-thread-10} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler interpreter_1122984007
 INFO [2020-01-19 00:13:37,883] ({pool-2-thread-18} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler interpreter_1122984007
 INFO [2020-01-19 00:13:37,970] ({pool-2-thread-18} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler interpreter_1122984007
 INFO [2020-01-19 00:15:34,307] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler interpreter_1122984007
 INFO [2020-01-19 00:15:34,535] ({Thread-20} Logging.scala[logInfo]:54) - Code generated in 48.232548 ms
 INFO [2020-01-19 00:15:34,594] ({Thread-20} Logging.scala[logInfo]:54) - Code generated in 43.660935 ms
 INFO [2020-01-19 00:15:34,614] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: collect at <ipython-input-50-4294447afb77>:4
 INFO [2020-01-19 00:15:34,615] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Registering RDD 32 (collect at <ipython-input-50-4294447afb77>:4)
 INFO [2020-01-19 00:15:34,615] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 4 (collect at <ipython-input-50-4294447afb77>:4) with 1 output partitions
 INFO [2020-01-19 00:15:34,616] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 6 (collect at <ipython-input-50-4294447afb77>:4)
 INFO [2020-01-19 00:15:34,616] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List(ShuffleMapStage 5)
 INFO [2020-01-19 00:15:34,616] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List(ShuffleMapStage 5)
 INFO [2020-01-19 00:15:34,617] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ShuffleMapStage 5 (MapPartitionsRDD[32] at collect at <ipython-input-50-4294447afb77>:4), which has no missing parents
 INFO [2020-01-19 00:15:34,622] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_5 stored as values in memory (estimated size 13.6 KB, free 366.2 MB)
 INFO [2020-01-19 00:15:34,624] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.2 KB, free 366.2 MB)
 INFO [2020-01-19 00:15:34,626] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_5_piece0 in memory on zeppelin:34711 (size: 7.2 KB, free: 366.3 MB)
 INFO [2020-01-19 00:15:34,627] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 5 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:15:34,628] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 6 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[32] at collect at <ipython-input-50-4294447afb77>:4) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
 INFO [2020-01-19 00:15:34,628] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 5.0 with 6 tasks
 INFO [2020-01-19 00:15:34,629] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 5.0 (TID 13, 172.18.0.7, executor 1, partition 0, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:15:34,630] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 1.0 in stage 5.0 (TID 14, 172.18.0.6, executor 0, partition 1, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:15:34,631] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 2.0 in stage 5.0 (TID 15, 172.18.0.8, executor 2, partition 2, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:15:34,632] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 3.0 in stage 5.0 (TID 16, 172.18.0.7, executor 1, partition 3, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:15:34,632] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 4.0 in stage 5.0 (TID 17, 172.18.0.6, executor 0, partition 4, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:15:34,633] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 5.0 in stage 5.0 (TID 18, 172.18.0.8, executor 2, partition 5, PROCESS_LOCAL, 7909 bytes)
 INFO [2020-01-19 00:15:34,699] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_5_piece0 in memory on 172.18.0.7:43081 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:15:34,744] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_5_piece0 in memory on 172.18.0.6:40877 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:15:34,918] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_5_piece0 in memory on 172.18.0.8:45595 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:15:34,970] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 5.0 (TID 13) in 341 ms on 172.18.0.7 (executor 1) (1/6)
 INFO [2020-01-19 00:15:34,991] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 3.0 in stage 5.0 (TID 16) in 360 ms on 172.18.0.7 (executor 1) (2/6)
 INFO [2020-01-19 00:15:35,036] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 4.0 in stage 5.0 (TID 17) in 404 ms on 172.18.0.6 (executor 0) (3/6)
 INFO [2020-01-19 00:15:35,038] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 1.0 in stage 5.0 (TID 14) in 408 ms on 172.18.0.6 (executor 0) (4/6)
 INFO [2020-01-19 00:15:35,131] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 2.0 in stage 5.0 (TID 15) in 500 ms on 172.18.0.8 (executor 2) (5/6)
 INFO [2020-01-19 00:15:35,148] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 5.0 in stage 5.0 (TID 18) in 516 ms on 172.18.0.8 (executor 2) (6/6)
 INFO [2020-01-19 00:15:35,152] ({task-result-getter-2} Logging.scala[logInfo]:54) - Removed TaskSet 5.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:15:35,153] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ShuffleMapStage 5 (collect at <ipython-input-50-4294447afb77>:4) finished in 0.534 s
 INFO [2020-01-19 00:15:35,153] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - looking for newly runnable stages
 INFO [2020-01-19 00:15:35,153] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - running: Set()
 INFO [2020-01-19 00:15:35,154] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - waiting: Set(ResultStage 6)
 INFO [2020-01-19 00:15:35,154] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - failed: Set()
 INFO [2020-01-19 00:15:35,155] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 6 (MapPartitionsRDD[35] at collect at <ipython-input-50-4294447afb77>:4), which has no missing parents
 INFO [2020-01-19 00:15:35,157] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_6 stored as values in memory (estimated size 8.1 KB, free 366.2 MB)
 INFO [2020-01-19 00:15:35,160] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.3 KB, free 366.2 MB)
 INFO [2020-01-19 00:15:35,165] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_6_piece0 in memory on zeppelin:34711 (size: 4.3 KB, free: 366.3 MB)
 INFO [2020-01-19 00:15:35,166] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 6 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:15:35,167] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[35] at collect at <ipython-input-50-4294447afb77>:4) (first 15 tasks are for partitions Vector(0))
 INFO [2020-01-19 00:15:35,167] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 6.0 with 1 tasks
 INFO [2020-01-19 00:15:35,176] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 6.0 (TID 19, 172.18.0.8, executor 2, partition 0, NODE_LOCAL, 7771 bytes)
 INFO [2020-01-19 00:15:35,213] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_6_piece0 in memory on 172.18.0.8:45595 (size: 4.3 KB, free: 912.3 MB)
 INFO [2020-01-19 00:15:35,237] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Asked to send map output locations for shuffle 1 to 172.18.0.8:53782
 INFO [2020-01-19 00:15:35,389] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 6.0 (TID 19) in 213 ms on 172.18.0.8 (executor 2) (1/1)
 INFO [2020-01-19 00:15:35,390] ({task-result-getter-0} Logging.scala[logInfo]:54) - Removed TaskSet 6.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:15:35,391] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 6 (collect at <ipython-input-50-4294447afb77>:4) finished in 0.234 s
 INFO [2020-01-19 00:15:35,391] ({Thread-20} Logging.scala[logInfo]:54) - Job 4 finished: collect at <ipython-input-50-4294447afb77>:4, took 0.776529 s
 INFO [2020-01-19 00:15:35,418] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler interpreter_1122984007
 INFO [2020-01-19 00:15:48,841] ({pool-2-thread-19} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler interpreter_1122984007
 INFO [2020-01-19 00:15:48,963] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: collect at <ipython-input-52-4294447afb77>:4
 INFO [2020-01-19 00:15:48,967] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Registering RDD 38 (collect at <ipython-input-52-4294447afb77>:4)
 INFO [2020-01-19 00:15:48,967] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 5 (collect at <ipython-input-52-4294447afb77>:4) with 1 output partitions
 INFO [2020-01-19 00:15:48,967] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 8 (collect at <ipython-input-52-4294447afb77>:4)
 INFO [2020-01-19 00:15:48,967] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List(ShuffleMapStage 7)
 INFO [2020-01-19 00:15:48,968] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List(ShuffleMapStage 7)
 INFO [2020-01-19 00:15:48,968] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ShuffleMapStage 7 (MapPartitionsRDD[38] at collect at <ipython-input-52-4294447afb77>:4), which has no missing parents
 INFO [2020-01-19 00:15:48,974] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_7 stored as values in memory (estimated size 13.6 KB, free 366.2 MB)
 INFO [2020-01-19 00:15:48,976] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.2 KB, free 366.2 MB)
 INFO [2020-01-19 00:15:48,976] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_7_piece0 in memory on zeppelin:34711 (size: 7.2 KB, free: 366.3 MB)
 INFO [2020-01-19 00:15:48,976] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 7 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:15:48,977] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 6 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[38] at collect at <ipython-input-52-4294447afb77>:4) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
 INFO [2020-01-19 00:15:48,978] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 7.0 with 6 tasks
 INFO [2020-01-19 00:15:48,991] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 7.0 (TID 20, 172.18.0.7, executor 1, partition 0, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:15:48,992] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 1.0 in stage 7.0 (TID 21, 172.18.0.6, executor 0, partition 1, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:15:48,992] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 2.0 in stage 7.0 (TID 22, 172.18.0.8, executor 2, partition 2, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:15:48,993] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 3.0 in stage 7.0 (TID 23, 172.18.0.7, executor 1, partition 3, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:15:48,994] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 4.0 in stage 7.0 (TID 24, 172.18.0.6, executor 0, partition 4, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:15:48,995] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 5.0 in stage 7.0 (TID 25, 172.18.0.8, executor 2, partition 5, PROCESS_LOCAL, 7909 bytes)
 INFO [2020-01-19 00:15:49,049] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_7_piece0 in memory on 172.18.0.7:43081 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:15:49,063] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_7_piece0 in memory on 172.18.0.6:40877 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:15:49,077] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_7_piece0 in memory on 172.18.0.8:45595 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:15:49,157] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 3.0 in stage 7.0 (TID 23) in 164 ms on 172.18.0.7 (executor 1) (1/6)
 INFO [2020-01-19 00:15:49,165] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 7.0 (TID 20) in 174 ms on 172.18.0.7 (executor 1) (2/6)
 INFO [2020-01-19 00:15:49,175] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 4.0 in stage 7.0 (TID 24) in 181 ms on 172.18.0.6 (executor 0) (3/6)
 INFO [2020-01-19 00:15:49,175] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 1.0 in stage 7.0 (TID 21) in 183 ms on 172.18.0.6 (executor 0) (4/6)
 INFO [2020-01-19 00:15:49,244] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 5.0 in stage 7.0 (TID 25) in 249 ms on 172.18.0.8 (executor 2) (5/6)
 INFO [2020-01-19 00:15:49,245] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 2.0 in stage 7.0 (TID 22) in 253 ms on 172.18.0.8 (executor 2) (6/6)
 INFO [2020-01-19 00:15:49,245] ({task-result-getter-1} Logging.scala[logInfo]:54) - Removed TaskSet 7.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:15:49,249] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ShuffleMapStage 7 (collect at <ipython-input-52-4294447afb77>:4) finished in 0.280 s
 INFO [2020-01-19 00:15:49,250] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - looking for newly runnable stages
 INFO [2020-01-19 00:15:49,250] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - running: Set()
 INFO [2020-01-19 00:15:49,250] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - waiting: Set(ResultStage 8)
 INFO [2020-01-19 00:15:49,251] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - failed: Set()
 INFO [2020-01-19 00:15:49,252] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 8 (MapPartitionsRDD[41] at collect at <ipython-input-52-4294447afb77>:4), which has no missing parents
 INFO [2020-01-19 00:15:49,260] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_8 stored as values in memory (estimated size 8.1 KB, free 366.2 MB)
 INFO [2020-01-19 00:15:49,261] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.3 KB, free 366.2 MB)
 INFO [2020-01-19 00:15:49,262] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_8_piece0 in memory on zeppelin:34711 (size: 4.3 KB, free: 366.2 MB)
 INFO [2020-01-19 00:15:49,262] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 8 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:15:49,263] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[41] at collect at <ipython-input-52-4294447afb77>:4) (first 15 tasks are for partitions Vector(0))
 INFO [2020-01-19 00:15:49,264] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 8.0 with 1 tasks
 INFO [2020-01-19 00:15:49,265] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 8.0 (TID 26, 172.18.0.6, executor 0, partition 0, NODE_LOCAL, 7771 bytes)
 INFO [2020-01-19 00:15:49,283] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_8_piece0 in memory on 172.18.0.6:40877 (size: 4.3 KB, free: 912.3 MB)
 INFO [2020-01-19 00:15:49,305] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Asked to send map output locations for shuffle 2 to 172.18.0.6:32928
 INFO [2020-01-19 00:15:49,519] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 8.0 (TID 26) in 254 ms on 172.18.0.6 (executor 0) (1/1)
 INFO [2020-01-19 00:15:49,519] ({task-result-getter-2} Logging.scala[logInfo]:54) - Removed TaskSet 8.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:15:49,520] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 8 (collect at <ipython-input-52-4294447afb77>:4) finished in 0.268 s
 INFO [2020-01-19 00:15:49,521] ({Thread-20} Logging.scala[logInfo]:54) - Job 5 finished: collect at <ipython-input-52-4294447afb77>:4, took 0.555162 s
 INFO [2020-01-19 00:15:49,558] ({pool-2-thread-19} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler interpreter_1122984007
 INFO [2020-01-19 00:18:58,449] ({pool-2-thread-11} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler interpreter_1122984007
 INFO [2020-01-19 00:18:58,619] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 159
 INFO [2020-01-19 00:18:58,619] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 266
 INFO [2020-01-19 00:18:58,620] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 132
 INFO [2020-01-19 00:18:58,620] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 29
 INFO [2020-01-19 00:18:58,637] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned shuffle 0
 INFO [2020-01-19 00:18:58,638] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 263
 INFO [2020-01-19 00:18:58,638] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 269
 INFO [2020-01-19 00:18:58,638] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 164
 INFO [2020-01-19 00:18:58,639] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 267
 INFO [2020-01-19 00:18:58,639] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 238
 INFO [2020-01-19 00:18:58,640] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 182
 INFO [2020-01-19 00:18:58,640] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 91
 INFO [2020-01-19 00:18:58,654] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: collect at <ipython-input-54-4294447afb77>:4
 INFO [2020-01-19 00:18:58,654] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 30
 INFO [2020-01-19 00:18:58,655] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 93
 INFO [2020-01-19 00:18:58,656] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 49
 INFO [2020-01-19 00:18:58,657] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 69
 INFO [2020-01-19 00:18:58,657] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 203
 INFO [2020-01-19 00:18:58,657] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 10
 INFO [2020-01-19 00:18:58,658] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 114
 INFO [2020-01-19 00:18:58,658] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 249
 INFO [2020-01-19 00:18:58,659] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 179
 INFO [2020-01-19 00:18:58,659] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 11
 INFO [2020-01-19 00:18:58,657] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Registering RDD 44 (collect at <ipython-input-54-4294447afb77>:4)
 INFO [2020-01-19 00:18:58,661] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 6 (collect at <ipython-input-54-4294447afb77>:4) with 1 output partitions
 INFO [2020-01-19 00:18:58,663] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 10 (collect at <ipython-input-54-4294447afb77>:4)
 INFO [2020-01-19 00:18:58,663] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List(ShuffleMapStage 9)
 INFO [2020-01-19 00:18:58,664] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List(ShuffleMapStage 9)
 INFO [2020-01-19 00:18:58,665] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ShuffleMapStage 9 (MapPartitionsRDD[44] at collect at <ipython-input-54-4294447afb77>:4), which has no missing parents
 INFO [2020-01-19 00:18:58,706] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_8_piece0 on 172.18.0.6:40877 in memory (size: 4.3 KB, free: 912.3 MB)
 INFO [2020-01-19 00:18:58,715] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_9 stored as values in memory (estimated size 13.6 KB, free 366.2 MB)
 INFO [2020-01-19 00:18:58,716] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.2 KB, free 366.1 MB)
 INFO [2020-01-19 00:18:58,718] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_9_piece0 in memory on zeppelin:34711 (size: 7.2 KB, free: 366.2 MB)
 INFO [2020-01-19 00:18:58,719] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 9 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:18:58,720] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_8_piece0 on zeppelin:34711 in memory (size: 4.3 KB, free: 366.2 MB)
 INFO [2020-01-19 00:18:58,731] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 6 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[44] at collect at <ipython-input-54-4294447afb77>:4) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
 INFO [2020-01-19 00:18:58,731] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 9.0 with 6 tasks
 INFO [2020-01-19 00:18:58,733] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 9.0 (TID 27, 172.18.0.7, executor 1, partition 0, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:18:58,733] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 1.0 in stage 9.0 (TID 28, 172.18.0.8, executor 2, partition 1, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:18:58,739] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 2.0 in stage 9.0 (TID 29, 172.18.0.6, executor 0, partition 2, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:18:58,739] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 3.0 in stage 9.0 (TID 30, 172.18.0.7, executor 1, partition 3, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:18:58,739] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 4.0 in stage 9.0 (TID 31, 172.18.0.8, executor 2, partition 4, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:18:58,740] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 5.0 in stage 9.0 (TID 32, 172.18.0.6, executor 0, partition 5, PROCESS_LOCAL, 7909 bytes)
 INFO [2020-01-19 00:18:58,784] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_9_piece0 in memory on 172.18.0.7:43081 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:18:58,871] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_9_piece0 in memory on 172.18.0.8:45595 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:18:58,872] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_9_piece0 in memory on 172.18.0.6:40877 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:18:58,900] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 78
 INFO [2020-01-19 00:18:58,901] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 231
 INFO [2020-01-19 00:18:58,903] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 45
 INFO [2020-01-19 00:18:58,926] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 3.0 in stage 9.0 (TID 30) in 187 ms on 172.18.0.7 (executor 1) (1/6)
 INFO [2020-01-19 00:18:58,928] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 9.0 (TID 27) in 196 ms on 172.18.0.7 (executor 1) (2/6)
 INFO [2020-01-19 00:18:58,957] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_0_piece0 on zeppelin:34711 in memory (size: 5.9 KB, free: 366.3 MB)
 INFO [2020-01-19 00:18:58,961] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_0_piece0 on 172.18.0.8:45595 in memory (size: 5.9 KB, free: 912.3 MB)
 INFO [2020-01-19 00:18:59,009] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 1.0 in stage 9.0 (TID 28) in 276 ms on 172.18.0.8 (executor 2) (3/6)
 INFO [2020-01-19 00:18:59,010] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 4.0 in stage 9.0 (TID 31) in 271 ms on 172.18.0.8 (executor 2) (4/6)
 INFO [2020-01-19 00:18:59,052] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 5.0 in stage 9.0 (TID 32) in 313 ms on 172.18.0.6 (executor 0) (5/6)
 INFO [2020-01-19 00:18:59,055] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 2.0 in stage 9.0 (TID 29) in 317 ms on 172.18.0.6 (executor 0) (6/6)
 INFO [2020-01-19 00:18:59,055] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 9.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:18:59,056] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ShuffleMapStage 9 (collect at <ipython-input-54-4294447afb77>:4) finished in 0.390 s
 INFO [2020-01-19 00:18:59,056] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - looking for newly runnable stages
 INFO [2020-01-19 00:18:59,057] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - running: Set()
 INFO [2020-01-19 00:18:59,057] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - waiting: Set(ResultStage 10)
 INFO [2020-01-19 00:18:59,058] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - failed: Set()
 INFO [2020-01-19 00:18:59,058] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 10 (MapPartitionsRDD[47] at collect at <ipython-input-54-4294447afb77>:4), which has no missing parents
 INFO [2020-01-19 00:18:59,061] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_10 stored as values in memory (estimated size 8.1 KB, free 366.2 MB)
 INFO [2020-01-19 00:18:59,062] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.3 KB, free 366.2 MB)
 INFO [2020-01-19 00:18:59,063] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_10_piece0 in memory on zeppelin:34711 (size: 4.3 KB, free: 366.2 MB)
 INFO [2020-01-19 00:18:59,064] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 10 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:18:59,065] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[47] at collect at <ipython-input-54-4294447afb77>:4) (first 15 tasks are for partitions Vector(0))
 INFO [2020-01-19 00:18:59,066] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 10.0 with 1 tasks
 INFO [2020-01-19 00:18:59,069] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 10.0 (TID 33, 172.18.0.7, executor 1, partition 0, NODE_LOCAL, 7771 bytes)
 INFO [2020-01-19 00:18:59,092] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_10_piece0 in memory on 172.18.0.7:43081 (size: 4.3 KB, free: 912.3 MB)
 INFO [2020-01-19 00:18:59,098] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Asked to send map output locations for shuffle 3 to 172.18.0.7:53826
 INFO [2020-01-19 00:18:59,136] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 66
 INFO [2020-01-19 00:18:59,137] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 272
 INFO [2020-01-19 00:18:59,138] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 183
 INFO [2020-01-19 00:18:59,142] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 98
 INFO [2020-01-19 00:18:59,143] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 234
 INFO [2020-01-19 00:18:59,143] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 122
 INFO [2020-01-19 00:18:59,143] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 76
 INFO [2020-01-19 00:18:59,143] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 38
 INFO [2020-01-19 00:18:59,143] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 37
 INFO [2020-01-19 00:18:59,148] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 244
 INFO [2020-01-19 00:18:59,148] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 208
 INFO [2020-01-19 00:18:59,148] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 246
 INFO [2020-01-19 00:18:59,149] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 26
 INFO [2020-01-19 00:18:59,149] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 73
 INFO [2020-01-19 00:18:59,149] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 4
 INFO [2020-01-19 00:18:59,149] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 195
 INFO [2020-01-19 00:18:59,149] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 14
 INFO [2020-01-19 00:18:59,149] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 111
 INFO [2020-01-19 00:18:59,150] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 99
 INFO [2020-01-19 00:18:59,150] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 84
 INFO [2020-01-19 00:18:59,154] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 109
 INFO [2020-01-19 00:18:59,154] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 165
 INFO [2020-01-19 00:18:59,155] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 232
 INFO [2020-01-19 00:18:59,160] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 75
 INFO [2020-01-19 00:18:59,160] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 166
 INFO [2020-01-19 00:18:59,161] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 270
 INFO [2020-01-19 00:18:59,161] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 199
 INFO [2020-01-19 00:18:59,162] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 104
 INFO [2020-01-19 00:18:59,164] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 57
 INFO [2020-01-19 00:18:59,164] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 27
 INFO [2020-01-19 00:18:59,165] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 169
 INFO [2020-01-19 00:18:59,165] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 260
 INFO [2020-01-19 00:18:59,165] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 242
 INFO [2020-01-19 00:18:59,168] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 227
 INFO [2020-01-19 00:18:59,169] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 158
 INFO [2020-01-19 00:18:59,173] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 16
 INFO [2020-01-19 00:18:59,173] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 107
 INFO [2020-01-19 00:18:59,174] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 237
 INFO [2020-01-19 00:18:59,174] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 191
 INFO [2020-01-19 00:18:59,179] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 43
 INFO [2020-01-19 00:18:59,182] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 173
 INFO [2020-01-19 00:18:59,182] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 32
 INFO [2020-01-19 00:18:59,182] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 47
 INFO [2020-01-19 00:18:59,182] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 135
 INFO [2020-01-19 00:18:59,185] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 224
 INFO [2020-01-19 00:18:59,186] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 90
 INFO [2020-01-19 00:18:59,186] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 8
 INFO [2020-01-19 00:18:59,212] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 10.0 (TID 33) in 143 ms on 172.18.0.7 (executor 1) (1/1)
 INFO [2020-01-19 00:18:59,213] ({task-result-getter-1} Logging.scala[logInfo]:54) - Removed TaskSet 10.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:18:59,217] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 10 (collect at <ipython-input-54-4294447afb77>:4) finished in 0.156 s
 INFO [2020-01-19 00:18:59,217] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_4_piece0 on 172.18.0.7:43081 in memory (size: 3.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:18:59,218] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_4_piece0 on zeppelin:34711 in memory (size: 3.8 KB, free: 366.3 MB)
 INFO [2020-01-19 00:18:59,218] ({Thread-20} Logging.scala[logInfo]:54) - Job 6 finished: collect at <ipython-input-54-4294447afb77>:4, took 0.563633 s
 INFO [2020-01-19 00:18:59,318] ({pool-2-thread-11} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler interpreter_1122984007
 INFO [2020-01-19 00:18:59,350] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 6
 INFO [2020-01-19 00:18:59,350] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 261
 INFO [2020-01-19 00:18:59,351] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 108
 INFO [2020-01-19 00:18:59,351] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 46
 INFO [2020-01-19 00:18:59,351] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 130
 INFO [2020-01-19 00:18:59,352] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 41
 INFO [2020-01-19 00:18:59,352] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 160
 INFO [2020-01-19 00:18:59,353] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 9
 INFO [2020-01-19 00:18:59,353] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 196
 INFO [2020-01-19 00:18:59,354] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 120
 INFO [2020-01-19 00:18:59,354] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 119
 INFO [2020-01-19 00:18:59,354] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 95
 INFO [2020-01-19 00:18:59,354] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 56
 INFO [2020-01-19 00:18:59,354] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 170
 INFO [2020-01-19 00:18:59,355] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 54
 INFO [2020-01-19 00:18:59,392] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_2_piece0 on 172.18.0.8:45595 in memory (size: 6.0 KB, free: 912.3 MB)
 INFO [2020-01-19 00:18:59,396] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_2_piece0 on zeppelin:34711 in memory (size: 6.0 KB, free: 366.3 MB)
 INFO [2020-01-19 00:18:59,406] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 136
 INFO [2020-01-19 00:18:59,406] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 80
 INFO [2020-01-19 00:18:59,406] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 133
 INFO [2020-01-19 00:18:59,407] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 63
 INFO [2020-01-19 00:18:59,407] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 110
 INFO [2020-01-19 00:18:59,408] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 7
 INFO [2020-01-19 00:18:59,425] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_1_piece0 on 172.18.0.6:40877 in memory (size: 6.0 KB, free: 912.3 MB)
 INFO [2020-01-19 00:18:59,426] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_1_piece0 on 172.18.0.8:45595 in memory (size: 6.0 KB, free: 912.3 MB)
 INFO [2020-01-19 00:18:59,426] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_1_piece0 on 172.18.0.7:43081 in memory (size: 6.0 KB, free: 912.3 MB)
 INFO [2020-01-19 00:18:59,435] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_1_piece0 on zeppelin:34711 in memory (size: 6.0 KB, free: 366.3 MB)
 INFO [2020-01-19 00:18:59,444] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 207
 INFO [2020-01-19 00:18:59,445] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 273
 INFO [2020-01-19 00:18:59,445] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 35
 INFO [2020-01-19 00:18:59,445] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 59
 INFO [2020-01-19 00:18:59,445] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 115
 INFO [2020-01-19 00:18:59,445] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 101
 INFO [2020-01-19 00:18:59,445] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 52
 INFO [2020-01-19 00:18:59,445] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 194
 INFO [2020-01-19 00:18:59,446] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 31
 INFO [2020-01-19 00:18:59,446] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 172
 INFO [2020-01-19 00:18:59,446] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 62
 INFO [2020-01-19 00:18:59,446] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 257
 INFO [2020-01-19 00:18:59,446] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 235
 INFO [2020-01-19 00:18:59,446] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 189
 INFO [2020-01-19 00:18:59,446] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 138
 INFO [2020-01-19 00:18:59,447] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 134
 INFO [2020-01-19 00:18:59,447] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 187
 INFO [2020-01-19 00:18:59,447] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 192
 INFO [2020-01-19 00:18:59,447] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 204
 INFO [2020-01-19 00:18:59,447] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 23
 INFO [2020-01-19 00:18:59,447] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 13
 INFO [2020-01-19 00:18:59,448] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 65
 INFO [2020-01-19 00:18:59,448] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 94
 INFO [2020-01-19 00:18:59,448] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 174
 INFO [2020-01-19 00:18:59,448] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 40
 INFO [2020-01-19 00:18:59,448] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 252
 INFO [2020-01-19 00:18:59,448] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 250
 INFO [2020-01-19 00:18:59,449] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 72
 INFO [2020-01-19 00:18:59,449] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 240
 INFO [2020-01-19 00:18:59,450] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 39
 INFO [2020-01-19 00:18:59,450] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 21
 INFO [2020-01-19 00:18:59,451] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 121
 INFO [2020-01-19 00:18:59,451] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 190
 INFO [2020-01-19 00:18:59,451] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 18
 INFO [2020-01-19 00:18:59,451] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 85
 INFO [2020-01-19 00:18:59,451] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 116
 INFO [2020-01-19 00:18:59,451] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 17
 INFO [2020-01-19 00:18:59,452] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 34
 INFO [2020-01-19 00:18:59,452] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 60
 INFO [2020-01-19 00:18:59,452] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 103
 INFO [2020-01-19 00:18:59,452] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 245
 INFO [2020-01-19 00:18:59,474] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_6_piece0 on 172.18.0.8:45595 in memory (size: 4.3 KB, free: 912.3 MB)
 INFO [2020-01-19 00:18:59,477] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_6_piece0 on zeppelin:34711 in memory (size: 4.3 KB, free: 366.3 MB)
 INFO [2020-01-19 00:18:59,488] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 126
 INFO [2020-01-19 00:18:59,489] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 161
 INFO [2020-01-19 00:18:59,489] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 33
 INFO [2020-01-19 00:18:59,492] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 137
 INFO [2020-01-19 00:18:59,492] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 178
 INFO [2020-01-19 00:18:59,493] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 239
 INFO [2020-01-19 00:18:59,493] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 20
 INFO [2020-01-19 00:18:59,494] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 125
 INFO [2020-01-19 00:18:59,494] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 92
 INFO [2020-01-19 00:18:59,495] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 253
 INFO [2020-01-19 00:18:59,495] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 176
 INFO [2020-01-19 00:18:59,496] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 50
 INFO [2020-01-19 00:18:59,496] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 117
 INFO [2020-01-19 00:18:59,497] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 102
 INFO [2020-01-19 00:18:59,497] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 163
 INFO [2020-01-19 00:18:59,498] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 258
 INFO [2020-01-19 00:18:59,498] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 124
 INFO [2020-01-19 00:18:59,499] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 64
 INFO [2020-01-19 00:18:59,500] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 197
 INFO [2020-01-19 00:18:59,501] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 42
 INFO [2020-01-19 00:18:59,502] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 171
 INFO [2020-01-19 00:18:59,502] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 44
 INFO [2020-01-19 00:18:59,503] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 127
 INFO [2020-01-19 00:18:59,503] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 5
 INFO [2020-01-19 00:18:59,503] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 233
 INFO [2020-01-19 00:18:59,503] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 259
 INFO [2020-01-19 00:18:59,503] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 255
 INFO [2020-01-19 00:18:59,503] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 48
 INFO [2020-01-19 00:18:59,503] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 168
 INFO [2020-01-19 00:18:59,503] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 123
 INFO [2020-01-19 00:18:59,504] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 236
 INFO [2020-01-19 00:18:59,504] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 118
 INFO [2020-01-19 00:18:59,504] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 2
 INFO [2020-01-19 00:18:59,504] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 68
 INFO [2020-01-19 00:18:59,504] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 268
 INFO [2020-01-19 00:18:59,504] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 97
 INFO [2020-01-19 00:18:59,504] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 200
 INFO [2020-01-19 00:18:59,504] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 262
 INFO [2020-01-19 00:18:59,505] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 105
 INFO [2020-01-19 00:18:59,505] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 225
 INFO [2020-01-19 00:18:59,505] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 71
 INFO [2020-01-19 00:18:59,505] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 177
 INFO [2020-01-19 00:18:59,505] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 186
 INFO [2020-01-19 00:18:59,505] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 256
 INFO [2020-01-19 00:18:59,505] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 140
 INFO [2020-01-19 00:18:59,505] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 82
 INFO [2020-01-19 00:18:59,514] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_5_piece0 on 172.18.0.8:45595 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:18:59,529] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_5_piece0 on 172.18.0.6:40877 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:18:59,535] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_5_piece0 on 172.18.0.7:43081 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:18:59,538] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_5_piece0 on zeppelin:34711 in memory (size: 7.2 KB, free: 366.3 MB)
 INFO [2020-01-19 00:18:59,559] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 188
 INFO [2020-01-19 00:18:59,560] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 36
 INFO [2020-01-19 00:18:59,560] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 128
 INFO [2020-01-19 00:18:59,560] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 131
 INFO [2020-01-19 00:18:59,560] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 185
 INFO [2020-01-19 00:18:59,560] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 3
 INFO [2020-01-19 00:18:59,560] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 15
 INFO [2020-01-19 00:18:59,560] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 86
 INFO [2020-01-19 00:18:59,561] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 87
 INFO [2020-01-19 00:18:59,561] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 142
 INFO [2020-01-19 00:18:59,561] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 206
 INFO [2020-01-19 00:18:59,561] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 55
 INFO [2020-01-19 00:18:59,561] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 83
 INFO [2020-01-19 00:18:59,561] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 223
 INFO [2020-01-19 00:18:59,561] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 28
 INFO [2020-01-19 00:18:59,561] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 226
 INFO [2020-01-19 00:18:59,561] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 88
 INFO [2020-01-19 00:18:59,562] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 248
 INFO [2020-01-19 00:18:59,562] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 143
 INFO [2020-01-19 00:18:59,562] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 79
 INFO [2020-01-19 00:18:59,562] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 201
 INFO [2020-01-19 00:18:59,562] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 265
 INFO [2020-01-19 00:18:59,562] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 81
 INFO [2020-01-19 00:18:59,562] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 167
 INFO [2020-01-19 00:18:59,562] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 24
 INFO [2020-01-19 00:18:59,563] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 113
 INFO [2020-01-19 00:18:59,563] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 162
 INFO [2020-01-19 00:18:59,568] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_3_piece0 on zeppelin:34711 in memory (size: 6.6 KB, free: 366.3 MB)
 INFO [2020-01-19 00:18:59,569] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_3_piece0 on 172.18.0.7:43081 in memory (size: 6.6 KB, free: 912.3 MB)
 INFO [2020-01-19 00:18:59,570] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_3_piece0 on 172.18.0.8:45595 in memory (size: 6.6 KB, free: 912.3 MB)
 INFO [2020-01-19 00:18:59,570] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_3_piece0 on 172.18.0.6:40877 in memory (size: 6.6 KB, free: 912.3 MB)
 INFO [2020-01-19 00:18:59,580] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 241
 INFO [2020-01-19 00:18:59,581] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 74
 INFO [2020-01-19 00:18:59,581] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 61
 INFO [2020-01-19 00:18:59,581] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 53
 INFO [2020-01-19 00:18:59,581] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 89
 INFO [2020-01-19 00:18:59,581] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 139
 INFO [2020-01-19 00:18:59,582] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 106
 INFO [2020-01-19 00:18:59,591] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_7_piece0 on 172.18.0.8:45595 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:18:59,599] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_7_piece0 on 172.18.0.6:40877 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:18:59,601] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_7_piece0 on 172.18.0.7:43081 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:18:59,602] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_7_piece0 on zeppelin:34711 in memory (size: 7.2 KB, free: 366.3 MB)
 INFO [2020-01-19 00:18:59,608] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 112
 INFO [2020-01-19 00:18:59,608] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 70
 INFO [2020-01-19 00:18:59,608] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 230
 INFO [2020-01-19 00:18:59,609] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 271
 INFO [2020-01-19 00:18:59,609] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 58
 INFO [2020-01-19 00:18:59,609] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 229
 INFO [2020-01-19 00:18:59,609] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 180
 INFO [2020-01-19 00:18:59,609] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 247
 INFO [2020-01-19 00:18:59,609] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 22
 INFO [2020-01-19 00:18:59,609] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 198
 INFO [2020-01-19 00:18:59,610] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 264
 INFO [2020-01-19 00:18:59,610] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 243
 INFO [2020-01-19 00:18:59,610] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 193
 INFO [2020-01-19 00:18:59,610] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 96
 INFO [2020-01-19 00:18:59,610] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 141
 INFO [2020-01-19 00:18:59,611] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 12
 INFO [2020-01-19 00:18:59,611] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 254
 INFO [2020-01-19 00:18:59,611] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 19
 INFO [2020-01-19 00:18:59,611] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 202
 INFO [2020-01-19 00:18:59,611] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 25
 INFO [2020-01-19 00:18:59,611] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 228
 INFO [2020-01-19 00:18:59,611] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1
 INFO [2020-01-19 00:18:59,611] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 205
 INFO [2020-01-19 00:18:59,612] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 77
 INFO [2020-01-19 00:18:59,612] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 175
 INFO [2020-01-19 00:18:59,612] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 129
 INFO [2020-01-19 00:18:59,612] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 181
 INFO [2020-01-19 00:18:59,612] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 251
 INFO [2020-01-19 00:18:59,612] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 184
 INFO [2020-01-19 00:18:59,612] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 51
 INFO [2020-01-19 00:18:59,613] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 67
 INFO [2020-01-19 00:18:59,613] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 100
 INFO [2020-01-19 00:20:24,248] ({pool-1-thread-1} NewSparkInterpreter.java[close]:134) - Close SparkInterpreter
 INFO [2020-01-19 00:20:24,258] ({pool-1-thread-1} AbstractConnector.java[doStop]:318) - Stopped Spark@1fe55f09{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
 INFO [2020-01-19 00:20:24,263] ({pool-1-thread-1} Logging.scala[logInfo]:54) - Stopped Spark web UI at http://zeppelin:4040
 INFO [2020-01-19 00:20:24,275] ({pool-1-thread-1} Logging.scala[logInfo]:54) - Shutting down all executors
 INFO [2020-01-19 00:20:24,276] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Asking each executor to shut down
 INFO [2020-01-19 00:20:24,366] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - MapOutputTrackerMasterEndpoint stopped!
 INFO [2020-01-19 00:20:24,428] ({pool-1-thread-1} Logging.scala[logInfo]:54) - MemoryStore cleared
 INFO [2020-01-19 00:20:24,429] ({pool-1-thread-1} Logging.scala[logInfo]:54) - BlockManager stopped
 INFO [2020-01-19 00:20:24,438] ({pool-1-thread-1} Logging.scala[logInfo]:54) - BlockManagerMaster stopped
 INFO [2020-01-19 00:20:24,441] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - OutputCommitCoordinator stopped!
 INFO [2020-01-19 00:20:24,452] ({pool-1-thread-1} Logging.scala[logInfo]:54) - Successfully stopped SparkContext
 INFO [2020-01-19 00:20:24,453] ({pool-1-thread-1} Logging.scala[logInfo]:54) - SparkContext already stopped.
 INFO [2020-01-19 00:20:24,471] ({pool-1-thread-1} NewSparkInterpreter.java[close]:134) - Close SparkInterpreter
ERROR [2020-01-19 00:20:24,473] ({grpc-default-executor-6} SerializingExecutor.java[run]:120) - Exception while executing runnable io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed@2df3ecb9
java.lang.NullPointerException
	at io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:395)
	at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:426)
	at io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:76)
	at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:512)
	at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$700(ClientCallImpl.java:429)
	at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:544)
	at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:52)
	at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:117)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
 INFO [2020-01-19 00:20:24,481] ({pool-1-thread-1} RemoteInterpreterServer.java[shutdown]:209) - Shutting down...
 WARN [2020-01-19 00:20:25,420] ({Exec Default Executor} IPythonInterpreter.java[onProcessFailed]:398) - Exception happens in Python Process
org.apache.commons.exec.ExecuteException: Process exited with an error: 143 (Exit value: 143)
	at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:404)
	at org.apache.commons.exec.DefaultExecutor.access$200(DefaultExecutor.java:48)
	at org.apache.commons.exec.DefaultExecutor$1.run(DefaultExecutor.java:200)
	at java.lang.Thread.run(Thread.java:748)
 INFO [2020-01-19 00:20:26,599] ({Thread-1} Logging.scala[logInfo]:54) - Shutdown hook called
 INFO [2020-01-19 00:20:26,602] ({Thread-1} Logging.scala[logInfo]:54) - Deleting directory /tmp/spark-3f626649-fa0d-45d1-9d3f-d9caa0b6d6c4
 INFO [2020-01-19 00:20:26,604] ({Thread-1} Logging.scala[logInfo]:54) - Deleting directory /tmp/spark-72c1477b-34c0-492f-b18f-00f633abcd07
 INFO [2020-01-19 00:20:26,606] ({Thread-1} Logging.scala[logInfo]:54) - Deleting directory /tmp/spark-3f626649-fa0d-45d1-9d3f-d9caa0b6d6c4/pyspark-edc825d8-838c-427e-9734-df8c0f619a9e
 WARN [2020-01-19 00:23:26,519] ({main} NativeCodeLoader.java[<clinit>]:62) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
 INFO [2020-01-19 00:23:26,714] ({main} RemoteInterpreterServer.java[main]:261) - URL:jar:file:/zeppelin/interpreter/spark/spark-interpreter-0.8.1.jar!/org/apache/zeppelin/interpreter/remote/RemoteInterpreterServer.class
 INFO [2020-01-19 00:23:26,739] ({main} RemoteInterpreterServer.java[<init>]:162) - Launching ThriftServer at 172.18.0.2:45335
 INFO [2020-01-19 00:23:26,741] ({main} RemoteInterpreterServer.java[<init>]:166) - Starting remote interpreter server on port 45335
 INFO [2020-01-19 00:23:26,742] ({Thread-3} RemoteInterpreterServer.java[run]:203) - Starting remote interpreter server on port 45335
 INFO [2020-01-19 00:23:27,747] ({Thread-4} RemoteInterpreterUtils.java[registerInterpreter]:165) - callbackHost: 172.18.0.2, callbackPort: 36335, callbackInfo: CallbackInfo(host:172.18.0.2, port:45335)
 INFO [2020-01-19 00:23:27,911] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.SparkInterpreter
 INFO [2020-01-19 00:23:27,918] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.SparkSqlInterpreter
 INFO [2020-01-19 00:23:27,922] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.DepInterpreter
 INFO [2020-01-19 00:23:27,926] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.PySparkInterpreter
 INFO [2020-01-19 00:23:27,934] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.IPySparkInterpreter
 INFO [2020-01-19 00:23:27,937] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.SparkRInterpreter
 WARN [2020-01-19 00:23:28,044] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:117) - Failed to load configuration, proceeding with a default
 INFO [2020-01-19 00:23:28,076] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:129) - Server Host: 0.0.0.0
 INFO [2020-01-19 00:23:28,076] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:131) - Server Port: 8090
 INFO [2020-01-19 00:23:28,078] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:135) - Context Path: /
 INFO [2020-01-19 00:23:28,085] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:136) - Zeppelin Version: 0.8.1
 INFO [2020-01-19 00:23:28,085] ({pool-1-thread-1} SchedulerFactory.java[<init>]:59) - Scheduler Thread Pool Size: 100
 INFO [2020-01-19 00:23:28,088] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195314_1536993808 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:23:28,527] ({pool-2-thread-2} IPythonInterpreter.java[checkIPythonPrerequisite]:198) - IPython prerequisite is met
 INFO [2020-01-19 00:23:28,535] ({pool-2-thread-2} NewSparkInterpreter.java[open]:83) - Using Scala Version: 2.11
 INFO [2020-01-19 00:23:33,464] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Running Spark version 2.4.3
 INFO [2020-01-19 00:23:33,486] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Submitted application: Zeppelin
 INFO [2020-01-19 00:23:33,540] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Changing view acls to: root
 INFO [2020-01-19 00:23:33,540] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Changing modify acls to: root
 INFO [2020-01-19 00:23:33,540] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Changing view acls groups to: 
 INFO [2020-01-19 00:23:33,541] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Changing modify acls groups to: 
 INFO [2020-01-19 00:23:33,541] ({pool-2-thread-2} Logging.scala[logInfo]:54) - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
 INFO [2020-01-19 00:23:34,072] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Successfully started service 'sparkDriver' on port 46505.
 INFO [2020-01-19 00:23:34,094] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registering MapOutputTracker
 INFO [2020-01-19 00:23:34,111] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registering BlockManagerMaster
 INFO [2020-01-19 00:23:34,115] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
 INFO [2020-01-19 00:23:34,115] ({pool-2-thread-2} Logging.scala[logInfo]:54) - BlockManagerMasterEndpoint up
 INFO [2020-01-19 00:23:34,125] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Created local directory at /tmp/blockmgr-704caf3b-d3d5-4ed1-8181-b700d9cfa933
 INFO [2020-01-19 00:23:34,136] ({pool-2-thread-2} Logging.scala[logInfo]:54) - MemoryStore started with capacity 366.3 MB
 INFO [2020-01-19 00:23:34,148] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registering OutputCommitCoordinator
 INFO [2020-01-19 00:23:34,207] ({pool-2-thread-2} Log.java[initialized]:192) - Logging initialized @8803ms
 INFO [2020-01-19 00:23:34,264] ({pool-2-thread-2} Server.java[doStart]:351) - jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
 INFO [2020-01-19 00:23:34,280] ({pool-2-thread-2} Server.java[doStart]:419) - Started @8875ms
 INFO [2020-01-19 00:23:34,299] ({pool-2-thread-2} AbstractConnector.java[doStart]:278) - Started ServerConnector@7c9f6d49{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
 INFO [2020-01-19 00:23:34,299] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Successfully started service 'SparkUI' on port 4040.
 INFO [2020-01-19 00:23:34,325] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@7f02eaa9{/jobs,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:34,326] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@7a20eb10{/jobs/json,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:34,327] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@150a943b{/jobs/job,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:34,330] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@54b01e37{/jobs/job/json,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:34,332] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@29b507da{/stages,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:34,333] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@685fe59b{/stages/json,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:34,334] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@5c3a1556{/stages/stage,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:34,336] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@1e41fbca{/stages/stage/json,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:34,339] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@125dddf1{/stages/pool,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:34,341] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@65cf4303{/stages/pool/json,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:34,341] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@41ec2c96{/storage,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:34,342] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@50e200ca{/storage/json,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:34,343] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@2caced72{/storage/rdd,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:34,345] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@33e474bf{/storage/rdd/json,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:34,346] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@13cc66c9{/environment,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:34,348] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@5368c02a{/environment/json,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:34,349] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@18646048{/executors,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:34,352] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@835ae77{/executors/json,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:34,354] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@783f7b96{/executors/threadDump,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:34,355] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@6dc39d2b{/executors/threadDump/json,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:34,362] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@569ed6fb{/static,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:34,363] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@4fdc00fd{/,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:34,366] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@76527eee{/api,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:34,367] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@853a4e6{/jobs/job/kill,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:34,368] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@207e7a06{/stages/stage/kill,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:34,370] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Bound SparkUI to 0.0.0.0, and started at http://zeppelin:4040
 INFO [2020-01-19 00:23:34,399] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Added JAR file:/zeppelin/interpreter/spark/spark-interpreter-0.8.1.jar at spark://zeppelin:46505/jars/spark-interpreter-0.8.1.jar with timestamp 1579393414398
 WARN [2020-01-19 00:23:34,400] ({pool-2-thread-2} Logging.scala[logWarning]:66) - The jar /zeppelin/interpreter/spark/spark-interpreter-0.8.1.jar has been added already. Overwriting of added jars is not supported in the current version.
 INFO [2020-01-19 00:23:34,496] ({appclient-register-master-threadpool-0} Logging.scala[logInfo]:54) - Connecting to master spark://spark-master:7077...
 INFO [2020-01-19 00:23:34,549] ({netty-rpc-connection-0} TransportClientFactory.java[createClient]:267) - Successfully created connection to spark-master/172.18.0.4:7077 after 29 ms (0 ms spent in bootstraps)
 INFO [2020-01-19 00:23:34,666] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Connected to Spark cluster with app ID app-20200119002334-0000
 INFO [2020-01-19 00:23:34,677] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33185.
 INFO [2020-01-19 00:23:34,678] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Server created on zeppelin:33185
 INFO [2020-01-19 00:23:34,681] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
 INFO [2020-01-19 00:23:34,702] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registering BlockManager BlockManagerId(driver, zeppelin, 33185, None)
 INFO [2020-01-19 00:23:34,717] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Registering block manager zeppelin:33185 with 366.3 MB RAM, BlockManagerId(driver, zeppelin, 33185, None)
 INFO [2020-01-19 00:23:34,734] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Executor added: app-20200119002334-0000/0 on worker-20200119002058-172.18.0.8-42783 (172.18.0.8:42783) with 2 core(s)
 INFO [2020-01-19 00:23:34,741] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Granted executor ID app-20200119002334-0000/0 on hostPort 172.18.0.8:42783 with 2 core(s), 2.0 GB RAM
 INFO [2020-01-19 00:23:34,746] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Executor added: app-20200119002334-0000/1 on worker-20200119002058-172.18.0.7-40995 (172.18.0.7:40995) with 2 core(s)
 INFO [2020-01-19 00:23:34,754] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Granted executor ID app-20200119002334-0000/1 on hostPort 172.18.0.7:40995 with 2 core(s), 2.0 GB RAM
 INFO [2020-01-19 00:23:34,754] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Executor added: app-20200119002334-0000/2 on worker-20200119002058-172.18.0.6-38687 (172.18.0.6:38687) with 2 core(s)
 INFO [2020-01-19 00:23:34,755] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Granted executor ID app-20200119002334-0000/2 on hostPort 172.18.0.6:38687 with 2 core(s), 2.0 GB RAM
 INFO [2020-01-19 00:23:34,754] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registered BlockManager BlockManagerId(driver, zeppelin, 33185, None)
 INFO [2020-01-19 00:23:34,764] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Initialized BlockManager: BlockManagerId(driver, zeppelin, 33185, None)
 INFO [2020-01-19 00:23:34,906] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Executor updated: app-20200119002334-0000/0 is now RUNNING
 INFO [2020-01-19 00:23:34,918] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Executor updated: app-20200119002334-0000/2 is now RUNNING
 INFO [2020-01-19 00:23:34,931] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Executor updated: app-20200119002334-0000/1 is now RUNNING
 INFO [2020-01-19 00:23:35,270] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@8cde873{/metrics/json,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:35,308] ({pool-2-thread-2} Logging.scala[logInfo]:54) - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
 INFO [2020-01-19 00:23:42,205] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.7:47870) with ID 1
 INFO [2020-01-19 00:23:42,318] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:54036) with ID 2
 INFO [2020-01-19 00:23:42,472] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.8:59242) with ID 0
 INFO [2020-01-19 00:23:42,773] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Registering block manager 172.18.0.7:37317 with 912.3 MB RAM, BlockManagerId(1, 172.18.0.7, 37317, None)
 INFO [2020-01-19 00:23:42,835] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Registering block manager 172.18.0.6:42495 with 912.3 MB RAM, BlockManagerId(2, 172.18.0.6, 42495, None)
 INFO [2020-01-19 00:23:42,927] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Registering block manager 172.18.0.8:44045 with 912.3 MB RAM, BlockManagerId(0, 172.18.0.8, 44045, None)
 INFO [2020-01-19 00:23:46,921] ({pool-2-thread-2} SparkShims.java[loadShims]:62) - Initializing shims for Spark 2.x
 INFO [2020-01-19 00:23:47,161] ({pool-2-thread-2} IPythonInterpreter.java[setAdditionalPythonPath]:103) - setAdditionalPythonPath: /usr/local/spark/python/lib/pyspark.zip:/usr/local/spark/python/lib/py4j-0.10.7-src.zip:/zeppelin/interpreter/lib/python
 INFO [2020-01-19 00:23:47,164] ({pool-2-thread-2} IPythonInterpreter.java[open]:135) - Python Exec: python
 INFO [2020-01-19 00:23:47,582] ({pool-2-thread-2} IPythonInterpreter.java[checkIPythonPrerequisite]:198) - IPython prerequisite is met
 INFO [2020-01-19 00:23:47,584] ({pool-2-thread-2} IPythonInterpreter.java[open]:146) - Launching IPython Kernel at port: 36735
 INFO [2020-01-19 00:23:47,585] ({pool-2-thread-2} IPythonInterpreter.java[open]:147) - Launching JVM Gateway at port: 33267
 INFO [2020-01-19 00:23:47,696] ({pool-2-thread-2} IPythonInterpreter.java[setupIPythonEnv]:319) - PYTHONPATH:/usr/local/spark/python/lib/pyspark.zip:/usr/local/spark/python/lib/py4j-0.10.7-src.zip:/zeppelin/interpreter/lib/python:/usr/local/spark//python/lib/py4j-0.10.7-src.zip:/usr/local/spark//python/:
 INFO [2020-01-19 00:23:48,016] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:297) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:23:48,117] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:297) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:23:48,218] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:297) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:23:48,319] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:297) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:23:48,420] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:297) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:23:48,521] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:297) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:23:48,621] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:297) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:23:48,722] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:297) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:23:48,823] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:297) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:23:48,924] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:297) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:23:49,062] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:293) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:23:49,171] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:293) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:23:49,280] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:293) - Wait for IPython Kernel to be started
 INFO [2020-01-19 00:23:49,386] ({pool-2-thread-2} IPythonInterpreter.java[launchIPythonKernel]:290) - IPython Kernel is Running
 INFO [2020-01-19 00:23:49,386] ({pool-2-thread-2} Py4JUtils.java[createGatewayServer]:44) - Launching GatewayServer at 127.0.0.1:33267
 INFO [2020-01-19 00:23:50,207] ({pool-2-thread-2} PySparkInterpreter.java[open]:130) - IPython is available, Use IPySparkInterpreter to replace PySparkInterpreter
 INFO [2020-01-19 00:23:50,333] ({Thread-20} Logging.scala[logInfo]:54) - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/zeppelin/spark-warehouse').
 INFO [2020-01-19 00:23:50,335] ({Thread-20} Logging.scala[logInfo]:54) - Warehouse path is 'file:/zeppelin/spark-warehouse'.
 INFO [2020-01-19 00:23:50,347] ({Thread-20} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@1e8f149{/SQL,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:50,355] ({Thread-20} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@43312de3{/SQL/json,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:50,356] ({Thread-20} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@599c05e4{/SQL/execution,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:50,357] ({Thread-20} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@196a48e2{/SQL/execution/json,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:50,358] ({Thread-20} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@5699c295{/static/sql,null,AVAILABLE,@Spark}
 INFO [2020-01-19 00:23:50,866] ({Thread-20} Logging.scala[logInfo]:54) - Registered StateStoreCoordinator endpoint
 INFO [2020-01-19 00:23:50,905] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195314_1536993808 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:23:55,157] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:23:56,622] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:24:00,686] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:24:01,674] ({Thread-20} Logging.scala[logInfo]:54) - Code generated in 311.236009 ms
 INFO [2020-01-19 00:24:01,731] ({Thread-20} Logging.scala[logInfo]:54) - Code generated in 43.180186 ms
 INFO [2020-01-19 00:24:01,865] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: collect at <ipython-input-8-4294447afb77>:4
 INFO [2020-01-19 00:24:01,879] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Registering RDD 7 (collect at <ipython-input-8-4294447afb77>:4)
 INFO [2020-01-19 00:24:01,882] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 0 (collect at <ipython-input-8-4294447afb77>:4) with 1 output partitions
 INFO [2020-01-19 00:24:01,883] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 1 (collect at <ipython-input-8-4294447afb77>:4)
 INFO [2020-01-19 00:24:01,886] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List(ShuffleMapStage 0)
 INFO [2020-01-19 00:24:01,887] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List(ShuffleMapStage 0)
 INFO [2020-01-19 00:24:01,907] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ShuffleMapStage 0 (MapPartitionsRDD[7] at collect at <ipython-input-8-4294447afb77>:4), which has no missing parents
 INFO [2020-01-19 00:24:02,098] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_0 stored as values in memory (estimated size 13.6 KB, free 366.3 MB)
 INFO [2020-01-19 00:24:02,134] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.2 KB, free 366.3 MB)
 INFO [2020-01-19 00:24:02,136] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_0_piece0 in memory on zeppelin:33185 (size: 7.2 KB, free: 366.3 MB)
 INFO [2020-01-19 00:24:02,141] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 0 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:24:02,155] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 6 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[7] at collect at <ipython-input-8-4294447afb77>:4) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
 INFO [2020-01-19 00:24:02,156] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 0.0 with 6 tasks
 INFO [2020-01-19 00:24:02,199] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1
 INFO [2020-01-19 00:24:02,221] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 0.0 (TID 0, 172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:24:02,230] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 1.0 in stage 0.0 (TID 1, 172.18.0.6, executor 2, partition 1, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:24:02,231] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 2.0 in stage 0.0 (TID 2, 172.18.0.7, executor 1, partition 2, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:24:02,232] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 3.0 in stage 0.0 (TID 3, 172.18.0.8, executor 0, partition 3, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:24:02,233] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 4.0 in stage 0.0 (TID 4, 172.18.0.6, executor 2, partition 4, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:24:02,234] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 5.0 in stage 0.0 (TID 5, 172.18.0.7, executor 1, partition 5, PROCESS_LOCAL, 7909 bytes)
 INFO [2020-01-19 00:24:03,822] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_0_piece0 in memory on 172.18.0.8:44045 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:24:04,158] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_0_piece0 in memory on 172.18.0.7:37317 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:24:04,231] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_0_piece0 in memory on 172.18.0.6:42495 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:24:07,739] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 3.0 in stage 0.0 (TID 3) in 5492 ms on 172.18.0.8 (executor 0) (1/6)
 INFO [2020-01-19 00:24:07,743] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 0.0 (TID 0) in 5570 ms on 172.18.0.8 (executor 0) (2/6)
 INFO [2020-01-19 00:24:07,752] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Connected to AccumulatorServer at host: 127.0.0.1 port: 39557
 INFO [2020-01-19 00:24:07,967] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 2.0 in stage 0.0 (TID 2) in 5736 ms on 172.18.0.7 (executor 1) (3/6)
 INFO [2020-01-19 00:24:08,042] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 5.0 in stage 0.0 (TID 5) in 5807 ms on 172.18.0.7 (executor 1) (4/6)
 INFO [2020-01-19 00:24:08,043] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 1.0 in stage 0.0 (TID 1) in 5813 ms on 172.18.0.6 (executor 2) (5/6)
 INFO [2020-01-19 00:24:08,044] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 4.0 in stage 0.0 (TID 4) in 5812 ms on 172.18.0.6 (executor 2) (6/6)
 INFO [2020-01-19 00:24:08,057] ({task-result-getter-0} Logging.scala[logInfo]:54) - Removed TaskSet 0.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:24:08,085] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ShuffleMapStage 0 (collect at <ipython-input-8-4294447afb77>:4) finished in 6.120 s
 INFO [2020-01-19 00:24:08,097] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - looking for newly runnable stages
 INFO [2020-01-19 00:24:08,098] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - running: Set()
 INFO [2020-01-19 00:24:08,099] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - waiting: Set(ResultStage 1)
 INFO [2020-01-19 00:24:08,106] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - failed: Set()
 INFO [2020-01-19 00:24:08,130] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 1 (MapPartitionsRDD[10] at collect at <ipython-input-8-4294447afb77>:4), which has no missing parents
 INFO [2020-01-19 00:24:08,180] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_1 stored as values in memory (estimated size 8.1 KB, free 366.3 MB)
 INFO [2020-01-19 00:24:08,191] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.3 KB, free 366.3 MB)
 INFO [2020-01-19 00:24:08,192] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_1_piece0 in memory on zeppelin:33185 (size: 4.3 KB, free: 366.3 MB)
 INFO [2020-01-19 00:24:08,193] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 1 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:24:08,202] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at collect at <ipython-input-8-4294447afb77>:4) (first 15 tasks are for partitions Vector(0))
 INFO [2020-01-19 00:24:08,213] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 1.0 with 1 tasks
 INFO [2020-01-19 00:24:08,230] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 1.0 (TID 6, 172.18.0.7, executor 1, partition 0, NODE_LOCAL, 7771 bytes)
 INFO [2020-01-19 00:24:08,263] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_1_piece0 in memory on 172.18.0.7:37317 (size: 4.3 KB, free: 912.3 MB)
 INFO [2020-01-19 00:24:08,318] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Asked to send map output locations for shuffle 0 to 172.18.0.7:47870
 INFO [2020-01-19 00:24:08,581] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 1.0 (TID 6) in 359 ms on 172.18.0.7 (executor 1) (1/1)
 INFO [2020-01-19 00:24:08,581] ({task-result-getter-2} Logging.scala[logInfo]:54) - Removed TaskSet 1.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:24:08,583] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 1 (collect at <ipython-input-8-4294447afb77>:4) finished in 0.417 s
 INFO [2020-01-19 00:24:08,596] ({Thread-20} Logging.scala[logInfo]:54) - Job 0 finished: collect at <ipython-input-8-4294447afb77>:4, took 6.729830 s
 INFO [2020-01-19 00:24:08,816] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:24:21,211] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:24:21,364] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: collect at <ipython-input-10-4294447afb77>:4
 INFO [2020-01-19 00:24:21,366] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Registering RDD 13 (collect at <ipython-input-10-4294447afb77>:4)
 INFO [2020-01-19 00:24:21,368] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 1 (collect at <ipython-input-10-4294447afb77>:4) with 1 output partitions
 INFO [2020-01-19 00:24:21,368] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 3 (collect at <ipython-input-10-4294447afb77>:4)
 INFO [2020-01-19 00:24:21,368] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List(ShuffleMapStage 2)
 INFO [2020-01-19 00:24:21,368] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List(ShuffleMapStage 2)
 INFO [2020-01-19 00:24:21,369] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at collect at <ipython-input-10-4294447afb77>:4), which has no missing parents
 INFO [2020-01-19 00:24:21,374] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_2 stored as values in memory (estimated size 13.6 KB, free 366.3 MB)
 INFO [2020-01-19 00:24:21,376] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.2 KB, free 366.2 MB)
 INFO [2020-01-19 00:24:21,377] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_2_piece0 in memory on zeppelin:33185 (size: 7.2 KB, free: 366.3 MB)
 INFO [2020-01-19 00:24:21,378] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 2 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:24:21,379] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 6 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at collect at <ipython-input-10-4294447afb77>:4) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
 INFO [2020-01-19 00:24:21,380] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 2.0 with 6 tasks
 INFO [2020-01-19 00:24:21,381] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 2.0 (TID 7, 172.18.0.7, executor 1, partition 0, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:24:21,382] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 1.0 in stage 2.0 (TID 8, 172.18.0.8, executor 0, partition 1, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:24:21,383] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 2.0 in stage 2.0 (TID 9, 172.18.0.6, executor 2, partition 2, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:24:21,384] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 3.0 in stage 2.0 (TID 10, 172.18.0.7, executor 1, partition 3, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:24:21,384] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 4.0 in stage 2.0 (TID 11, 172.18.0.8, executor 0, partition 4, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:24:21,385] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 5.0 in stage 2.0 (TID 12, 172.18.0.6, executor 2, partition 5, PROCESS_LOCAL, 7909 bytes)
 INFO [2020-01-19 00:24:21,446] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_2_piece0 in memory on 172.18.0.7:37317 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:24:21,462] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_2_piece0 in memory on 172.18.0.8:44045 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:24:21,467] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_2_piece0 in memory on 172.18.0.6:42495 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:24:21,546] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 2.0 (TID 7) in 165 ms on 172.18.0.7 (executor 1) (1/6)
 INFO [2020-01-19 00:24:21,559] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 3.0 in stage 2.0 (TID 10) in 176 ms on 172.18.0.7 (executor 1) (2/6)
 INFO [2020-01-19 00:24:21,569] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 4.0 in stage 2.0 (TID 11) in 185 ms on 172.18.0.8 (executor 0) (3/6)
 INFO [2020-01-19 00:24:21,593] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 5.0 in stage 2.0 (TID 12) in 209 ms on 172.18.0.6 (executor 2) (4/6)
 INFO [2020-01-19 00:24:21,605] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 1.0 in stage 2.0 (TID 8) in 223 ms on 172.18.0.8 (executor 0) (5/6)
 INFO [2020-01-19 00:24:21,630] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 2.0 in stage 2.0 (TID 9) in 247 ms on 172.18.0.6 (executor 2) (6/6)
 INFO [2020-01-19 00:24:21,630] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 2.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:24:21,634] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ShuffleMapStage 2 (collect at <ipython-input-10-4294447afb77>:4) finished in 0.263 s
 INFO [2020-01-19 00:24:21,636] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - looking for newly runnable stages
 INFO [2020-01-19 00:24:21,637] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - running: Set()
 INFO [2020-01-19 00:24:21,637] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - waiting: Set(ResultStage 3)
 INFO [2020-01-19 00:24:21,637] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - failed: Set()
 INFO [2020-01-19 00:24:21,637] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 3 (MapPartitionsRDD[16] at collect at <ipython-input-10-4294447afb77>:4), which has no missing parents
 INFO [2020-01-19 00:24:21,639] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_3 stored as values in memory (estimated size 8.1 KB, free 366.2 MB)
 INFO [2020-01-19 00:24:21,640] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.3 KB, free 366.2 MB)
 INFO [2020-01-19 00:24:21,641] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_3_piece0 in memory on zeppelin:33185 (size: 4.3 KB, free: 366.3 MB)
 INFO [2020-01-19 00:24:21,642] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 3 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:24:21,642] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[16] at collect at <ipython-input-10-4294447afb77>:4) (first 15 tasks are for partitions Vector(0))
 INFO [2020-01-19 00:24:21,646] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 3.0 with 1 tasks
 INFO [2020-01-19 00:24:21,648] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 3.0 (TID 13, 172.18.0.8, executor 0, partition 0, NODE_LOCAL, 7771 bytes)
 INFO [2020-01-19 00:24:21,675] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_3_piece0 in memory on 172.18.0.8:44045 (size: 4.3 KB, free: 912.3 MB)
 INFO [2020-01-19 00:24:21,707] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Asked to send map output locations for shuffle 1 to 172.18.0.8:59242
 INFO [2020-01-19 00:24:21,885] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 3.0 (TID 13) in 237 ms on 172.18.0.8 (executor 0) (1/1)
 INFO [2020-01-19 00:24:21,889] ({task-result-getter-0} Logging.scala[logInfo]:54) - Removed TaskSet 3.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:24:21,890] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 3 (collect at <ipython-input-10-4294447afb77>:4) finished in 0.252 s
 INFO [2020-01-19 00:24:21,892] ({Thread-20} Logging.scala[logInfo]:54) - Job 1 finished: collect at <ipython-input-10-4294447afb77>:4, took 0.527755 s
 INFO [2020-01-19 00:24:21,922] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:24:29,580] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200119-001021_531760765 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:24:29,640] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200119-001021_531760765 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:24:39,502] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20200119-001021_531760765 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:24:39,555] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20200119-001021_531760765 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:24:45,492] ({pool-2-thread-9} SchedulerFactory.java[jobStarted]:114) - Job 20200119-001021_531760765 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:24:45,570] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: runJob at PythonRDD.scala:153
 INFO [2020-01-19 00:24:45,572] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 2 (runJob at PythonRDD.scala:153) with 1 output partitions
 INFO [2020-01-19 00:24:45,574] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 4 (runJob at PythonRDD.scala:153)
 INFO [2020-01-19 00:24:45,574] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-01-19 00:24:45,575] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-01-19 00:24:45,575] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 4 (PythonRDD[20] at RDD at PythonRDD.scala:53), which has no missing parents
 INFO [2020-01-19 00:24:45,582] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_4 stored as values in memory (estimated size 10.2 KB, free 366.2 MB)
 INFO [2020-01-19 00:24:45,587] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.8 KB, free 366.2 MB)
 INFO [2020-01-19 00:24:45,588] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_4_piece0 in memory on zeppelin:33185 (size: 5.8 KB, free: 366.3 MB)
 INFO [2020-01-19 00:24:45,589] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 4 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:24:45,591] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 4 (PythonRDD[20] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))
 INFO [2020-01-19 00:24:45,592] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 4.0 with 1 tasks
 INFO [2020-01-19 00:24:45,593] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 4.0 (TID 14, 172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:24:45,628] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_4_piece0 in memory on 172.18.0.8:44045 (size: 5.8 KB, free: 912.3 MB)
 WARN [2020-01-19 00:24:45,860] ({task-result-getter-2} Logging.scala[logWarning]:66) - Lost task 0.0 in stage 4.0 (TID 14, 172.18.0.8, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main
    process()
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 393, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 1354, in takeUpToNumLeft
  File "<ipython-input-16-d877e8200fea>", line 3, in sumar
TypeError: unsupported operand type(s) for +: 'int' and 'Row'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)
	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

 INFO [2020-01-19 00:24:45,866] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.1 in stage 4.0 (TID 15, 172.18.0.7, executor 1, partition 0, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:24:45,895] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_4_piece0 in memory on 172.18.0.7:37317 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:24:46,062] ({task-result-getter-1} Logging.scala[logInfo]:54) - Lost task 0.1 in stage 4.0 (TID 15) on 172.18.0.7, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main
    process()
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 393, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 1354, in takeUpToNumLeft
  File "<ipython-input-16-d877e8200fea>", line 3, in sumar
TypeError: unsupported operand type(s) for +: 'int' and 'Row'
) [duplicate 1]
 INFO [2020-01-19 00:24:46,063] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.2 in stage 4.0 (TID 16, 172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:24:46,151] ({task-result-getter-3} Logging.scala[logInfo]:54) - Lost task 0.2 in stage 4.0 (TID 16) on 172.18.0.8, executor 0: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main
    process()
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 393, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 1354, in takeUpToNumLeft
  File "<ipython-input-16-d877e8200fea>", line 3, in sumar
TypeError: unsupported operand type(s) for +: 'int' and 'Row'
) [duplicate 2]
 INFO [2020-01-19 00:24:46,152] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.3 in stage 4.0 (TID 17, 172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:24:46,257] ({task-result-getter-0} Logging.scala[logInfo]:54) - Lost task 0.3 in stage 4.0 (TID 17) on 172.18.0.8, executor 0: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main
    process()
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 393, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 1354, in takeUpToNumLeft
  File "<ipython-input-16-d877e8200fea>", line 3, in sumar
TypeError: unsupported operand type(s) for +: 'int' and 'Row'
) [duplicate 3]
ERROR [2020-01-19 00:24:46,259] ({task-result-getter-0} Logging.scala[logError]:70) - Task 0 in stage 4.0 failed 4 times; aborting job
 INFO [2020-01-19 00:24:46,260] ({task-result-getter-0} Logging.scala[logInfo]:54) - Removed TaskSet 4.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:24:46,264] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Cancelling stage 4
 INFO [2020-01-19 00:24:46,265] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Killing all running tasks in stage 4: Stage cancelled
 INFO [2020-01-19 00:24:46,271] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 4 (runJob at PythonRDD.scala:153) failed in 0.695 s due to Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 17, 172.18.0.8, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main
    process()
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 393, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 1354, in takeUpToNumLeft
  File "<ipython-input-16-d877e8200fea>", line 3, in sumar
TypeError: unsupported operand type(s) for +: 'int' and 'Row'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)
	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
 INFO [2020-01-19 00:24:46,272] ({Thread-20} Logging.scala[logInfo]:54) - Job 2 failed: runJob at PythonRDD.scala:153, took 0.702055 s
 INFO [2020-01-19 00:24:46,596] ({pool-2-thread-9} SchedulerFactory.java[jobFinished]:120) - Job 20200119-001021_531760765 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:25:19,952] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200119-001021_531760765 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:25:20,029] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: runJob at PythonRDD.scala:153
 INFO [2020-01-19 00:25:20,030] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 3 (runJob at PythonRDD.scala:153) with 1 output partitions
 INFO [2020-01-19 00:25:20,033] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 5 (runJob at PythonRDD.scala:153)
 INFO [2020-01-19 00:25:20,034] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-01-19 00:25:20,035] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-01-19 00:25:20,035] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 5 (PythonRDD[21] at RDD at PythonRDD.scala:53), which has no missing parents
 INFO [2020-01-19 00:25:20,037] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_5 stored as values in memory (estimated size 10.2 KB, free 366.2 MB)
 INFO [2020-01-19 00:25:20,043] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.8 KB, free 366.2 MB)
 INFO [2020-01-19 00:25:20,046] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_5_piece0 in memory on zeppelin:33185 (size: 5.8 KB, free: 366.3 MB)
 INFO [2020-01-19 00:25:20,047] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 5 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:25:20,048] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 5 (PythonRDD[21] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))
 INFO [2020-01-19 00:25:20,049] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 5.0 with 1 tasks
 INFO [2020-01-19 00:25:20,051] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 5.0 (TID 18, 172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:25:20,074] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_5_piece0 in memory on 172.18.0.8:44045 (size: 5.8 KB, free: 912.3 MB)
 WARN [2020-01-19 00:25:20,157] ({task-result-getter-2} Logging.scala[logWarning]:66) - Lost task 0.0 in stage 5.0 (TID 18, 172.18.0.8, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main
    process()
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 393, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 1354, in takeUpToNumLeft
  File "<ipython-input-18-5589a9fc2c75>", line 3, in sumar
TypeError: unsupported operand type(s) for +: 'int' and 'Row'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)
	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

 INFO [2020-01-19 00:25:20,159] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.1 in stage 5.0 (TID 19, 172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:25:20,248] ({task-result-getter-1} Logging.scala[logInfo]:54) - Lost task 0.1 in stage 5.0 (TID 19) on 172.18.0.8, executor 0: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main
    process()
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 393, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 1354, in takeUpToNumLeft
  File "<ipython-input-18-5589a9fc2c75>", line 3, in sumar
TypeError: unsupported operand type(s) for +: 'int' and 'Row'
) [duplicate 1]
 INFO [2020-01-19 00:25:20,250] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.2 in stage 5.0 (TID 20, 172.18.0.6, executor 2, partition 0, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:25:20,275] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_5_piece0 in memory on 172.18.0.6:42495 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:25:20,474] ({task-result-getter-3} Logging.scala[logInfo]:54) - Lost task 0.2 in stage 5.0 (TID 20) on 172.18.0.6, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main
    process()
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 393, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 1354, in takeUpToNumLeft
  File "<ipython-input-18-5589a9fc2c75>", line 3, in sumar
TypeError: unsupported operand type(s) for +: 'int' and 'Row'
) [duplicate 2]
 INFO [2020-01-19 00:25:20,475] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.3 in stage 5.0 (TID 21, 172.18.0.7, executor 1, partition 0, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:25:20,501] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_5_piece0 in memory on 172.18.0.7:37317 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:25:20,627] ({task-result-getter-0} Logging.scala[logInfo]:54) - Lost task 0.3 in stage 5.0 (TID 21) on 172.18.0.7, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main
    process()
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 393, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 1354, in takeUpToNumLeft
  File "<ipython-input-18-5589a9fc2c75>", line 3, in sumar
TypeError: unsupported operand type(s) for +: 'int' and 'Row'
) [duplicate 3]
ERROR [2020-01-19 00:25:20,628] ({task-result-getter-0} Logging.scala[logError]:70) - Task 0 in stage 5.0 failed 4 times; aborting job
 INFO [2020-01-19 00:25:20,630] ({task-result-getter-0} Logging.scala[logInfo]:54) - Removed TaskSet 5.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:25:20,630] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Cancelling stage 5
 INFO [2020-01-19 00:25:20,630] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Killing all running tasks in stage 5: Stage cancelled
 INFO [2020-01-19 00:25:20,631] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 5 (runJob at PythonRDD.scala:153) failed in 0.595 s due to Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 21, 172.18.0.7, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main
    process()
  File "/spark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 393, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 1354, in takeUpToNumLeft
  File "<ipython-input-18-5589a9fc2c75>", line 3, in sumar
TypeError: unsupported operand type(s) for +: 'int' and 'Row'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)
	at org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
 INFO [2020-01-19 00:25:20,632] ({Thread-20} Logging.scala[logInfo]:54) - Job 3 failed: runJob at PythonRDD.scala:153, took 0.602562 s
 INFO [2020-01-19 00:25:20,991] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200119-001021_531760765 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:26:06,818] ({pool-2-thread-7} SchedulerFactory.java[jobStarted]:114) - Job 20200119-001021_531760765 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:26:06,862] ({pool-2-thread-7} SchedulerFactory.java[jobFinished]:120) - Job 20200119-001021_531760765 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:26:15,814] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200119-001021_531760765 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:26:15,872] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: runJob at PythonRDD.scala:153
 INFO [2020-01-19 00:26:15,872] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 4 (runJob at PythonRDD.scala:153) with 1 output partitions
 INFO [2020-01-19 00:26:15,873] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 6 (runJob at PythonRDD.scala:153)
 INFO [2020-01-19 00:26:15,873] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-01-19 00:26:15,873] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-01-19 00:26:15,873] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 6 (PythonRDD[22] at RDD at PythonRDD.scala:53), which has no missing parents
 INFO [2020-01-19 00:26:15,875] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_6 stored as values in memory (estimated size 10.2 KB, free 366.2 MB)
 INFO [2020-01-19 00:26:15,877] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.8 KB, free 366.2 MB)
 INFO [2020-01-19 00:26:15,878] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_6_piece0 in memory on zeppelin:33185 (size: 5.8 KB, free: 366.3 MB)
 INFO [2020-01-19 00:26:15,881] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 6 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:26:15,881] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 6 (PythonRDD[22] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))
 INFO [2020-01-19 00:26:15,881] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 6.0 with 1 tasks
 INFO [2020-01-19 00:26:15,882] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 6.0 (TID 22, 172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:26:15,898] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_6_piece0 in memory on 172.18.0.8:44045 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:26:15,974] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 6.0 (TID 22) in 92 ms on 172.18.0.8 (executor 0) (1/1)
 INFO [2020-01-19 00:26:15,975] ({task-result-getter-2} Logging.scala[logInfo]:54) - Removed TaskSet 6.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:26:15,978] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 6 (runJob at PythonRDD.scala:153) finished in 0.104 s
 INFO [2020-01-19 00:26:15,979] ({Thread-20} Logging.scala[logInfo]:54) - Job 4 finished: runJob at PythonRDD.scala:153, took 0.106877 s
 INFO [2020-01-19 00:26:16,000] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: runJob at PythonRDD.scala:153
 INFO [2020-01-19 00:26:16,001] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 5 (runJob at PythonRDD.scala:153) with 4 output partitions
 INFO [2020-01-19 00:26:16,002] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 7 (runJob at PythonRDD.scala:153)
 INFO [2020-01-19 00:26:16,002] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-01-19 00:26:16,003] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-01-19 00:26:16,003] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 7 (PythonRDD[23] at RDD at PythonRDD.scala:53), which has no missing parents
 INFO [2020-01-19 00:26:16,009] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_7 stored as values in memory (estimated size 10.2 KB, free 366.2 MB)
 INFO [2020-01-19 00:26:16,011] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.8 KB, free 366.2 MB)
 INFO [2020-01-19 00:26:16,012] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_7_piece0 in memory on zeppelin:33185 (size: 5.8 KB, free: 366.3 MB)
 INFO [2020-01-19 00:26:16,031] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 7 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:26:16,032] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 4 missing tasks from ResultStage 7 (PythonRDD[23] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(1, 2, 3, 4))
 INFO [2020-01-19 00:26:16,032] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 7.0 with 4 tasks
 INFO [2020-01-19 00:26:16,033] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 7.0 (TID 23, 172.18.0.7, executor 1, partition 1, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:26:16,034] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 1.0 in stage 7.0 (TID 24, 172.18.0.6, executor 2, partition 2, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:26:16,035] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 2.0 in stage 7.0 (TID 25, 172.18.0.8, executor 0, partition 3, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:26:16,035] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 3.0 in stage 7.0 (TID 26, 172.18.0.7, executor 1, partition 4, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:26:16,070] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_7_piece0 in memory on 172.18.0.6:42495 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:26:16,077] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_7_piece0 in memory on 172.18.0.7:37317 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:26:16,087] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_7_piece0 in memory on 172.18.0.8:44045 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:26:16,183] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 1.0 in stage 7.0 (TID 24) in 149 ms on 172.18.0.6 (executor 2) (1/4)
 INFO [2020-01-19 00:26:16,195] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 2.0 in stage 7.0 (TID 25) in 161 ms on 172.18.0.8 (executor 0) (2/4)
 INFO [2020-01-19 00:26:16,243] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 7.0 (TID 23) in 210 ms on 172.18.0.7 (executor 1) (3/4)
 INFO [2020-01-19 00:26:16,300] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 3.0 in stage 7.0 (TID 26) in 265 ms on 172.18.0.7 (executor 1) (4/4)
 INFO [2020-01-19 00:26:16,301] ({task-result-getter-2} Logging.scala[logInfo]:54) - Removed TaskSet 7.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:26:16,302] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 7 (runJob at PythonRDD.scala:153) finished in 0.295 s
 INFO [2020-01-19 00:26:16,302] ({Thread-20} Logging.scala[logInfo]:54) - Job 5 finished: runJob at PythonRDD.scala:153, took 0.301431 s
 INFO [2020-01-19 00:26:16,335] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: runJob at PythonRDD.scala:153
 INFO [2020-01-19 00:26:16,336] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 6 (runJob at PythonRDD.scala:153) with 1 output partitions
 INFO [2020-01-19 00:26:16,338] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 8 (runJob at PythonRDD.scala:153)
 INFO [2020-01-19 00:26:16,338] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-01-19 00:26:16,338] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-01-19 00:26:16,338] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 8 (PythonRDD[24] at RDD at PythonRDD.scala:53), which has no missing parents
 INFO [2020-01-19 00:26:16,346] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_8 stored as values in memory (estimated size 10.2 KB, free 366.2 MB)
 INFO [2020-01-19 00:26:16,349] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_8_piece0 stored as bytes in memory (estimated size 5.8 KB, free 366.2 MB)
 INFO [2020-01-19 00:26:16,350] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_8_piece0 in memory on zeppelin:33185 (size: 5.8 KB, free: 366.2 MB)
 INFO [2020-01-19 00:26:16,351] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 8 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:26:16,354] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 8 (PythonRDD[24] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(5))
 INFO [2020-01-19 00:26:16,355] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 8.0 with 1 tasks
 INFO [2020-01-19 00:26:16,356] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 8.0 (TID 27, 172.18.0.8, executor 0, partition 5, PROCESS_LOCAL, 7920 bytes)
 INFO [2020-01-19 00:26:16,368] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_8_piece0 in memory on 172.18.0.8:44045 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:26:16,451] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 8.0 (TID 27) in 95 ms on 172.18.0.8 (executor 0) (1/1)
 INFO [2020-01-19 00:26:16,452] ({task-result-getter-1} Logging.scala[logInfo]:54) - Removed TaskSet 8.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:26:16,452] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 8 (runJob at PythonRDD.scala:153) finished in 0.113 s
 INFO [2020-01-19 00:26:16,453] ({Thread-20} Logging.scala[logInfo]:54) - Job 6 finished: runJob at PythonRDD.scala:153, took 0.117799 s
 INFO [2020-01-19 00:26:16,526] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200119-001021_531760765 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:26:26,530] ({pool-2-thread-8} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:26:26,585] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 4
 INFO [2020-01-19 00:26:26,640] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned shuffle 1
 INFO [2020-01-19 00:26:26,641] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 216
 INFO [2020-01-19 00:26:26,641] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 150
 INFO [2020-01-19 00:26:26,642] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 36
 INFO [2020-01-19 00:26:26,643] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 227
 INFO [2020-01-19 00:26:26,643] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 244
 INFO [2020-01-19 00:26:26,643] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 92
 INFO [2020-01-19 00:26:26,644] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 245
 INFO [2020-01-19 00:26:26,644] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 155
 INFO [2020-01-19 00:26:26,645] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 59
 INFO [2020-01-19 00:26:26,645] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 228
 INFO [2020-01-19 00:26:26,646] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 66
 INFO [2020-01-19 00:26:26,646] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 5
 INFO [2020-01-19 00:26:26,646] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 200
 INFO [2020-01-19 00:26:26,646] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 103
 INFO [2020-01-19 00:26:26,647] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 146
 INFO [2020-01-19 00:26:26,647] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 101
 INFO [2020-01-19 00:26:26,648] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 15
 INFO [2020-01-19 00:26:26,648] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 185
 INFO [2020-01-19 00:26:26,648] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 123
 INFO [2020-01-19 00:26:26,649] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 186
 INFO [2020-01-19 00:26:26,649] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 213
 INFO [2020-01-19 00:26:26,650] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 55
 INFO [2020-01-19 00:26:26,650] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 145
 INFO [2020-01-19 00:26:26,650] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 77
 INFO [2020-01-19 00:26:26,687] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_6_piece0 on 172.18.0.8:44045 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:26:26,700] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_6_piece0 on zeppelin:33185 in memory (size: 5.8 KB, free: 366.3 MB)
 INFO [2020-01-19 00:26:26,719] ({pool-2-thread-8} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:26:26,788] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 24
 INFO [2020-01-19 00:26:26,788] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 69
 INFO [2020-01-19 00:26:26,803] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_0_piece0 on 172.18.0.8:44045 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:26:26,806] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_0_piece0 on 172.18.0.6:42495 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:26:26,809] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_0_piece0 on 172.18.0.7:37317 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:26:26,823] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_0_piece0 on zeppelin:33185 in memory (size: 7.2 KB, free: 366.3 MB)
 INFO [2020-01-19 00:26:26,864] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 31
 INFO [2020-01-19 00:26:26,864] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 214
 INFO [2020-01-19 00:26:26,865] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 254
 INFO [2020-01-19 00:26:26,865] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 171
 INFO [2020-01-19 00:26:26,865] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 255
 INFO [2020-01-19 00:26:26,865] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 107
 INFO [2020-01-19 00:26:26,865] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 118
 INFO [2020-01-19 00:26:26,865] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 43
 INFO [2020-01-19 00:26:26,865] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 178
 INFO [2020-01-19 00:26:26,865] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 223
 INFO [2020-01-19 00:26:26,865] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 117
 INFO [2020-01-19 00:26:26,866] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 138
 INFO [2020-01-19 00:26:26,866] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 163
 INFO [2020-01-19 00:26:26,866] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 251
 INFO [2020-01-19 00:26:26,866] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 253
 INFO [2020-01-19 00:26:26,867] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 90
 INFO [2020-01-19 00:26:26,867] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 250
 INFO [2020-01-19 00:26:26,867] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 89
 INFO [2020-01-19 00:26:26,867] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 225
 INFO [2020-01-19 00:26:26,867] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 170
 INFO [2020-01-19 00:26:26,867] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 240
 INFO [2020-01-19 00:26:26,867] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 143
 INFO [2020-01-19 00:26:26,868] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 197
 INFO [2020-01-19 00:26:26,868] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 246
 INFO [2020-01-19 00:26:26,868] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 133
 INFO [2020-01-19 00:26:26,868] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 119
 INFO [2020-01-19 00:26:26,868] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 141
 INFO [2020-01-19 00:26:26,868] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 93
 INFO [2020-01-19 00:26:26,868] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 38
 INFO [2020-01-19 00:26:26,868] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 137
 INFO [2020-01-19 00:26:26,869] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 230
 INFO [2020-01-19 00:26:26,869] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 243
 INFO [2020-01-19 00:26:26,869] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 11
 INFO [2020-01-19 00:26:26,869] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 173
 INFO [2020-01-19 00:26:26,869] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 50
 INFO [2020-01-19 00:26:26,869] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 75
 INFO [2020-01-19 00:26:26,869] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 74
 INFO [2020-01-19 00:26:26,879] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_3_piece0 on 172.18.0.8:44045 in memory (size: 4.3 KB, free: 912.3 MB)
 INFO [2020-01-19 00:26:26,888] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_3_piece0 on zeppelin:33185 in memory (size: 4.3 KB, free: 366.3 MB)
 INFO [2020-01-19 00:26:26,900] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 144
 INFO [2020-01-19 00:26:26,900] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 158
 INFO [2020-01-19 00:26:26,900] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 241
 INFO [2020-01-19 00:26:26,901] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 235
 INFO [2020-01-19 00:26:26,901] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 23
 INFO [2020-01-19 00:26:26,901] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 2
 INFO [2020-01-19 00:26:26,901] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 13
 INFO [2020-01-19 00:26:26,901] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 3
 INFO [2020-01-19 00:26:26,901] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 48
 INFO [2020-01-19 00:26:26,901] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 124
 INFO [2020-01-19 00:26:26,901] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 130
 INFO [2020-01-19 00:26:26,901] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 165
 INFO [2020-01-19 00:26:26,901] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 177
 INFO [2020-01-19 00:26:26,902] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 226
 INFO [2020-01-19 00:26:26,902] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 111
 INFO [2020-01-19 00:26:26,902] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 161
 INFO [2020-01-19 00:26:26,902] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 26
 INFO [2020-01-19 00:26:26,902] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 219
 INFO [2020-01-19 00:26:26,902] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 154
 INFO [2020-01-19 00:26:26,902] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 149
 INFO [2020-01-19 00:26:26,902] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 229
 INFO [2020-01-19 00:26:26,902] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 99
 INFO [2020-01-19 00:26:26,902] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 142
 INFO [2020-01-19 00:26:26,903] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 120
 INFO [2020-01-19 00:26:26,903] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 172
 INFO [2020-01-19 00:26:26,903] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 88
 INFO [2020-01-19 00:26:26,903] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 187
 INFO [2020-01-19 00:26:26,903] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 236
 INFO [2020-01-19 00:26:26,903] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 37
 INFO [2020-01-19 00:26:26,903] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 180
 INFO [2020-01-19 00:26:26,903] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 220
 INFO [2020-01-19 00:26:26,903] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 218
 INFO [2020-01-19 00:26:26,904] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 27
 INFO [2020-01-19 00:26:26,904] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 71
 INFO [2020-01-19 00:26:26,904] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 62
 INFO [2020-01-19 00:26:26,904] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 156
 INFO [2020-01-19 00:26:26,904] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 242
 INFO [2020-01-19 00:26:26,904] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 61
 INFO [2020-01-19 00:26:26,904] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 72
 INFO [2020-01-19 00:26:26,904] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 193
 INFO [2020-01-19 00:26:26,904] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 96
 INFO [2020-01-19 00:26:26,905] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 157
 INFO [2020-01-19 00:26:26,905] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 44
 INFO [2020-01-19 00:26:26,905] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 139
 INFO [2020-01-19 00:26:26,905] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 176
 INFO [2020-01-19 00:26:26,905] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 128
 INFO [2020-01-19 00:26:26,905] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 45
 INFO [2020-01-19 00:26:26,905] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 181
 INFO [2020-01-19 00:26:26,905] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 207
 INFO [2020-01-19 00:26:26,905] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 126
 INFO [2020-01-19 00:26:26,906] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 32
 INFO [2020-01-19 00:26:26,906] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 256
 INFO [2020-01-19 00:26:26,906] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 19
 INFO [2020-01-19 00:26:26,906] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 46
 INFO [2020-01-19 00:26:26,906] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 238
 INFO [2020-01-19 00:26:26,906] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 210
 INFO [2020-01-19 00:26:26,906] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 195
 INFO [2020-01-19 00:26:26,906] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 237
 INFO [2020-01-19 00:26:26,907] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 91
 INFO [2020-01-19 00:26:26,907] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 167
 INFO [2020-01-19 00:26:26,907] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 182
 INFO [2020-01-19 00:26:26,907] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 53
 INFO [2020-01-19 00:26:26,907] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 6
 INFO [2020-01-19 00:26:26,907] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 189
 INFO [2020-01-19 00:26:26,907] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 80
 INFO [2020-01-19 00:26:26,907] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 199
 INFO [2020-01-19 00:26:26,907] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 159
 INFO [2020-01-19 00:26:26,908] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 65
 INFO [2020-01-19 00:26:26,908] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 234
 INFO [2020-01-19 00:26:26,908] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 29
 INFO [2020-01-19 00:26:26,908] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 10
 INFO [2020-01-19 00:26:26,908] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 70
 INFO [2020-01-19 00:26:26,908] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 64
 INFO [2020-01-19 00:26:26,908] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 129
 INFO [2020-01-19 00:26:26,908] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 211
 INFO [2020-01-19 00:26:26,909] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 196
 INFO [2020-01-19 00:26:26,926] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_8_piece0 on 172.18.0.8:44045 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:26:26,931] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_8_piece0 on zeppelin:33185 in memory (size: 5.8 KB, free: 366.3 MB)
 INFO [2020-01-19 00:26:26,934] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 151
 INFO [2020-01-19 00:26:26,934] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 191
 INFO [2020-01-19 00:26:26,934] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 201
 INFO [2020-01-19 00:26:26,934] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 109
 INFO [2020-01-19 00:26:26,935] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 28
 INFO [2020-01-19 00:26:26,935] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 231
 INFO [2020-01-19 00:26:26,935] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 147
 INFO [2020-01-19 00:26:26,935] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 247
 INFO [2020-01-19 00:26:26,935] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 9
 INFO [2020-01-19 00:26:26,935] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 84
 INFO [2020-01-19 00:26:26,935] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 140
 INFO [2020-01-19 00:26:26,935] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 148
 INFO [2020-01-19 00:26:26,935] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 30
 INFO [2020-01-19 00:26:26,936] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 49
 INFO [2020-01-19 00:26:26,936] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 108
 INFO [2020-01-19 00:26:26,936] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 104
 INFO [2020-01-19 00:26:26,936] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 82
 INFO [2020-01-19 00:26:26,948] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_4_piece0 on zeppelin:33185 in memory (size: 5.8 KB, free: 366.3 MB)
 INFO [2020-01-19 00:26:26,951] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_4_piece0 on 172.18.0.8:44045 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:26:26,952] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_4_piece0 on 172.18.0.7:37317 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:26:26,964] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 192
 INFO [2020-01-19 00:26:26,965] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 114
 INFO [2020-01-19 00:26:26,965] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 68
 INFO [2020-01-19 00:26:26,965] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 169
 INFO [2020-01-19 00:26:26,965] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 168
 INFO [2020-01-19 00:26:26,965] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 166
 INFO [2020-01-19 00:26:26,977] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_2_piece0 on 172.18.0.7:37317 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:26:26,978] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_2_piece0 on 172.18.0.8:44045 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:26:26,980] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_2_piece0 on 172.18.0.6:42495 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:26:26,980] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_2_piece0 on zeppelin:33185 in memory (size: 7.2 KB, free: 366.3 MB)
 INFO [2020-01-19 00:26:26,991] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 203
 INFO [2020-01-19 00:26:26,991] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 212
 INFO [2020-01-19 00:26:26,991] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 248
 INFO [2020-01-19 00:26:26,991] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 8
 INFO [2020-01-19 00:26:26,992] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 7
 INFO [2020-01-19 00:26:26,992] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 116
 INFO [2020-01-19 00:26:26,992] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 42
 INFO [2020-01-19 00:26:26,992] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 60
 INFO [2020-01-19 00:26:26,992] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 224
 INFO [2020-01-19 00:26:26,992] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 222
 INFO [2020-01-19 00:26:26,992] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 102
 INFO [2020-01-19 00:26:26,992] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 249
 INFO [2020-01-19 00:26:26,992] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 81
 INFO [2020-01-19 00:26:26,992] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 205
 INFO [2020-01-19 00:26:26,993] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 86
 INFO [2020-01-19 00:26:26,993] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 206
 INFO [2020-01-19 00:26:26,993] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 233
 INFO [2020-01-19 00:26:26,993] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 94
 INFO [2020-01-19 00:26:26,993] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 179
 INFO [2020-01-19 00:26:26,993] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 54
 INFO [2020-01-19 00:26:26,993] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 76
 INFO [2020-01-19 00:26:26,993] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 18
 INFO [2020-01-19 00:26:27,002] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_1_piece0 on 172.18.0.7:37317 in memory (size: 4.3 KB, free: 912.3 MB)
 INFO [2020-01-19 00:26:27,007] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_1_piece0 on zeppelin:33185 in memory (size: 4.3 KB, free: 366.3 MB)
 INFO [2020-01-19 00:26:27,020] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 63
 INFO [2020-01-19 00:26:27,020] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 183
 INFO [2020-01-19 00:26:27,020] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 134
 INFO [2020-01-19 00:26:27,020] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 98
 INFO [2020-01-19 00:26:27,020] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 184
 INFO [2020-01-19 00:26:27,020] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 14
 INFO [2020-01-19 00:26:27,020] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 105
 INFO [2020-01-19 00:26:27,020] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 135
 INFO [2020-01-19 00:26:27,020] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 22
 INFO [2020-01-19 00:26:27,028] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned shuffle 0
 INFO [2020-01-19 00:26:27,028] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 152
 INFO [2020-01-19 00:26:27,028] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 16
 INFO [2020-01-19 00:26:27,028] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 95
 INFO [2020-01-19 00:26:27,028] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 97
 INFO [2020-01-19 00:26:27,028] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 190
 INFO [2020-01-19 00:26:27,028] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 215
 INFO [2020-01-19 00:26:27,028] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 252
 INFO [2020-01-19 00:26:27,029] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 56
 INFO [2020-01-19 00:26:27,029] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 136
 INFO [2020-01-19 00:26:27,029] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 239
 INFO [2020-01-19 00:26:27,029] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 122
 INFO [2020-01-19 00:26:27,030] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 127
 INFO [2020-01-19 00:26:27,030] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 73
 INFO [2020-01-19 00:26:27,030] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 162
 INFO [2020-01-19 00:26:27,030] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 132
 INFO [2020-01-19 00:26:27,036] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_7_piece0 on 172.18.0.6:42495 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:26:27,037] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_7_piece0 on 172.18.0.8:44045 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:26:27,039] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_7_piece0 on 172.18.0.7:37317 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:26:27,043] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_7_piece0 on zeppelin:33185 in memory (size: 5.8 KB, free: 366.3 MB)
 INFO [2020-01-19 00:26:27,055] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 160
 INFO [2020-01-19 00:26:27,055] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 47
 INFO [2020-01-19 00:26:27,055] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 100
 INFO [2020-01-19 00:26:27,055] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 209
 INFO [2020-01-19 00:26:27,055] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 217
 INFO [2020-01-19 00:26:27,055] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 112
 INFO [2020-01-19 00:26:27,055] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 21
 INFO [2020-01-19 00:26:27,055] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 232
 INFO [2020-01-19 00:26:27,056] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 174
 INFO [2020-01-19 00:26:27,056] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 204
 INFO [2020-01-19 00:26:27,056] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 35
 INFO [2020-01-19 00:26:27,056] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 40
 INFO [2020-01-19 00:26:27,056] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 39
 INFO [2020-01-19 00:26:27,056] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 52
 INFO [2020-01-19 00:26:27,056] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 208
 INFO [2020-01-19 00:26:27,056] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 58
 INFO [2020-01-19 00:26:27,056] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 113
 INFO [2020-01-19 00:26:27,056] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 51
 INFO [2020-01-19 00:26:27,057] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 110
 INFO [2020-01-19 00:26:27,057] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 194
 INFO [2020-01-19 00:26:27,057] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 41
 INFO [2020-01-19 00:26:27,057] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 78
 INFO [2020-01-19 00:26:27,057] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 33
 INFO [2020-01-19 00:26:27,057] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 125
 INFO [2020-01-19 00:26:27,057] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 153
 INFO [2020-01-19 00:26:27,057] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 188
 INFO [2020-01-19 00:26:27,057] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 221
 INFO [2020-01-19 00:26:27,057] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 87
 INFO [2020-01-19 00:26:27,058] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 34
 INFO [2020-01-19 00:26:27,058] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 115
 INFO [2020-01-19 00:26:27,058] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 106
 INFO [2020-01-19 00:26:27,058] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 67
 INFO [2020-01-19 00:26:27,058] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 164
 INFO [2020-01-19 00:26:27,058] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 25
 INFO [2020-01-19 00:26:27,058] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 85
 INFO [2020-01-19 00:26:27,058] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 198
 INFO [2020-01-19 00:26:27,059] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 17
 INFO [2020-01-19 00:26:27,059] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 121
 INFO [2020-01-19 00:26:27,059] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 175
 INFO [2020-01-19 00:26:27,059] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 83
 INFO [2020-01-19 00:26:27,059] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 12
 INFO [2020-01-19 00:26:27,063] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_5_piece0 on 172.18.0.6:42495 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:26:27,064] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_5_piece0 on 172.18.0.7:37317 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:26:27,066] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_5_piece0 on zeppelin:33185 in memory (size: 5.8 KB, free: 366.3 MB)
 INFO [2020-01-19 00:26:27,070] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_5_piece0 on 172.18.0.8:44045 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:26:27,076] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 20
 INFO [2020-01-19 00:26:27,076] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 57
 INFO [2020-01-19 00:26:27,077] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 79
 INFO [2020-01-19 00:26:27,077] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 202
 INFO [2020-01-19 00:26:29,761] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20200119-001021_531760765 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:26:29,831] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: runJob at PythonRDD.scala:153
 INFO [2020-01-19 00:26:29,835] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 7 (runJob at PythonRDD.scala:153) with 1 output partitions
 INFO [2020-01-19 00:26:29,835] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 9 (runJob at PythonRDD.scala:153)
 INFO [2020-01-19 00:26:29,835] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-01-19 00:26:29,836] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-01-19 00:26:29,837] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 9 (PythonRDD[33] at RDD at PythonRDD.scala:53), which has no missing parents
 INFO [2020-01-19 00:26:29,843] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_9 stored as values in memory (estimated size 10.2 KB, free 366.3 MB)
 INFO [2020-01-19 00:26:29,846] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_9_piece0 stored as bytes in memory (estimated size 5.8 KB, free 366.3 MB)
 INFO [2020-01-19 00:26:29,847] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_9_piece0 in memory on zeppelin:33185 (size: 5.8 KB, free: 366.3 MB)
 INFO [2020-01-19 00:26:29,848] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 9 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:26:29,849] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 9 (PythonRDD[33] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))
 INFO [2020-01-19 00:26:29,849] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 9.0 with 1 tasks
 INFO [2020-01-19 00:26:29,851] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 9.0 (TID 28, 172.18.0.7, executor 1, partition 0, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:26:29,867] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_9_piece0 in memory on 172.18.0.7:37317 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:26:29,944] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 9.0 (TID 28) in 93 ms on 172.18.0.7 (executor 1) (1/1)
 INFO [2020-01-19 00:26:29,944] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 9.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:26:29,945] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 9 (runJob at PythonRDD.scala:153) finished in 0.104 s
 INFO [2020-01-19 00:26:29,947] ({Thread-20} Logging.scala[logInfo]:54) - Job 7 finished: runJob at PythonRDD.scala:153, took 0.114860 s
 INFO [2020-01-19 00:26:29,971] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: runJob at PythonRDD.scala:153
 INFO [2020-01-19 00:26:29,972] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 8 (runJob at PythonRDD.scala:153) with 4 output partitions
 INFO [2020-01-19 00:26:29,976] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 10 (runJob at PythonRDD.scala:153)
 INFO [2020-01-19 00:26:29,976] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-01-19 00:26:29,976] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-01-19 00:26:29,977] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 10 (PythonRDD[34] at RDD at PythonRDD.scala:53), which has no missing parents
 INFO [2020-01-19 00:26:29,980] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_10 stored as values in memory (estimated size 10.2 KB, free 366.3 MB)
 INFO [2020-01-19 00:26:29,996] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_10_piece0 stored as bytes in memory (estimated size 5.8 KB, free 366.3 MB)
 INFO [2020-01-19 00:26:29,999] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_10_piece0 in memory on zeppelin:33185 (size: 5.8 KB, free: 366.3 MB)
 INFO [2020-01-19 00:26:30,000] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 10 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:26:30,001] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 4 missing tasks from ResultStage 10 (PythonRDD[34] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(1, 2, 3, 4))
 INFO [2020-01-19 00:26:30,001] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 10.0 with 4 tasks
 INFO [2020-01-19 00:26:30,002] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 10.0 (TID 29, 172.18.0.6, executor 2, partition 1, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:26:30,003] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 1.0 in stage 10.0 (TID 30, 172.18.0.8, executor 0, partition 2, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:26:30,003] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 2.0 in stage 10.0 (TID 31, 172.18.0.7, executor 1, partition 3, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:26:30,003] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 3.0 in stage 10.0 (TID 32, 172.18.0.6, executor 2, partition 4, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:26:30,031] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_10_piece0 in memory on 172.18.0.7:37317 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:26:30,040] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_10_piece0 in memory on 172.18.0.8:44045 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:26:30,101] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_10_piece0 in memory on 172.18.0.6:42495 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:26:30,127] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 1.0 in stage 10.0 (TID 30) in 125 ms on 172.18.0.8 (executor 0) (1/4)
 INFO [2020-01-19 00:26:30,137] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 2.0 in stage 10.0 (TID 31) in 134 ms on 172.18.0.7 (executor 1) (2/4)
 INFO [2020-01-19 00:26:30,221] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 10.0 (TID 29) in 219 ms on 172.18.0.6 (executor 2) (3/4)
 INFO [2020-01-19 00:26:30,229] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 3.0 in stage 10.0 (TID 32) in 226 ms on 172.18.0.6 (executor 2) (4/4)
 INFO [2020-01-19 00:26:30,232] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 10.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:26:30,233] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 10 (runJob at PythonRDD.scala:153) finished in 0.253 s
 INFO [2020-01-19 00:26:30,233] ({Thread-20} Logging.scala[logInfo]:54) - Job 8 finished: runJob at PythonRDD.scala:153, took 0.261743 s
 INFO [2020-01-19 00:26:30,263] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: runJob at PythonRDD.scala:153
 INFO [2020-01-19 00:26:30,273] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 9 (runJob at PythonRDD.scala:153) with 1 output partitions
 INFO [2020-01-19 00:26:30,274] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 11 (runJob at PythonRDD.scala:153)
 INFO [2020-01-19 00:26:30,274] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-01-19 00:26:30,275] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-01-19 00:26:30,276] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 11 (PythonRDD[35] at RDD at PythonRDD.scala:53), which has no missing parents
 INFO [2020-01-19 00:26:30,285] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_11 stored as values in memory (estimated size 10.2 KB, free 366.3 MB)
 INFO [2020-01-19 00:26:30,286] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_11_piece0 stored as bytes in memory (estimated size 5.8 KB, free 366.3 MB)
 INFO [2020-01-19 00:26:30,287] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_11_piece0 in memory on zeppelin:33185 (size: 5.8 KB, free: 366.3 MB)
 INFO [2020-01-19 00:26:30,288] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 11 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:26:30,288] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 11 (PythonRDD[35] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(5))
 INFO [2020-01-19 00:26:30,288] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 11.0 with 1 tasks
 INFO [2020-01-19 00:26:30,289] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 11.0 (TID 33, 172.18.0.6, executor 2, partition 5, PROCESS_LOCAL, 7920 bytes)
 INFO [2020-01-19 00:26:30,306] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_11_piece0 in memory on 172.18.0.6:42495 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:26:30,375] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 11.0 (TID 33) in 86 ms on 172.18.0.6 (executor 2) (1/1)
 INFO [2020-01-19 00:26:30,376] ({task-result-getter-0} Logging.scala[logInfo]:54) - Removed TaskSet 11.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:26:30,377] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 11 (runJob at PythonRDD.scala:153) finished in 0.094 s
 INFO [2020-01-19 00:26:30,377] ({Thread-20} Logging.scala[logInfo]:54) - Job 9 finished: runJob at PythonRDD.scala:153, took 0.104501 s
 INFO [2020-01-19 00:26:30,439] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20200119-001021_531760765 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:27:05,764] ({pool-2-thread-6} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:27:05,857] ({pool-2-thread-6} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:27:06,552] ({pool-2-thread-9} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:27:06,677] ({pool-2-thread-9} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:27:07,958] ({pool-2-thread-10} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:27:08,089] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: collect at <ipython-input-32-4294447afb77>:4
 INFO [2020-01-19 00:27:08,097] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Registering RDD 48 (collect at <ipython-input-32-4294447afb77>:4)
 INFO [2020-01-19 00:27:08,098] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 10 (collect at <ipython-input-32-4294447afb77>:4) with 1 output partitions
 INFO [2020-01-19 00:27:08,098] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 13 (collect at <ipython-input-32-4294447afb77>:4)
 INFO [2020-01-19 00:27:08,099] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List(ShuffleMapStage 12)
 INFO [2020-01-19 00:27:08,099] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List(ShuffleMapStage 12)
 INFO [2020-01-19 00:27:08,100] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ShuffleMapStage 12 (MapPartitionsRDD[48] at collect at <ipython-input-32-4294447afb77>:4), which has no missing parents
 INFO [2020-01-19 00:27:08,111] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_12 stored as values in memory (estimated size 13.6 KB, free 366.2 MB)
 INFO [2020-01-19 00:27:08,113] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_12_piece0 stored as bytes in memory (estimated size 7.2 KB, free 366.2 MB)
 INFO [2020-01-19 00:27:08,114] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_12_piece0 in memory on zeppelin:33185 (size: 7.2 KB, free: 366.3 MB)
 INFO [2020-01-19 00:27:08,114] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 12 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:27:08,115] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 6 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[48] at collect at <ipython-input-32-4294447afb77>:4) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
 INFO [2020-01-19 00:27:08,117] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 12.0 with 6 tasks
 INFO [2020-01-19 00:27:08,118] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 12.0 (TID 34, 172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:27:08,118] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 1.0 in stage 12.0 (TID 35, 172.18.0.7, executor 1, partition 1, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:27:08,119] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 2.0 in stage 12.0 (TID 36, 172.18.0.6, executor 2, partition 2, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:27:08,119] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 3.0 in stage 12.0 (TID 37, 172.18.0.8, executor 0, partition 3, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:27:08,120] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 4.0 in stage 12.0 (TID 38, 172.18.0.7, executor 1, partition 4, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:27:08,120] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 5.0 in stage 12.0 (TID 39, 172.18.0.6, executor 2, partition 5, PROCESS_LOCAL, 7909 bytes)
 INFO [2020-01-19 00:27:08,157] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_12_piece0 in memory on 172.18.0.7:37317 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:08,166] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_12_piece0 in memory on 172.18.0.6:42495 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:08,174] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_12_piece0 in memory on 172.18.0.8:44045 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:08,210] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 4.0 in stage 12.0 (TID 38) in 90 ms on 172.18.0.7 (executor 1) (1/6)
 INFO [2020-01-19 00:27:08,260] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 1.0 in stage 12.0 (TID 35) in 142 ms on 172.18.0.7 (executor 1) (2/6)
 INFO [2020-01-19 00:27:08,288] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 12.0 (TID 34) in 170 ms on 172.18.0.8 (executor 0) (3/6)
 INFO [2020-01-19 00:27:08,289] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 5.0 in stage 12.0 (TID 39) in 169 ms on 172.18.0.6 (executor 2) (4/6)
 INFO [2020-01-19 00:27:08,301] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 2.0 in stage 12.0 (TID 36) in 182 ms on 172.18.0.6 (executor 2) (5/6)
 INFO [2020-01-19 00:27:08,303] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 3.0 in stage 12.0 (TID 37) in 184 ms on 172.18.0.8 (executor 0) (6/6)
 INFO [2020-01-19 00:27:08,304] ({task-result-getter-1} Logging.scala[logInfo]:54) - Removed TaskSet 12.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:27:08,312] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ShuffleMapStage 12 (collect at <ipython-input-32-4294447afb77>:4) finished in 0.210 s
 INFO [2020-01-19 00:27:08,323] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - looking for newly runnable stages
 INFO [2020-01-19 00:27:08,323] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - running: Set()
 INFO [2020-01-19 00:27:08,324] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - waiting: Set(ResultStage 13)
 INFO [2020-01-19 00:27:08,324] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - failed: Set()
 INFO [2020-01-19 00:27:08,325] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 13 (MapPartitionsRDD[51] at collect at <ipython-input-32-4294447afb77>:4), which has no missing parents
 INFO [2020-01-19 00:27:08,333] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_13 stored as values in memory (estimated size 8.1 KB, free 366.2 MB)
 INFO [2020-01-19 00:27:08,335] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_13_piece0 stored as bytes in memory (estimated size 4.3 KB, free 366.2 MB)
 INFO [2020-01-19 00:27:08,335] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_13_piece0 in memory on zeppelin:33185 (size: 4.3 KB, free: 366.3 MB)
 INFO [2020-01-19 00:27:08,336] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 13 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:27:08,336] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[51] at collect at <ipython-input-32-4294447afb77>:4) (first 15 tasks are for partitions Vector(0))
 INFO [2020-01-19 00:27:08,336] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 13.0 with 1 tasks
 INFO [2020-01-19 00:27:08,339] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 13.0 (TID 40, 172.18.0.8, executor 0, partition 0, NODE_LOCAL, 7771 bytes)
 INFO [2020-01-19 00:27:08,370] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_13_piece0 in memory on 172.18.0.8:44045 (size: 4.3 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:08,374] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Asked to send map output locations for shuffle 2 to 172.18.0.8:59242
 INFO [2020-01-19 00:27:08,396] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 13.0 (TID 40) in 57 ms on 172.18.0.8 (executor 0) (1/1)
 INFO [2020-01-19 00:27:08,396] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 13.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:27:08,397] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 13 (collect at <ipython-input-32-4294447afb77>:4) finished in 0.071 s
 INFO [2020-01-19 00:27:08,397] ({Thread-20} Logging.scala[logInfo]:54) - Job 10 finished: collect at <ipython-input-32-4294447afb77>:4, took 0.301503 s
 INFO [2020-01-19 00:27:08,442] ({pool-2-thread-10} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:27:11,266] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200119-001021_531760765 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:27:11,359] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: runJob at PythonRDD.scala:153
 INFO [2020-01-19 00:27:11,363] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 11 (runJob at PythonRDD.scala:153) with 1 output partitions
 INFO [2020-01-19 00:27:11,364] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 14 (runJob at PythonRDD.scala:153)
 INFO [2020-01-19 00:27:11,364] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-01-19 00:27:11,365] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-01-19 00:27:11,365] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 14 (PythonRDD[55] at RDD at PythonRDD.scala:53), which has no missing parents
 INFO [2020-01-19 00:27:11,369] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_14 stored as values in memory (estimated size 10.2 KB, free 366.2 MB)
 INFO [2020-01-19 00:27:11,371] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_14_piece0 stored as bytes in memory (estimated size 5.8 KB, free 366.2 MB)
 INFO [2020-01-19 00:27:11,371] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_14_piece0 in memory on zeppelin:33185 (size: 5.8 KB, free: 366.3 MB)
 INFO [2020-01-19 00:27:11,372] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 14 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:27:11,373] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 14 (PythonRDD[55] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))
 INFO [2020-01-19 00:27:11,373] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 14.0 with 1 tasks
 INFO [2020-01-19 00:27:11,374] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 14.0 (TID 41, 172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:27:11,402] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_14_piece0 in memory on 172.18.0.8:44045 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:11,478] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 14.0 (TID 41) in 104 ms on 172.18.0.8 (executor 0) (1/1)
 INFO [2020-01-19 00:27:11,479] ({task-result-getter-2} Logging.scala[logInfo]:54) - Removed TaskSet 14.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:27:11,480] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 14 (runJob at PythonRDD.scala:153) finished in 0.113 s
 INFO [2020-01-19 00:27:11,482] ({Thread-20} Logging.scala[logInfo]:54) - Job 11 finished: runJob at PythonRDD.scala:153, took 0.119819 s
 INFO [2020-01-19 00:27:11,534] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: runJob at PythonRDD.scala:153
 INFO [2020-01-19 00:27:11,540] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 12 (runJob at PythonRDD.scala:153) with 4 output partitions
 INFO [2020-01-19 00:27:11,540] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 15 (runJob at PythonRDD.scala:153)
 INFO [2020-01-19 00:27:11,541] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-01-19 00:27:11,542] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-01-19 00:27:11,542] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 15 (PythonRDD[56] at RDD at PythonRDD.scala:53), which has no missing parents
 INFO [2020-01-19 00:27:11,544] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_15 stored as values in memory (estimated size 10.2 KB, free 366.2 MB)
 INFO [2020-01-19 00:27:11,547] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_15_piece0 stored as bytes in memory (estimated size 5.8 KB, free 366.2 MB)
 INFO [2020-01-19 00:27:11,553] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_15_piece0 in memory on zeppelin:33185 (size: 5.8 KB, free: 366.3 MB)
 INFO [2020-01-19 00:27:11,553] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 15 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:27:11,557] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 4 missing tasks from ResultStage 15 (PythonRDD[56] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(1, 2, 3, 4))
 INFO [2020-01-19 00:27:11,558] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 15.0 with 4 tasks
 INFO [2020-01-19 00:27:11,570] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 15.0 (TID 42, 172.18.0.8, executor 0, partition 1, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:27:11,570] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 1.0 in stage 15.0 (TID 43, 172.18.0.6, executor 2, partition 2, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:27:11,571] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 2.0 in stage 15.0 (TID 44, 172.18.0.7, executor 1, partition 3, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:27:11,571] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 3.0 in stage 15.0 (TID 45, 172.18.0.8, executor 0, partition 4, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:27:11,601] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_15_piece0 in memory on 172.18.0.6:42495 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:11,612] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_15_piece0 in memory on 172.18.0.7:37317 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:11,624] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_15_piece0 in memory on 172.18.0.8:44045 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:11,692] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 1.0 in stage 15.0 (TID 43) in 122 ms on 172.18.0.6 (executor 2) (1/4)
 INFO [2020-01-19 00:27:11,706] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 2.0 in stage 15.0 (TID 44) in 135 ms on 172.18.0.7 (executor 1) (2/4)
 INFO [2020-01-19 00:27:11,745] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 3.0 in stage 15.0 (TID 45) in 174 ms on 172.18.0.8 (executor 0) (3/4)
 INFO [2020-01-19 00:27:11,791] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 15.0 (TID 42) in 230 ms on 172.18.0.8 (executor 0) (4/4)
 INFO [2020-01-19 00:27:11,791] ({task-result-getter-2} Logging.scala[logInfo]:54) - Removed TaskSet 15.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:27:11,794] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 15 (runJob at PythonRDD.scala:153) finished in 0.251 s
 INFO [2020-01-19 00:27:11,795] ({Thread-20} Logging.scala[logInfo]:54) - Job 12 finished: runJob at PythonRDD.scala:153, took 0.255034 s
 INFO [2020-01-19 00:27:11,821] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: runJob at PythonRDD.scala:153
 INFO [2020-01-19 00:27:11,828] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 13 (runJob at PythonRDD.scala:153) with 1 output partitions
 INFO [2020-01-19 00:27:11,829] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 16 (runJob at PythonRDD.scala:153)
 INFO [2020-01-19 00:27:11,829] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-01-19 00:27:11,830] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-01-19 00:27:11,830] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 16 (PythonRDD[57] at RDD at PythonRDD.scala:53), which has no missing parents
 INFO [2020-01-19 00:27:11,840] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_16 stored as values in memory (estimated size 10.2 KB, free 366.2 MB)
 INFO [2020-01-19 00:27:11,841] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_16_piece0 stored as bytes in memory (estimated size 5.8 KB, free 366.2 MB)
 INFO [2020-01-19 00:27:11,842] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_16_piece0 in memory on zeppelin:33185 (size: 5.8 KB, free: 366.3 MB)
 INFO [2020-01-19 00:27:11,842] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 16 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:27:11,843] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 16 (PythonRDD[57] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(5))
 INFO [2020-01-19 00:27:11,843] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 16.0 with 1 tasks
 INFO [2020-01-19 00:27:11,844] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 16.0 (TID 46, 172.18.0.6, executor 2, partition 5, PROCESS_LOCAL, 7920 bytes)
 INFO [2020-01-19 00:27:11,871] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_16_piece0 in memory on 172.18.0.6:42495 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:11,940] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 16.0 (TID 46) in 96 ms on 172.18.0.6 (executor 2) (1/1)
 INFO [2020-01-19 00:27:11,941] ({task-result-getter-0} Logging.scala[logInfo]:54) - Removed TaskSet 16.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:27:11,942] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 16 (runJob at PythonRDD.scala:153) finished in 0.103 s
 INFO [2020-01-19 00:27:11,942] ({Thread-20} Logging.scala[logInfo]:54) - Job 13 finished: runJob at PythonRDD.scala:153, took 0.114648 s
 INFO [2020-01-19 00:27:12,113] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200119-001021_531760765 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:27:22,301] ({pool-2-thread-11} SchedulerFactory.java[jobStarted]:114) - Job 20200119-001021_531760765 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:27:22,551] ({pool-2-thread-11} SchedulerFactory.java[jobFinished]:120) - Job 20200119-001021_531760765 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:27:32,895] ({pool-2-thread-7} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:27:33,044] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: collect at <ipython-input-38-78e246484f6b>:4
 INFO [2020-01-19 00:27:33,045] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Registering RDD 60 (collect at <ipython-input-38-78e246484f6b>:4)
 INFO [2020-01-19 00:27:33,046] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 14 (collect at <ipython-input-38-78e246484f6b>:4) with 1 output partitions
 INFO [2020-01-19 00:27:33,046] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 18 (collect at <ipython-input-38-78e246484f6b>:4)
 INFO [2020-01-19 00:27:33,047] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List(ShuffleMapStage 17)
 INFO [2020-01-19 00:27:33,047] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List(ShuffleMapStage 17)
 INFO [2020-01-19 00:27:33,048] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ShuffleMapStage 17 (MapPartitionsRDD[60] at collect at <ipython-input-38-78e246484f6b>:4), which has no missing parents
 INFO [2020-01-19 00:27:33,053] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_17 stored as values in memory (estimated size 13.6 KB, free 366.2 MB)
 INFO [2020-01-19 00:27:33,055] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_17_piece0 stored as bytes in memory (estimated size 7.2 KB, free 366.2 MB)
 INFO [2020-01-19 00:27:33,056] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_17_piece0 in memory on zeppelin:33185 (size: 7.2 KB, free: 366.2 MB)
 INFO [2020-01-19 00:27:33,057] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 17 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:27:33,057] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 6 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[60] at collect at <ipython-input-38-78e246484f6b>:4) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
 INFO [2020-01-19 00:27:33,057] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 17.0 with 6 tasks
 INFO [2020-01-19 00:27:33,059] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 17.0 (TID 47, 172.18.0.7, executor 1, partition 0, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:27:33,059] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 1.0 in stage 17.0 (TID 48, 172.18.0.6, executor 2, partition 1, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:27:33,059] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 2.0 in stage 17.0 (TID 49, 172.18.0.8, executor 0, partition 2, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:27:33,059] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 3.0 in stage 17.0 (TID 50, 172.18.0.7, executor 1, partition 3, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:27:33,060] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 4.0 in stage 17.0 (TID 51, 172.18.0.6, executor 2, partition 4, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:27:33,060] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 5.0 in stage 17.0 (TID 52, 172.18.0.8, executor 0, partition 5, PROCESS_LOCAL, 7909 bytes)
 INFO [2020-01-19 00:27:33,111] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_17_piece0 in memory on 172.18.0.7:37317 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:33,119] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_17_piece0 in memory on 172.18.0.6:42495 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:33,128] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_17_piece0 in memory on 172.18.0.8:44045 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:33,198] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 5.0 in stage 17.0 (TID 52) in 138 ms on 172.18.0.8 (executor 0) (1/6)
 INFO [2020-01-19 00:27:33,208] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 17.0 (TID 47) in 150 ms on 172.18.0.7 (executor 1) (2/6)
 INFO [2020-01-19 00:27:33,217] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 3.0 in stage 17.0 (TID 50) in 158 ms on 172.18.0.7 (executor 1) (3/6)
 INFO [2020-01-19 00:27:33,232] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 4.0 in stage 17.0 (TID 51) in 172 ms on 172.18.0.6 (executor 2) (4/6)
 INFO [2020-01-19 00:27:33,233] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 2.0 in stage 17.0 (TID 49) in 174 ms on 172.18.0.8 (executor 0) (5/6)
 INFO [2020-01-19 00:27:33,234] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 1.0 in stage 17.0 (TID 48) in 175 ms on 172.18.0.6 (executor 2) (6/6)
 INFO [2020-01-19 00:27:33,239] ({task-result-getter-1} Logging.scala[logInfo]:54) - Removed TaskSet 17.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:27:33,248] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ShuffleMapStage 17 (collect at <ipython-input-38-78e246484f6b>:4) finished in 0.197 s
 INFO [2020-01-19 00:27:33,256] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - looking for newly runnable stages
 INFO [2020-01-19 00:27:33,257] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - running: Set()
 INFO [2020-01-19 00:27:33,257] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - waiting: Set(ResultStage 18)
 INFO [2020-01-19 00:27:33,258] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - failed: Set()
 INFO [2020-01-19 00:27:33,258] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 18 (MapPartitionsRDD[63] at collect at <ipython-input-38-78e246484f6b>:4), which has no missing parents
 INFO [2020-01-19 00:27:33,259] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_18 stored as values in memory (estimated size 8.1 KB, free 366.1 MB)
 INFO [2020-01-19 00:27:33,261] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_18_piece0 stored as bytes in memory (estimated size 4.3 KB, free 366.1 MB)
 INFO [2020-01-19 00:27:33,263] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_18_piece0 in memory on zeppelin:33185 (size: 4.3 KB, free: 366.2 MB)
 INFO [2020-01-19 00:27:33,265] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 18 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:27:33,265] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[63] at collect at <ipython-input-38-78e246484f6b>:4) (first 15 tasks are for partitions Vector(0))
 INFO [2020-01-19 00:27:33,266] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 18.0 with 1 tasks
 INFO [2020-01-19 00:27:33,266] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 18.0 (TID 53, 172.18.0.8, executor 0, partition 0, NODE_LOCAL, 7771 bytes)
 INFO [2020-01-19 00:27:33,277] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_18_piece0 in memory on 172.18.0.8:44045 (size: 4.3 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:33,289] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Asked to send map output locations for shuffle 3 to 172.18.0.8:59242
 INFO [2020-01-19 00:27:33,314] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 18.0 (TID 53) in 48 ms on 172.18.0.8 (executor 0) (1/1)
 INFO [2020-01-19 00:27:33,314] ({task-result-getter-2} Logging.scala[logInfo]:54) - Removed TaskSet 18.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:27:33,315] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 18 (collect at <ipython-input-38-78e246484f6b>:4) finished in 0.056 s
 INFO [2020-01-19 00:27:33,318] ({Thread-20} Logging.scala[logInfo]:54) - Job 14 finished: collect at <ipython-input-38-78e246484f6b>:4, took 0.272760 s
 INFO [2020-01-19 00:27:33,346] ({pool-2-thread-7} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:27:36,884] ({pool-2-thread-12} SchedulerFactory.java[jobStarted]:114) - Job 20200119-001021_531760765 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:27:36,937] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: runJob at PythonRDD.scala:153
 INFO [2020-01-19 00:27:36,938] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 15 (runJob at PythonRDD.scala:153) with 1 output partitions
 INFO [2020-01-19 00:27:36,938] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 19 (runJob at PythonRDD.scala:153)
 INFO [2020-01-19 00:27:36,939] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-01-19 00:27:36,939] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-01-19 00:27:36,940] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 19 (PythonRDD[64] at RDD at PythonRDD.scala:53), which has no missing parents
 INFO [2020-01-19 00:27:36,945] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_19 stored as values in memory (estimated size 10.2 KB, free 366.1 MB)
 INFO [2020-01-19 00:27:36,946] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_19_piece0 stored as bytes in memory (estimated size 5.8 KB, free 366.1 MB)
 INFO [2020-01-19 00:27:36,952] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_19_piece0 in memory on zeppelin:33185 (size: 5.8 KB, free: 366.2 MB)
 INFO [2020-01-19 00:27:36,953] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 19 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:27:36,955] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 19 (PythonRDD[64] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))
 INFO [2020-01-19 00:27:36,955] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 19.0 with 1 tasks
 INFO [2020-01-19 00:27:36,956] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 19.0 (TID 54, 172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:27:36,969] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_19_piece0 in memory on 172.18.0.8:44045 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:37,038] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 19.0 (TID 54) in 82 ms on 172.18.0.8 (executor 0) (1/1)
 INFO [2020-01-19 00:27:37,039] ({task-result-getter-0} Logging.scala[logInfo]:54) - Removed TaskSet 19.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:27:37,039] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 19 (runJob at PythonRDD.scala:153) finished in 0.095 s
 INFO [2020-01-19 00:27:37,040] ({Thread-20} Logging.scala[logInfo]:54) - Job 15 finished: runJob at PythonRDD.scala:153, took 0.102615 s
 INFO [2020-01-19 00:27:37,068] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: runJob at PythonRDD.scala:153
 INFO [2020-01-19 00:27:37,069] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 16 (runJob at PythonRDD.scala:153) with 4 output partitions
 INFO [2020-01-19 00:27:37,069] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 20 (runJob at PythonRDD.scala:153)
 INFO [2020-01-19 00:27:37,069] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-01-19 00:27:37,069] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-01-19 00:27:37,069] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 20 (PythonRDD[65] at RDD at PythonRDD.scala:53), which has no missing parents
 INFO [2020-01-19 00:27:37,072] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_20 stored as values in memory (estimated size 10.2 KB, free 366.1 MB)
 INFO [2020-01-19 00:27:37,089] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 277
 INFO [2020-01-19 00:27:37,091] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 302
 INFO [2020-01-19 00:27:37,092] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_20_piece0 stored as bytes in memory (estimated size 5.8 KB, free 366.1 MB)
 INFO [2020-01-19 00:27:37,100] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_20_piece0 in memory on zeppelin:33185 (size: 5.8 KB, free: 366.2 MB)
 INFO [2020-01-19 00:27:37,100] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_13_piece0 on 172.18.0.8:44045 in memory (size: 4.3 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:37,102] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 20 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:27:37,104] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 4 missing tasks from ResultStage 20 (PythonRDD[65] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(1, 2, 3, 4))
 INFO [2020-01-19 00:27:37,104] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 20.0 with 4 tasks
 INFO [2020-01-19 00:27:37,105] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 20.0 (TID 55, 172.18.0.8, executor 0, partition 1, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:27:37,105] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 1.0 in stage 20.0 (TID 56, 172.18.0.6, executor 2, partition 2, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:27:37,106] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 2.0 in stage 20.0 (TID 57, 172.18.0.7, executor 1, partition 3, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:27:37,106] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 3.0 in stage 20.0 (TID 58, 172.18.0.8, executor 0, partition 4, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:27:37,120] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_13_piece0 on zeppelin:33185 in memory (size: 4.3 KB, free: 366.2 MB)
 INFO [2020-01-19 00:27:37,128] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_20_piece0 in memory on 172.18.0.7:37317 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:37,158] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_20_piece0 in memory on 172.18.0.6:42495 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:37,173] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_20_piece0 in memory on 172.18.0.8:44045 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:37,216] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 369
 INFO [2020-01-19 00:27:37,217] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 332
 INFO [2020-01-19 00:27:37,217] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 492
 INFO [2020-01-19 00:27:37,217] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 525
 INFO [2020-01-19 00:27:37,217] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 428
 INFO [2020-01-19 00:27:37,218] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 280
 INFO [2020-01-19 00:27:37,218] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 448
 INFO [2020-01-19 00:27:37,219] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 319
 INFO [2020-01-19 00:27:37,219] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 381
 INFO [2020-01-19 00:27:37,219] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 455
 INFO [2020-01-19 00:27:37,220] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 526
 INFO [2020-01-19 00:27:37,220] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 271
 INFO [2020-01-19 00:27:37,220] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 294
 INFO [2020-01-19 00:27:37,221] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 392
 INFO [2020-01-19 00:27:37,221] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 313
 INFO [2020-01-19 00:27:37,222] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 427
 INFO [2020-01-19 00:27:37,222] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 289
 INFO [2020-01-19 00:27:37,222] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 317
 INFO [2020-01-19 00:27:37,223] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 523
 INFO [2020-01-19 00:27:37,223] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 320
 INFO [2020-01-19 00:27:37,223] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 430
 INFO [2020-01-19 00:27:37,224] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 552
 INFO [2020-01-19 00:27:37,224] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 454
 INFO [2020-01-19 00:27:37,224] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 527
 INFO [2020-01-19 00:27:37,225] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 307
 INFO [2020-01-19 00:27:37,230] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 273
 INFO [2020-01-19 00:27:37,231] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 464
 INFO [2020-01-19 00:27:37,231] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 556
 INFO [2020-01-19 00:27:37,231] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 359
 INFO [2020-01-19 00:27:37,232] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 363
 INFO [2020-01-19 00:27:37,232] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 554
 INFO [2020-01-19 00:27:37,233] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 362
 INFO [2020-01-19 00:27:37,235] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 321
 INFO [2020-01-19 00:27:37,235] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 384
 INFO [2020-01-19 00:27:37,235] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 403
 INFO [2020-01-19 00:27:37,236] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 390
 INFO [2020-01-19 00:27:37,236] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 330
 INFO [2020-01-19 00:27:37,238] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 393
 INFO [2020-01-19 00:27:37,237] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 2.0 in stage 20.0 (TID 57) in 131 ms on 172.18.0.7 (executor 1) (1/4)
 INFO [2020-01-19 00:27:37,261] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 20.0 (TID 55) in 156 ms on 172.18.0.8 (executor 0) (2/4)
 INFO [2020-01-19 00:27:37,267] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_15_piece0 on zeppelin:33185 in memory (size: 5.8 KB, free: 366.2 MB)
 INFO [2020-01-19 00:27:37,276] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_15_piece0 on 172.18.0.8:44045 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:37,283] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_15_piece0 on 172.18.0.7:37317 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:37,283] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 1.0 in stage 20.0 (TID 56) in 178 ms on 172.18.0.6 (executor 2) (3/4)
 INFO [2020-01-19 00:27:37,287] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_15_piece0 on 172.18.0.6:42495 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:37,294] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 3.0 in stage 20.0 (TID 58) in 188 ms on 172.18.0.8 (executor 0) (4/4)
 INFO [2020-01-19 00:27:37,295] ({task-result-getter-0} Logging.scala[logInfo]:54) - Removed TaskSet 20.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:27:37,295] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 20 (runJob at PythonRDD.scala:153) finished in 0.224 s
 INFO [2020-01-19 00:27:37,296] ({Thread-20} Logging.scala[logInfo]:54) - Job 16 finished: runJob at PythonRDD.scala:153, took 0.227163 s
 INFO [2020-01-19 00:27:37,317] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 328
 INFO [2020-01-19 00:27:37,318] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 394
 INFO [2020-01-19 00:27:37,318] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 414
 INFO [2020-01-19 00:27:37,319] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 470
 INFO [2020-01-19 00:27:37,319] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 424
 INFO [2020-01-19 00:27:37,320] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 433
 INFO [2020-01-19 00:27:37,320] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 375
 INFO [2020-01-19 00:27:37,320] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 557
 INFO [2020-01-19 00:27:37,320] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 270
 INFO [2020-01-19 00:27:37,332] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: runJob at PythonRDD.scala:153
 INFO [2020-01-19 00:27:37,333] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 17 (runJob at PythonRDD.scala:153) with 1 output partitions
 INFO [2020-01-19 00:27:37,333] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 21 (runJob at PythonRDD.scala:153)
 INFO [2020-01-19 00:27:37,334] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-01-19 00:27:37,334] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-01-19 00:27:37,334] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 21 (PythonRDD[66] at RDD at PythonRDD.scala:53), which has no missing parents
 INFO [2020-01-19 00:27:37,336] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_21 stored as values in memory (estimated size 10.2 KB, free 366.1 MB)
 INFO [2020-01-19 00:27:37,337] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_21_piece0 stored as bytes in memory (estimated size 5.8 KB, free 366.1 MB)
 INFO [2020-01-19 00:27:37,339] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_21_piece0 in memory on zeppelin:33185 (size: 5.8 KB, free: 366.2 MB)
 INFO [2020-01-19 00:27:37,339] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_11_piece0 on 172.18.0.6:42495 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:37,342] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 21 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:27:37,343] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 21 (PythonRDD[66] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(5))
 INFO [2020-01-19 00:27:37,343] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 21.0 with 1 tasks
 INFO [2020-01-19 00:27:37,344] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 21.0 (TID 59, 172.18.0.8, executor 0, partition 5, PROCESS_LOCAL, 7920 bytes)
 INFO [2020-01-19 00:27:37,362] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_11_piece0 on zeppelin:33185 in memory (size: 5.8 KB, free: 366.2 MB)
 INFO [2020-01-19 00:27:37,378] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_21_piece0 in memory on 172.18.0.8:44045 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:37,427] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 399
 INFO [2020-01-19 00:27:37,428] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 524
 INFO [2020-01-19 00:27:37,428] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 423
 INFO [2020-01-19 00:27:37,429] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 548
 INFO [2020-01-19 00:27:37,429] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 346
 INFO [2020-01-19 00:27:37,429] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 474
 INFO [2020-01-19 00:27:37,430] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 258
 INFO [2020-01-19 00:27:37,430] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 342
 INFO [2020-01-19 00:27:37,431] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 339
 INFO [2020-01-19 00:27:37,431] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 316
 INFO [2020-01-19 00:27:37,431] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 357
 INFO [2020-01-19 00:27:37,431] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 503
 INFO [2020-01-19 00:27:37,438] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_17_piece0 on 172.18.0.7:37317 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:37,449] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_17_piece0 on zeppelin:33185 in memory (size: 7.2 KB, free: 366.2 MB)
 INFO [2020-01-19 00:27:37,467] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_17_piece0 on 172.18.0.6:42495 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:37,468] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_17_piece0 on 172.18.0.8:44045 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:37,471] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 21.0 (TID 59) in 127 ms on 172.18.0.8 (executor 0) (1/1)
 INFO [2020-01-19 00:27:37,472] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 21.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:27:37,477] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 21 (runJob at PythonRDD.scala:153) finished in 0.141 s
 INFO [2020-01-19 00:27:37,477] ({Thread-20} Logging.scala[logInfo]:54) - Job 17 finished: runJob at PythonRDD.scala:153, took 0.144459 s
 INFO [2020-01-19 00:27:37,573] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 276
 INFO [2020-01-19 00:27:37,573] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 376
 INFO [2020-01-19 00:27:37,574] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 426
 INFO [2020-01-19 00:27:37,574] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 360
 INFO [2020-01-19 00:27:37,574] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 291
 INFO [2020-01-19 00:27:37,575] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 306
 INFO [2020-01-19 00:27:37,580] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned shuffle 2
 INFO [2020-01-19 00:27:37,580] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 452
 INFO [2020-01-19 00:27:37,581] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 405
 INFO [2020-01-19 00:27:37,581] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 389
 INFO [2020-01-19 00:27:37,581] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 491
 INFO [2020-01-19 00:27:37,581] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 515
 INFO [2020-01-19 00:27:37,582] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 382
 INFO [2020-01-19 00:27:37,582] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 517
 INFO [2020-01-19 00:27:37,583] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 468
 INFO [2020-01-19 00:27:37,583] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 563
 INFO [2020-01-19 00:27:37,586] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 537
 INFO [2020-01-19 00:27:37,587] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 282
 INFO [2020-01-19 00:27:37,587] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 353
 INFO [2020-01-19 00:27:37,587] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 443
 INFO [2020-01-19 00:27:37,587] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 489
 INFO [2020-01-19 00:27:37,588] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 284
 INFO [2020-01-19 00:27:37,588] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 497
 INFO [2020-01-19 00:27:37,588] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 300
 INFO [2020-01-19 00:27:37,589] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 460
 INFO [2020-01-19 00:27:37,589] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 266
 INFO [2020-01-19 00:27:37,589] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 522
 INFO [2020-01-19 00:27:37,589] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 370
 INFO [2020-01-19 00:27:37,590] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 520
 INFO [2020-01-19 00:27:37,590] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 507
 INFO [2020-01-19 00:27:37,590] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 518
 INFO [2020-01-19 00:27:37,591] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 349
 INFO [2020-01-19 00:27:37,591] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 415
 INFO [2020-01-19 00:27:37,591] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 285
 INFO [2020-01-19 00:27:37,592] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 329
 INFO [2020-01-19 00:27:37,592] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 275
 INFO [2020-01-19 00:27:37,592] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 411
 INFO [2020-01-19 00:27:37,593] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 449
 INFO [2020-01-19 00:27:37,593] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 406
 INFO [2020-01-19 00:27:37,593] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 458
 INFO [2020-01-19 00:27:37,594] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 533
 INFO [2020-01-19 00:27:37,597] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 446
 INFO [2020-01-19 00:27:37,599] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 404
 INFO [2020-01-19 00:27:37,599] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 461
 INFO [2020-01-19 00:27:37,617] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_10_piece0 on 172.18.0.7:37317 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:37,618] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_10_piece0 on 172.18.0.8:44045 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:37,623] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_10_piece0 on zeppelin:33185 in memory (size: 5.8 KB, free: 366.3 MB)
 INFO [2020-01-19 00:27:37,639] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_10_piece0 on 172.18.0.6:42495 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:37,696] ({pool-2-thread-12} SchedulerFactory.java[jobFinished]:120) - Job 20200119-001021_531760765 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:27:37,727] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 386
 INFO [2020-01-19 00:27:37,728] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 296
 INFO [2020-01-19 00:27:37,728] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 536
 INFO [2020-01-19 00:27:37,729] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 371
 INFO [2020-01-19 00:27:37,729] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 516
 INFO [2020-01-19 00:27:37,729] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 260
 INFO [2020-01-19 00:27:37,729] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 541
 INFO [2020-01-19 00:27:37,729] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 268
 INFO [2020-01-19 00:27:37,730] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 318
 INFO [2020-01-19 00:27:37,732] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 510
 INFO [2020-01-19 00:27:37,733] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 310
 INFO [2020-01-19 00:27:37,733] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 314
 INFO [2020-01-19 00:27:37,733] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 347
 INFO [2020-01-19 00:27:37,733] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 290
 INFO [2020-01-19 00:27:37,734] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 500
 INFO [2020-01-19 00:27:37,734] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 546
 INFO [2020-01-19 00:27:37,734] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 462
 INFO [2020-01-19 00:27:37,735] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 298
 INFO [2020-01-19 00:27:37,746] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_14_piece0 on zeppelin:33185 in memory (size: 5.8 KB, free: 366.3 MB)
 INFO [2020-01-19 00:27:37,781] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_14_piece0 on 172.18.0.8:44045 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:37,789] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 287
 INFO [2020-01-19 00:27:37,790] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 401
 INFO [2020-01-19 00:27:37,790] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 293
 INFO [2020-01-19 00:27:37,790] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 421
 INFO [2020-01-19 00:27:37,790] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 496
 INFO [2020-01-19 00:27:37,791] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 438
 INFO [2020-01-19 00:27:37,791] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 333
 INFO [2020-01-19 00:27:37,792] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 434
 INFO [2020-01-19 00:27:37,794] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 498
 INFO [2020-01-19 00:27:37,798] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 450
 INFO [2020-01-19 00:27:37,799] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 514
 INFO [2020-01-19 00:27:37,800] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 366
 INFO [2020-01-19 00:27:37,800] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 338
 INFO [2020-01-19 00:27:37,800] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 315
 INFO [2020-01-19 00:27:37,800] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 264
 INFO [2020-01-19 00:27:37,800] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 459
 INFO [2020-01-19 00:27:37,800] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 374
 INFO [2020-01-19 00:27:37,800] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 297
 INFO [2020-01-19 00:27:37,800] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 529
 INFO [2020-01-19 00:27:37,800] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 286
 INFO [2020-01-19 00:27:37,800] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 355
 INFO [2020-01-19 00:27:37,801] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 334
 INFO [2020-01-19 00:27:37,801] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 380
 INFO [2020-01-19 00:27:37,801] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 435
 INFO [2020-01-19 00:27:37,801] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 400
 INFO [2020-01-19 00:27:37,801] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 337
 INFO [2020-01-19 00:27:37,801] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 327
 INFO [2020-01-19 00:27:37,801] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 263
 INFO [2020-01-19 00:27:37,801] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 407
 INFO [2020-01-19 00:27:37,801] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 451
 INFO [2020-01-19 00:27:37,802] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 373
 INFO [2020-01-19 00:27:37,802] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 378
 INFO [2020-01-19 00:27:37,802] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 539
 INFO [2020-01-19 00:27:37,802] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 351
 INFO [2020-01-19 00:27:37,802] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 531
 INFO [2020-01-19 00:27:37,802] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 388
 INFO [2020-01-19 00:27:37,802] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 309
 INFO [2020-01-19 00:27:37,802] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 345
 INFO [2020-01-19 00:27:37,802] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 385
 INFO [2020-01-19 00:27:37,803] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 281
 INFO [2020-01-19 00:27:37,803] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 292
 INFO [2020-01-19 00:27:37,803] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 322
 INFO [2020-01-19 00:27:37,804] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 544
 INFO [2020-01-19 00:27:37,810] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 343
 INFO [2020-01-19 00:27:37,810] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 425
 INFO [2020-01-19 00:27:37,810] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 341
 INFO [2020-01-19 00:27:37,810] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 372
 INFO [2020-01-19 00:27:37,811] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 467
 INFO [2020-01-19 00:27:37,811] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 513
 INFO [2020-01-19 00:27:37,811] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 472
 INFO [2020-01-19 00:27:37,812] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 508
 INFO [2020-01-19 00:27:37,813] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 418
 INFO [2020-01-19 00:27:37,813] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 465
 INFO [2020-01-19 00:27:37,813] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 519
 INFO [2020-01-19 00:27:37,814] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 397
 INFO [2020-01-19 00:27:37,814] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 274
 INFO [2020-01-19 00:27:37,814] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 473
 INFO [2020-01-19 00:27:37,814] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 505
 INFO [2020-01-19 00:27:37,814] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 551
 INFO [2020-01-19 00:27:37,814] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 528
 INFO [2020-01-19 00:27:37,814] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 364
 INFO [2020-01-19 00:27:37,814] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 441
 INFO [2020-01-19 00:27:37,815] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 440
 INFO [2020-01-19 00:27:37,815] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 420
 INFO [2020-01-19 00:27:37,815] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 543
 INFO [2020-01-19 00:27:37,825] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_9_piece0 on zeppelin:33185 in memory (size: 5.8 KB, free: 366.3 MB)
 INFO [2020-01-19 00:27:37,831] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_9_piece0 on 172.18.0.7:37317 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:37,860] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 511
 INFO [2020-01-19 00:27:37,860] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 365
 INFO [2020-01-19 00:27:37,860] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 410
 INFO [2020-01-19 00:27:37,860] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 288
 INFO [2020-01-19 00:27:37,861] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 495
 INFO [2020-01-19 00:27:37,861] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 442
 INFO [2020-01-19 00:27:37,862] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 493
 INFO [2020-01-19 00:27:37,862] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 456
 INFO [2020-01-19 00:27:37,862] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 269
 INFO [2020-01-19 00:27:37,877] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_18_piece0 on zeppelin:33185 in memory (size: 4.3 KB, free: 366.3 MB)
 INFO [2020-01-19 00:27:37,884] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_18_piece0 on 172.18.0.8:44045 in memory (size: 4.3 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:37,898] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 550
 INFO [2020-01-19 00:27:37,900] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 323
 INFO [2020-01-19 00:27:37,908] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_12_piece0 on zeppelin:33185 in memory (size: 7.2 KB, free: 366.3 MB)
 INFO [2020-01-19 00:27:37,909] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_12_piece0 on 172.18.0.7:37317 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:37,916] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_12_piece0 on 172.18.0.8:44045 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:37,933] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_12_piece0 on 172.18.0.6:42495 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:37,942] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 331
 INFO [2020-01-19 00:27:37,943] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 542
 INFO [2020-01-19 00:27:37,943] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 457
 INFO [2020-01-19 00:27:37,943] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 558
 INFO [2020-01-19 00:27:37,943] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 368
 INFO [2020-01-19 00:27:37,943] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 509
 INFO [2020-01-19 00:27:37,943] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 463
 INFO [2020-01-19 00:27:37,943] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 561
 INFO [2020-01-19 00:27:37,944] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 453
 INFO [2020-01-19 00:27:37,944] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 506
 INFO [2020-01-19 00:27:37,944] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 553
 INFO [2020-01-19 00:27:37,944] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 278
 INFO [2020-01-19 00:27:37,944] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 377
 INFO [2020-01-19 00:27:37,944] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 416
 INFO [2020-01-19 00:27:37,944] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 336
 INFO [2020-01-19 00:27:37,944] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 354
 INFO [2020-01-19 00:27:37,945] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 502
 INFO [2020-01-19 00:27:37,945] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 395
 INFO [2020-01-19 00:27:37,945] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 504
 INFO [2020-01-19 00:27:37,945] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 324
 INFO [2020-01-19 00:27:37,945] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 501
 INFO [2020-01-19 00:27:37,945] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 490
 INFO [2020-01-19 00:27:37,945] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 367
 INFO [2020-01-19 00:27:37,946] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 494
 INFO [2020-01-19 00:27:37,946] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 305
 INFO [2020-01-19 00:27:37,946] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 304
 INFO [2020-01-19 00:27:37,946] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 538
 INFO [2020-01-19 00:27:37,946] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 555
 INFO [2020-01-19 00:27:37,946] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 299
 INFO [2020-01-19 00:27:37,946] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 439
 INFO [2020-01-19 00:27:37,946] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 279
 INFO [2020-01-19 00:27:37,947] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 444
 INFO [2020-01-19 00:27:37,947] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 261
 INFO [2020-01-19 00:27:37,947] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 259
 INFO [2020-01-19 00:27:37,947] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 521
 INFO [2020-01-19 00:27:37,947] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 335
 INFO [2020-01-19 00:27:37,947] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 272
 INFO [2020-01-19 00:27:37,947] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 344
 INFO [2020-01-19 00:27:37,947] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 471
 INFO [2020-01-19 00:27:37,948] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 419
 INFO [2020-01-19 00:27:37,948] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 267
 INFO [2020-01-19 00:27:37,948] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 356
 INFO [2020-01-19 00:27:37,948] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 445
 INFO [2020-01-19 00:27:37,948] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 469
 INFO [2020-01-19 00:27:37,948] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 325
 INFO [2020-01-19 00:27:37,954] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_16_piece0 on zeppelin:33185 in memory (size: 5.8 KB, free: 366.3 MB)
 INFO [2020-01-19 00:27:37,958] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_16_piece0 on 172.18.0.6:42495 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:37,961] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 466
 INFO [2020-01-19 00:27:37,961] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 499
 INFO [2020-01-19 00:27:37,966] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_19_piece0 on zeppelin:33185 in memory (size: 5.8 KB, free: 366.3 MB)
 INFO [2020-01-19 00:27:37,969] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_19_piece0 on 172.18.0.8:44045 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:37,977] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 358
 INFO [2020-01-19 00:27:37,978] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 301
 INFO [2020-01-19 00:27:37,978] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 262
 INFO [2020-01-19 00:27:37,978] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 540
 INFO [2020-01-19 00:27:37,978] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 429
 INFO [2020-01-19 00:27:37,978] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 549
 INFO [2020-01-19 00:27:37,978] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 535
 INFO [2020-01-19 00:27:37,979] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 312
 INFO [2020-01-19 00:27:37,979] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 350
 INFO [2020-01-19 00:27:37,979] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 265
 INFO [2020-01-19 00:27:37,979] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 283
 INFO [2020-01-19 00:27:37,979] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 387
 INFO [2020-01-19 00:27:37,979] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 437
 INFO [2020-01-19 00:27:37,979] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 534
 INFO [2020-01-19 00:27:37,979] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 560
 INFO [2020-01-19 00:27:37,979] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 447
 INFO [2020-01-19 00:27:37,979] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 396
 INFO [2020-01-19 00:27:37,979] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 391
 INFO [2020-01-19 00:27:37,980] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 559
 INFO [2020-01-19 00:27:37,980] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 361
 INFO [2020-01-19 00:27:37,980] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 408
 INFO [2020-01-19 00:27:37,980] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 348
 INFO [2020-01-19 00:27:37,980] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 308
 INFO [2020-01-19 00:27:37,980] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 379
 INFO [2020-01-19 00:27:37,980] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 409
 INFO [2020-01-19 00:27:37,980] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 562
 INFO [2020-01-19 00:27:37,980] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 295
 INFO [2020-01-19 00:27:37,980] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 326
 INFO [2020-01-19 00:27:37,981] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 545
 INFO [2020-01-19 00:27:37,981] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 422
 INFO [2020-01-19 00:27:37,981] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 512
 INFO [2020-01-19 00:27:37,981] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 303
 INFO [2020-01-19 00:27:37,981] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 530
 INFO [2020-01-19 00:27:37,981] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 383
 INFO [2020-01-19 00:27:37,981] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 311
 INFO [2020-01-19 00:27:37,981] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 340
 INFO [2020-01-19 00:27:37,981] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 417
 INFO [2020-01-19 00:27:37,981] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 352
 INFO [2020-01-19 00:27:37,982] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 412
 INFO [2020-01-19 00:27:37,982] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 547
 INFO [2020-01-19 00:27:37,982] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 402
 INFO [2020-01-19 00:27:37,982] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 436
 INFO [2020-01-19 00:27:37,982] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 431
 INFO [2020-01-19 00:27:37,982] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 532
 INFO [2020-01-19 00:27:37,982] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 413
 INFO [2020-01-19 00:27:37,982] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 432
 INFO [2020-01-19 00:27:47,626] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:27:47,763] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: collect at <ipython-input-42-3fb512590a39>:4
 INFO [2020-01-19 00:27:47,764] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Registering RDD 69 (collect at <ipython-input-42-3fb512590a39>:4)
 INFO [2020-01-19 00:27:47,764] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 18 (collect at <ipython-input-42-3fb512590a39>:4) with 1 output partitions
 INFO [2020-01-19 00:27:47,764] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 23 (collect at <ipython-input-42-3fb512590a39>:4)
 INFO [2020-01-19 00:27:47,764] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List(ShuffleMapStage 22)
 INFO [2020-01-19 00:27:47,764] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List(ShuffleMapStage 22)
 INFO [2020-01-19 00:27:47,765] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ShuffleMapStage 22 (MapPartitionsRDD[69] at collect at <ipython-input-42-3fb512590a39>:4), which has no missing parents
 INFO [2020-01-19 00:27:47,769] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_22 stored as values in memory (estimated size 13.6 KB, free 366.3 MB)
 INFO [2020-01-19 00:27:47,771] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_22_piece0 stored as bytes in memory (estimated size 7.2 KB, free 366.2 MB)
 INFO [2020-01-19 00:27:47,771] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_22_piece0 in memory on zeppelin:33185 (size: 7.2 KB, free: 366.3 MB)
 INFO [2020-01-19 00:27:47,772] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 22 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:27:47,772] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 6 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[69] at collect at <ipython-input-42-3fb512590a39>:4) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
 INFO [2020-01-19 00:27:47,772] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 22.0 with 6 tasks
 INFO [2020-01-19 00:27:47,773] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 22.0 (TID 60, 172.18.0.6, executor 2, partition 0, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:27:47,773] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 1.0 in stage 22.0 (TID 61, 172.18.0.8, executor 0, partition 1, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:27:47,774] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 2.0 in stage 22.0 (TID 62, 172.18.0.7, executor 1, partition 2, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:27:47,774] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 3.0 in stage 22.0 (TID 63, 172.18.0.6, executor 2, partition 3, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:27:47,774] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 4.0 in stage 22.0 (TID 64, 172.18.0.8, executor 0, partition 4, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:27:47,774] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 5.0 in stage 22.0 (TID 65, 172.18.0.7, executor 1, partition 5, PROCESS_LOCAL, 7909 bytes)
 INFO [2020-01-19 00:27:47,812] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_22_piece0 in memory on 172.18.0.6:42495 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:47,815] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_22_piece0 in memory on 172.18.0.8:44045 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:47,819] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_22_piece0 in memory on 172.18.0.7:37317 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:47,895] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 5.0 in stage 22.0 (TID 65) in 121 ms on 172.18.0.7 (executor 1) (1/6)
 INFO [2020-01-19 00:27:47,896] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 4.0 in stage 22.0 (TID 64) in 122 ms on 172.18.0.8 (executor 0) (2/6)
 INFO [2020-01-19 00:27:47,922] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 2.0 in stage 22.0 (TID 62) in 148 ms on 172.18.0.7 (executor 1) (3/6)
 INFO [2020-01-19 00:27:47,923] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 22.0 (TID 60) in 150 ms on 172.18.0.6 (executor 2) (4/6)
 INFO [2020-01-19 00:27:47,940] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 3.0 in stage 22.0 (TID 63) in 165 ms on 172.18.0.6 (executor 2) (5/6)
 INFO [2020-01-19 00:27:47,945] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 1.0 in stage 22.0 (TID 61) in 172 ms on 172.18.0.8 (executor 0) (6/6)
 INFO [2020-01-19 00:27:47,948] ({task-result-getter-2} Logging.scala[logInfo]:54) - Removed TaskSet 22.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:27:47,951] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ShuffleMapStage 22 (collect at <ipython-input-42-3fb512590a39>:4) finished in 0.184 s
 INFO [2020-01-19 00:27:47,953] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - looking for newly runnable stages
 INFO [2020-01-19 00:27:47,955] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - running: Set()
 INFO [2020-01-19 00:27:47,955] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - waiting: Set(ResultStage 23)
 INFO [2020-01-19 00:27:47,955] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - failed: Set()
 INFO [2020-01-19 00:27:47,956] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 23 (MapPartitionsRDD[72] at collect at <ipython-input-42-3fb512590a39>:4), which has no missing parents
 INFO [2020-01-19 00:27:47,957] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_23 stored as values in memory (estimated size 8.1 KB, free 366.2 MB)
 INFO [2020-01-19 00:27:47,959] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_23_piece0 stored as bytes in memory (estimated size 4.3 KB, free 366.2 MB)
 INFO [2020-01-19 00:27:47,960] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_23_piece0 in memory on zeppelin:33185 (size: 4.3 KB, free: 366.3 MB)
 INFO [2020-01-19 00:27:47,960] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 23 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:27:47,961] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[72] at collect at <ipython-input-42-3fb512590a39>:4) (first 15 tasks are for partitions Vector(0))
 INFO [2020-01-19 00:27:47,962] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 23.0 with 1 tasks
 INFO [2020-01-19 00:27:47,963] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 23.0 (TID 66, 172.18.0.8, executor 0, partition 0, NODE_LOCAL, 7771 bytes)
 INFO [2020-01-19 00:27:47,985] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_23_piece0 in memory on 172.18.0.8:44045 (size: 4.3 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:47,990] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Asked to send map output locations for shuffle 4 to 172.18.0.8:59242
 INFO [2020-01-19 00:27:48,010] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 23.0 (TID 66) in 47 ms on 172.18.0.8 (executor 0) (1/1)
 INFO [2020-01-19 00:27:48,010] ({task-result-getter-0} Logging.scala[logInfo]:54) - Removed TaskSet 23.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:27:48,011] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 23 (collect at <ipython-input-42-3fb512590a39>:4) finished in 0.055 s
 INFO [2020-01-19 00:27:48,011] ({Thread-20} Logging.scala[logInfo]:54) - Job 18 finished: collect at <ipython-input-42-3fb512590a39>:4, took 0.248207 s
 INFO [2020-01-19 00:27:48,049] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:27:52,169] ({pool-2-thread-13} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:27:52,307] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: collect at <ipython-input-44-78e246484f6b>:4
 INFO [2020-01-19 00:27:52,308] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Registering RDD 75 (collect at <ipython-input-44-78e246484f6b>:4)
 INFO [2020-01-19 00:27:52,308] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 19 (collect at <ipython-input-44-78e246484f6b>:4) with 1 output partitions
 INFO [2020-01-19 00:27:52,308] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 25 (collect at <ipython-input-44-78e246484f6b>:4)
 INFO [2020-01-19 00:27:52,309] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List(ShuffleMapStage 24)
 INFO [2020-01-19 00:27:52,309] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List(ShuffleMapStage 24)
 INFO [2020-01-19 00:27:52,310] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ShuffleMapStage 24 (MapPartitionsRDD[75] at collect at <ipython-input-44-78e246484f6b>:4), which has no missing parents
 INFO [2020-01-19 00:27:52,315] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_24 stored as values in memory (estimated size 13.6 KB, free 366.2 MB)
 INFO [2020-01-19 00:27:52,317] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_24_piece0 stored as bytes in memory (estimated size 7.2 KB, free 366.2 MB)
 INFO [2020-01-19 00:27:52,318] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_24_piece0 in memory on zeppelin:33185 (size: 7.2 KB, free: 366.3 MB)
 INFO [2020-01-19 00:27:52,318] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 24 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:27:52,319] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 6 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[75] at collect at <ipython-input-44-78e246484f6b>:4) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
 INFO [2020-01-19 00:27:52,324] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 24.0 with 6 tasks
 INFO [2020-01-19 00:27:52,325] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 24.0 (TID 67, 172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:27:52,326] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 1.0 in stage 24.0 (TID 68, 172.18.0.7, executor 1, partition 1, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:27:52,327] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 2.0 in stage 24.0 (TID 69, 172.18.0.6, executor 2, partition 2, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:27:52,327] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 3.0 in stage 24.0 (TID 70, 172.18.0.8, executor 0, partition 3, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:27:52,328] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 4.0 in stage 24.0 (TID 71, 172.18.0.7, executor 1, partition 4, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:27:52,328] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 5.0 in stage 24.0 (TID 72, 172.18.0.6, executor 2, partition 5, PROCESS_LOCAL, 7909 bytes)
 INFO [2020-01-19 00:27:52,363] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_24_piece0 in memory on 172.18.0.6:42495 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:52,371] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_24_piece0 in memory on 172.18.0.8:44045 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:52,397] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_24_piece0 in memory on 172.18.0.7:37317 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:52,459] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 3.0 in stage 24.0 (TID 70) in 132 ms on 172.18.0.8 (executor 0) (1/6)
 INFO [2020-01-19 00:27:52,470] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 2.0 in stage 24.0 (TID 69) in 143 ms on 172.18.0.6 (executor 2) (2/6)
 INFO [2020-01-19 00:27:52,492] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 24.0 (TID 67) in 167 ms on 172.18.0.8 (executor 0) (3/6)
 INFO [2020-01-19 00:27:52,498] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 4.0 in stage 24.0 (TID 71) in 170 ms on 172.18.0.7 (executor 1) (4/6)
 INFO [2020-01-19 00:27:52,508] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 5.0 in stage 24.0 (TID 72) in 180 ms on 172.18.0.6 (executor 2) (5/6)
 INFO [2020-01-19 00:27:52,517] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 1.0 in stage 24.0 (TID 68) in 191 ms on 172.18.0.7 (executor 1) (6/6)
 INFO [2020-01-19 00:27:52,518] ({task-result-getter-1} Logging.scala[logInfo]:54) - Removed TaskSet 24.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:27:52,520] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ShuffleMapStage 24 (collect at <ipython-input-44-78e246484f6b>:4) finished in 0.207 s
 INFO [2020-01-19 00:27:52,528] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - looking for newly runnable stages
 INFO [2020-01-19 00:27:52,529] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - running: Set()
 INFO [2020-01-19 00:27:52,533] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - waiting: Set(ResultStage 25)
 INFO [2020-01-19 00:27:52,533] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - failed: Set()
 INFO [2020-01-19 00:27:52,534] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 25 (MapPartitionsRDD[78] at collect at <ipython-input-44-78e246484f6b>:4), which has no missing parents
 INFO [2020-01-19 00:27:52,535] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_25 stored as values in memory (estimated size 8.1 KB, free 366.2 MB)
 INFO [2020-01-19 00:27:52,538] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_25_piece0 stored as bytes in memory (estimated size 4.3 KB, free 366.2 MB)
 INFO [2020-01-19 00:27:52,540] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_25_piece0 in memory on zeppelin:33185 (size: 4.3 KB, free: 366.3 MB)
 INFO [2020-01-19 00:27:52,540] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 25 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:27:52,541] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[78] at collect at <ipython-input-44-78e246484f6b>:4) (first 15 tasks are for partitions Vector(0))
 INFO [2020-01-19 00:27:52,543] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 25.0 with 1 tasks
 INFO [2020-01-19 00:27:52,546] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 25.0 (TID 73, 172.18.0.6, executor 2, partition 0, NODE_LOCAL, 7771 bytes)
 INFO [2020-01-19 00:27:52,577] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_25_piece0 in memory on 172.18.0.6:42495 (size: 4.3 KB, free: 912.3 MB)
 INFO [2020-01-19 00:27:52,617] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Asked to send map output locations for shuffle 5 to 172.18.0.6:54036
 INFO [2020-01-19 00:27:53,247] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 25.0 (TID 73) in 701 ms on 172.18.0.6 (executor 2) (1/1)
 INFO [2020-01-19 00:27:53,247] ({task-result-getter-2} Logging.scala[logInfo]:54) - Removed TaskSet 25.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:27:53,247] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 25 (collect at <ipython-input-44-78e246484f6b>:4) finished in 0.712 s
 INFO [2020-01-19 00:27:53,248] ({Thread-20} Logging.scala[logInfo]:54) - Job 19 finished: collect at <ipython-input-44-78e246484f6b>:4, took 0.940703 s
 INFO [2020-01-19 00:27:53,280] ({pool-2-thread-13} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:28:22,330] ({pool-2-thread-8} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:28:22,482] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: collect at <ipython-input-46-78e246484f6b>:4
 INFO [2020-01-19 00:28:22,484] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Registering RDD 81 (collect at <ipython-input-46-78e246484f6b>:4)
 INFO [2020-01-19 00:28:22,484] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 20 (collect at <ipython-input-46-78e246484f6b>:4) with 1 output partitions
 INFO [2020-01-19 00:28:22,485] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 27 (collect at <ipython-input-46-78e246484f6b>:4)
 INFO [2020-01-19 00:28:22,485] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List(ShuffleMapStage 26)
 INFO [2020-01-19 00:28:22,486] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List(ShuffleMapStage 26)
 INFO [2020-01-19 00:28:22,487] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ShuffleMapStage 26 (MapPartitionsRDD[81] at collect at <ipython-input-46-78e246484f6b>:4), which has no missing parents
 INFO [2020-01-19 00:28:22,491] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_26 stored as values in memory (estimated size 13.6 KB, free 366.2 MB)
 INFO [2020-01-19 00:28:22,493] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_26_piece0 stored as bytes in memory (estimated size 7.2 KB, free 366.2 MB)
 INFO [2020-01-19 00:28:22,494] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_26_piece0 in memory on zeppelin:33185 (size: 7.2 KB, free: 366.3 MB)
 INFO [2020-01-19 00:28:22,494] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 26 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:28:22,495] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 6 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[81] at collect at <ipython-input-46-78e246484f6b>:4) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
 INFO [2020-01-19 00:28:22,496] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 26.0 with 6 tasks
 INFO [2020-01-19 00:28:22,497] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 26.0 (TID 74, 172.18.0.6, executor 2, partition 0, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:28:22,497] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 1.0 in stage 26.0 (TID 75, 172.18.0.8, executor 0, partition 1, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:28:22,498] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 2.0 in stage 26.0 (TID 76, 172.18.0.7, executor 1, partition 2, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:28:22,498] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 3.0 in stage 26.0 (TID 77, 172.18.0.6, executor 2, partition 3, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:28:22,499] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 4.0 in stage 26.0 (TID 78, 172.18.0.8, executor 0, partition 4, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:28:22,499] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 5.0 in stage 26.0 (TID 79, 172.18.0.7, executor 1, partition 5, PROCESS_LOCAL, 7909 bytes)
 INFO [2020-01-19 00:28:22,542] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_26_piece0 in memory on 172.18.0.8:44045 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:22,553] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_26_piece0 in memory on 172.18.0.6:42495 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:22,556] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_26_piece0 in memory on 172.18.0.7:37317 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:22,610] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 4.0 in stage 26.0 (TID 78) in 111 ms on 172.18.0.8 (executor 0) (1/6)
 INFO [2020-01-19 00:28:22,636] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 1.0 in stage 26.0 (TID 75) in 139 ms on 172.18.0.8 (executor 0) (2/6)
 INFO [2020-01-19 00:28:22,672] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 3.0 in stage 26.0 (TID 77) in 174 ms on 172.18.0.6 (executor 2) (3/6)
 INFO [2020-01-19 00:28:22,673] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 26.0 (TID 74) in 176 ms on 172.18.0.6 (executor 2) (4/6)
 INFO [2020-01-19 00:28:22,692] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 2.0 in stage 26.0 (TID 76) in 194 ms on 172.18.0.7 (executor 1) (5/6)
 INFO [2020-01-19 00:28:22,693] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 5.0 in stage 26.0 (TID 79) in 194 ms on 172.18.0.7 (executor 1) (6/6)
 INFO [2020-01-19 00:28:22,693] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 26.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:28:22,704] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ShuffleMapStage 26 (collect at <ipython-input-46-78e246484f6b>:4) finished in 0.216 s
 INFO [2020-01-19 00:28:22,710] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - looking for newly runnable stages
 INFO [2020-01-19 00:28:22,711] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - running: Set()
 INFO [2020-01-19 00:28:22,711] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - waiting: Set(ResultStage 27)
 INFO [2020-01-19 00:28:22,711] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - failed: Set()
 INFO [2020-01-19 00:28:22,712] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 27 (MapPartitionsRDD[84] at collect at <ipython-input-46-78e246484f6b>:4), which has no missing parents
 INFO [2020-01-19 00:28:22,714] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_27 stored as values in memory (estimated size 8.1 KB, free 366.2 MB)
 INFO [2020-01-19 00:28:22,715] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_27_piece0 stored as bytes in memory (estimated size 4.3 KB, free 366.2 MB)
 INFO [2020-01-19 00:28:22,725] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_27_piece0 in memory on zeppelin:33185 (size: 4.3 KB, free: 366.3 MB)
 INFO [2020-01-19 00:28:22,728] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 27 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:28:22,729] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[84] at collect at <ipython-input-46-78e246484f6b>:4) (first 15 tasks are for partitions Vector(0))
 INFO [2020-01-19 00:28:22,729] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 27.0 with 1 tasks
 INFO [2020-01-19 00:28:22,730] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 27.0 (TID 80, 172.18.0.6, executor 2, partition 0, NODE_LOCAL, 7771 bytes)
 INFO [2020-01-19 00:28:22,759] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_27_piece0 in memory on 172.18.0.6:42495 (size: 4.3 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:22,773] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Asked to send map output locations for shuffle 6 to 172.18.0.6:54036
 INFO [2020-01-19 00:28:22,808] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 27.0 (TID 80) in 78 ms on 172.18.0.6 (executor 2) (1/1)
 INFO [2020-01-19 00:28:22,808] ({task-result-getter-1} Logging.scala[logInfo]:54) - Removed TaskSet 27.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:28:22,808] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 27 (collect at <ipython-input-46-78e246484f6b>:4) finished in 0.095 s
 INFO [2020-01-19 00:28:22,809] ({Thread-20} Logging.scala[logInfo]:54) - Job 20 finished: collect at <ipython-input-46-78e246484f6b>:4, took 0.325245 s
 INFO [2020-01-19 00:28:22,874] ({pool-2-thread-8} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:28:43,449] ({pool-2-thread-14} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:28:43,534] ({pool-2-thread-14} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:28:45,672] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20200118-195328_2060007234 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:28:45,761] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20200118-195328_2060007234 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:28:48,382] ({pool-2-thread-15} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:28:48,515] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: collect at <ipython-input-52-78e246484f6b>:4
 INFO [2020-01-19 00:28:48,521] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Registering RDD 97 (collect at <ipython-input-52-78e246484f6b>:4)
 INFO [2020-01-19 00:28:48,522] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 21 (collect at <ipython-input-52-78e246484f6b>:4) with 1 output partitions
 INFO [2020-01-19 00:28:48,522] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 29 (collect at <ipython-input-52-78e246484f6b>:4)
 INFO [2020-01-19 00:28:48,523] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List(ShuffleMapStage 28)
 INFO [2020-01-19 00:28:48,523] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List(ShuffleMapStage 28)
 INFO [2020-01-19 00:28:48,524] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ShuffleMapStage 28 (MapPartitionsRDD[97] at collect at <ipython-input-52-78e246484f6b>:4), which has no missing parents
 INFO [2020-01-19 00:28:48,528] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_28 stored as values in memory (estimated size 13.6 KB, free 366.2 MB)
 INFO [2020-01-19 00:28:48,529] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_28_piece0 stored as bytes in memory (estimated size 7.2 KB, free 366.2 MB)
 INFO [2020-01-19 00:28:48,530] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_28_piece0 in memory on zeppelin:33185 (size: 7.2 KB, free: 366.2 MB)
 INFO [2020-01-19 00:28:48,531] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 28 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:28:48,532] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 6 missing tasks from ShuffleMapStage 28 (MapPartitionsRDD[97] at collect at <ipython-input-52-78e246484f6b>:4) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
 INFO [2020-01-19 00:28:48,532] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 28.0 with 6 tasks
 INFO [2020-01-19 00:28:48,533] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 28.0 (TID 81, 172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:28:48,534] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 1.0 in stage 28.0 (TID 82, 172.18.0.7, executor 1, partition 1, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:28:48,534] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 2.0 in stage 28.0 (TID 83, 172.18.0.6, executor 2, partition 2, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:28:48,535] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 3.0 in stage 28.0 (TID 84, 172.18.0.8, executor 0, partition 3, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:28:48,535] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 4.0 in stage 28.0 (TID 85, 172.18.0.7, executor 1, partition 4, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:28:48,536] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 5.0 in stage 28.0 (TID 86, 172.18.0.6, executor 2, partition 5, PROCESS_LOCAL, 7909 bytes)
 INFO [2020-01-19 00:28:48,581] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_28_piece0 in memory on 172.18.0.8:44045 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:48,587] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_28_piece0 in memory on 172.18.0.6:42495 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:48,587] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_28_piece0 in memory on 172.18.0.7:37317 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:48,651] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 4.0 in stage 28.0 (TID 85) in 116 ms on 172.18.0.7 (executor 1) (1/6)
 INFO [2020-01-19 00:28:48,681] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 1.0 in stage 28.0 (TID 82) in 148 ms on 172.18.0.7 (executor 1) (2/6)
 INFO [2020-01-19 00:28:48,682] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 3.0 in stage 28.0 (TID 84) in 147 ms on 172.18.0.8 (executor 0) (3/6)
 INFO [2020-01-19 00:28:48,697] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 28.0 (TID 81) in 164 ms on 172.18.0.8 (executor 0) (4/6)
 INFO [2020-01-19 00:28:48,698] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 5.0 in stage 28.0 (TID 86) in 162 ms on 172.18.0.6 (executor 2) (5/6)
 INFO [2020-01-19 00:28:48,716] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 2.0 in stage 28.0 (TID 83) in 182 ms on 172.18.0.6 (executor 2) (6/6)
 INFO [2020-01-19 00:28:48,717] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 28.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:28:48,717] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ShuffleMapStage 28 (collect at <ipython-input-52-78e246484f6b>:4) finished in 0.192 s
 INFO [2020-01-19 00:28:48,718] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - looking for newly runnable stages
 INFO [2020-01-19 00:28:48,719] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - running: Set()
 INFO [2020-01-19 00:28:48,719] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - waiting: Set(ResultStage 29)
 INFO [2020-01-19 00:28:48,719] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - failed: Set()
 INFO [2020-01-19 00:28:48,720] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 29 (MapPartitionsRDD[100] at collect at <ipython-input-52-78e246484f6b>:4), which has no missing parents
 INFO [2020-01-19 00:28:48,721] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_29 stored as values in memory (estimated size 8.1 KB, free 366.1 MB)
 INFO [2020-01-19 00:28:48,723] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_29_piece0 stored as bytes in memory (estimated size 4.3 KB, free 366.1 MB)
 INFO [2020-01-19 00:28:48,723] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_29_piece0 in memory on zeppelin:33185 (size: 4.3 KB, free: 366.2 MB)
 INFO [2020-01-19 00:28:48,724] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 29 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:28:48,724] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[100] at collect at <ipython-input-52-78e246484f6b>:4) (first 15 tasks are for partitions Vector(0))
 INFO [2020-01-19 00:28:48,725] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 29.0 with 1 tasks
 INFO [2020-01-19 00:28:48,725] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 29.0 (TID 87, 172.18.0.7, executor 1, partition 0, NODE_LOCAL, 7771 bytes)
 INFO [2020-01-19 00:28:48,747] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_29_piece0 in memory on 172.18.0.7:37317 (size: 4.3 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:48,754] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Asked to send map output locations for shuffle 7 to 172.18.0.7:47870
 INFO [2020-01-19 00:28:48,769] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 29.0 (TID 87) in 44 ms on 172.18.0.7 (executor 1) (1/1)
 INFO [2020-01-19 00:28:48,769] ({task-result-getter-0} Logging.scala[logInfo]:54) - Removed TaskSet 29.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:28:48,770] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 29 (collect at <ipython-input-52-78e246484f6b>:4) finished in 0.049 s
 INFO [2020-01-19 00:28:48,775] ({Thread-20} Logging.scala[logInfo]:54) - Job 21 finished: collect at <ipython-input-52-78e246484f6b>:4, took 0.254468 s
 INFO [2020-01-19 00:28:48,847] ({pool-2-thread-15} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:28:51,396] ({pool-2-thread-6} SchedulerFactory.java[jobStarted]:114) - Job 20200119-001021_531760765 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:28:51,455] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: runJob at PythonRDD.scala:153
 INFO [2020-01-19 00:28:51,457] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 22 (runJob at PythonRDD.scala:153) with 1 output partitions
 INFO [2020-01-19 00:28:51,457] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 30 (runJob at PythonRDD.scala:153)
 INFO [2020-01-19 00:28:51,458] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-01-19 00:28:51,458] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-01-19 00:28:51,459] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 30 (PythonRDD[104] at RDD at PythonRDD.scala:53), which has no missing parents
 INFO [2020-01-19 00:28:51,461] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_30 stored as values in memory (estimated size 10.2 KB, free 366.1 MB)
 INFO [2020-01-19 00:28:51,463] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_30_piece0 stored as bytes in memory (estimated size 5.8 KB, free 366.1 MB)
 INFO [2020-01-19 00:28:51,463] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_30_piece0 in memory on zeppelin:33185 (size: 5.8 KB, free: 366.2 MB)
 INFO [2020-01-19 00:28:51,464] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 30 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:28:51,465] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 30 (PythonRDD[104] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))
 INFO [2020-01-19 00:28:51,466] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 30.0 with 1 tasks
 INFO [2020-01-19 00:28:51,467] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 30.0 (TID 88, 172.18.0.7, executor 1, partition 0, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:28:51,486] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_30_piece0 in memory on 172.18.0.7:37317 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:51,826] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 30.0 (TID 88) in 359 ms on 172.18.0.7 (executor 1) (1/1)
 INFO [2020-01-19 00:28:51,826] ({task-result-getter-1} Logging.scala[logInfo]:54) - Removed TaskSet 30.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:28:51,827] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 30 (runJob at PythonRDD.scala:153) finished in 0.367 s
 INFO [2020-01-19 00:28:51,828] ({Thread-20} Logging.scala[logInfo]:54) - Job 22 finished: runJob at PythonRDD.scala:153, took 0.372290 s
 INFO [2020-01-19 00:28:51,845] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: runJob at PythonRDD.scala:153
 INFO [2020-01-19 00:28:51,846] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 23 (runJob at PythonRDD.scala:153) with 4 output partitions
 INFO [2020-01-19 00:28:51,846] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 31 (runJob at PythonRDD.scala:153)
 INFO [2020-01-19 00:28:51,846] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-01-19 00:28:51,846] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-01-19 00:28:51,847] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 31 (PythonRDD[105] at RDD at PythonRDD.scala:53), which has no missing parents
 INFO [2020-01-19 00:28:51,850] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_31 stored as values in memory (estimated size 10.2 KB, free 366.1 MB)
 INFO [2020-01-19 00:28:51,853] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_31_piece0 stored as bytes in memory (estimated size 5.8 KB, free 366.1 MB)
 INFO [2020-01-19 00:28:51,853] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_31_piece0 in memory on zeppelin:33185 (size: 5.8 KB, free: 366.2 MB)
 INFO [2020-01-19 00:28:51,854] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 31 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:28:51,855] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 4 missing tasks from ResultStage 31 (PythonRDD[105] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(1, 2, 3, 4))
 INFO [2020-01-19 00:28:51,856] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 31.0 with 4 tasks
 INFO [2020-01-19 00:28:51,856] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 31.0 (TID 89, 172.18.0.8, executor 0, partition 1, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:28:51,857] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 1.0 in stage 31.0 (TID 90, 172.18.0.7, executor 1, partition 2, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:28:51,857] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 2.0 in stage 31.0 (TID 91, 172.18.0.6, executor 2, partition 3, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:28:51,858] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 3.0 in stage 31.0 (TID 92, 172.18.0.8, executor 0, partition 4, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:28:51,886] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_31_piece0 in memory on 172.18.0.6:42495 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:51,888] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_31_piece0 in memory on 172.18.0.8:44045 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:51,912] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_31_piece0 in memory on 172.18.0.7:37317 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:51,969] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 2.0 in stage 31.0 (TID 91) in 112 ms on 172.18.0.6 (executor 2) (1/4)
 INFO [2020-01-19 00:28:51,980] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 3.0 in stage 31.0 (TID 92) in 123 ms on 172.18.0.8 (executor 0) (2/4)
 INFO [2020-01-19 00:28:52,001] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 1.0 in stage 31.0 (TID 90) in 144 ms on 172.18.0.7 (executor 1) (3/4)
 INFO [2020-01-19 00:28:52,002] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 31.0 (TID 89) in 146 ms on 172.18.0.8 (executor 0) (4/4)
 INFO [2020-01-19 00:28:52,003] ({task-result-getter-1} Logging.scala[logInfo]:54) - Removed TaskSet 31.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:28:52,004] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 31 (runJob at PythonRDD.scala:153) finished in 0.155 s
 INFO [2020-01-19 00:28:52,004] ({Thread-20} Logging.scala[logInfo]:54) - Job 23 finished: runJob at PythonRDD.scala:153, took 0.158835 s
 INFO [2020-01-19 00:28:52,030] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: runJob at PythonRDD.scala:153
 INFO [2020-01-19 00:28:52,037] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 24 (runJob at PythonRDD.scala:153) with 1 output partitions
 INFO [2020-01-19 00:28:52,037] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 32 (runJob at PythonRDD.scala:153)
 INFO [2020-01-19 00:28:52,037] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-01-19 00:28:52,038] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-01-19 00:28:52,038] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 32 (PythonRDD[106] at RDD at PythonRDD.scala:53), which has no missing parents
 INFO [2020-01-19 00:28:52,042] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_32 stored as values in memory (estimated size 10.2 KB, free 366.1 MB)
 INFO [2020-01-19 00:28:52,060] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_32_piece0 stored as bytes in memory (estimated size 5.8 KB, free 366.1 MB)
 INFO [2020-01-19 00:28:52,061] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 606
 INFO [2020-01-19 00:28:52,062] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 771
 INFO [2020-01-19 00:28:52,063] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 629
 INFO [2020-01-19 00:28:52,063] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 900
 INFO [2020-01-19 00:28:52,063] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 832
 INFO [2020-01-19 00:28:52,063] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 913
 INFO [2020-01-19 00:28:52,062] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_32_piece0 in memory on zeppelin:33185 (size: 5.8 KB, free: 366.2 MB)
 INFO [2020-01-19 00:28:52,064] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 32 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:28:52,067] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 32 (PythonRDD[106] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(5))
 INFO [2020-01-19 00:28:52,070] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 32.0 with 1 tasks
 INFO [2020-01-19 00:28:52,071] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 32.0 (TID 93, 172.18.0.8, executor 0, partition 5, PROCESS_LOCAL, 7920 bytes)
 INFO [2020-01-19 00:28:52,069] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_25_piece0 on zeppelin:33185 in memory (size: 4.3 KB, free: 366.2 MB)
 INFO [2020-01-19 00:28:52,073] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_25_piece0 on 172.18.0.6:42495 in memory (size: 4.3 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:52,099] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_32_piece0 in memory on 172.18.0.8:44045 (size: 5.8 KB, free: 912.2 MB)
 INFO [2020-01-19 00:28:52,132] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 668
 INFO [2020-01-19 00:28:52,132] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 914
 INFO [2020-01-19 00:28:52,132] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 893
 INFO [2020-01-19 00:28:52,132] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 845
 INFO [2020-01-19 00:28:52,133] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 679
 INFO [2020-01-19 00:28:52,134] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 723
 INFO [2020-01-19 00:28:52,138] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 875
 INFO [2020-01-19 00:28:52,141] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 618
 INFO [2020-01-19 00:28:52,145] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 764
 INFO [2020-01-19 00:28:52,148] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 883
 INFO [2020-01-19 00:28:52,148] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 908
 INFO [2020-01-19 00:28:52,149] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 854
 INFO [2020-01-19 00:28:52,152] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 644
 INFO [2020-01-19 00:28:52,154] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 480
 INFO [2020-01-19 00:28:52,155] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 839
 INFO [2020-01-19 00:28:52,158] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 601
 INFO [2020-01-19 00:28:52,162] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 898
 INFO [2020-01-19 00:28:52,163] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 647
 INFO [2020-01-19 00:28:52,166] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 890
 INFO [2020-01-19 00:28:52,169] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 710
 INFO [2020-01-19 00:28:52,172] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 718
 INFO [2020-01-19 00:28:52,172] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 599
 INFO [2020-01-19 00:28:52,172] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 807
 INFO [2020-01-19 00:28:52,175] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 32.0 (TID 93) in 104 ms on 172.18.0.8 (executor 0) (1/1)
 INFO [2020-01-19 00:28:52,177] ({task-result-getter-2} Logging.scala[logInfo]:54) - Removed TaskSet 32.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:28:52,177] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 32 (runJob at PythonRDD.scala:153) finished in 0.136 s
 INFO [2020-01-19 00:28:52,177] ({Thread-20} Logging.scala[logInfo]:54) - Job 24 finished: runJob at PythonRDD.scala:153, took 0.140660 s
 INFO [2020-01-19 00:28:52,177] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 488
 INFO [2020-01-19 00:28:52,180] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 688
 INFO [2020-01-19 00:28:52,180] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 721
 INFO [2020-01-19 00:28:52,180] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 475
 INFO [2020-01-19 00:28:52,181] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 793
 INFO [2020-01-19 00:28:52,181] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 715
 INFO [2020-01-19 00:28:52,190] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 876
 INFO [2020-01-19 00:28:52,198] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 600
 INFO [2020-01-19 00:28:52,199] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 628
 INFO [2020-01-19 00:28:52,199] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 834
 INFO [2020-01-19 00:28:52,199] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 568
 INFO [2020-01-19 00:28:52,199] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 624
 INFO [2020-01-19 00:28:52,204] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 612
 INFO [2020-01-19 00:28:52,205] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 758
 INFO [2020-01-19 00:28:52,205] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 643
 INFO [2020-01-19 00:28:52,205] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 651
 INFO [2020-01-19 00:28:52,205] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 479
 INFO [2020-01-19 00:28:52,206] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 636
 INFO [2020-01-19 00:28:52,206] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 911
 INFO [2020-01-19 00:28:52,206] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 656
 INFO [2020-01-19 00:28:52,213] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 707
 INFO [2020-01-19 00:28:52,213] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 842
 INFO [2020-01-19 00:28:52,213] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 769
 INFO [2020-01-19 00:28:52,213] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 894
 INFO [2020-01-19 00:28:52,214] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 565
 INFO [2020-01-19 00:28:52,242] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_21_piece0 on 172.18.0.8:44045 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:52,255] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_21_piece0 on zeppelin:33185 in memory (size: 5.8 KB, free: 366.2 MB)
 INFO [2020-01-19 00:28:52,324] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 915
 INFO [2020-01-19 00:28:52,324] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 567
 INFO [2020-01-19 00:28:52,324] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 571
 INFO [2020-01-19 00:28:52,325] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 870
 INFO [2020-01-19 00:28:52,325] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 577
 INFO [2020-01-19 00:28:52,326] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 592
 INFO [2020-01-19 00:28:52,326] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 850
 INFO [2020-01-19 00:28:52,329] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 587
 INFO [2020-01-19 00:28:52,331] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 776
 INFO [2020-01-19 00:28:52,331] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 633
 INFO [2020-01-19 00:28:52,333] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 623
 INFO [2020-01-19 00:28:52,334] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 689
 INFO [2020-01-19 00:28:52,335] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 696
 INFO [2020-01-19 00:28:52,337] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 719
 INFO [2020-01-19 00:28:52,339] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 598
 INFO [2020-01-19 00:28:52,339] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 595
 INFO [2020-01-19 00:28:52,339] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 849
 INFO [2020-01-19 00:28:52,345] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 863
 INFO [2020-01-19 00:28:52,345] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 899
 INFO [2020-01-19 00:28:52,346] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 799
 INFO [2020-01-19 00:28:52,346] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 713
 INFO [2020-01-19 00:28:52,346] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 835
 INFO [2020-01-19 00:28:52,348] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 784
 INFO [2020-01-19 00:28:52,376] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_24_piece0 on 172.18.0.6:42495 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:52,381] ({pool-2-thread-6} SchedulerFactory.java[jobFinished]:120) - Job 20200119-001021_531760765 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:28:52,389] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_24_piece0 on 172.18.0.7:37317 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:52,394] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_24_piece0 on 172.18.0.8:44045 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:52,395] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_24_piece0 on zeppelin:33185 in memory (size: 7.2 KB, free: 366.2 MB)
 INFO [2020-01-19 00:28:52,412] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 848
 INFO [2020-01-19 00:28:52,412] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 648
 INFO [2020-01-19 00:28:52,412] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 750
 INFO [2020-01-19 00:28:52,412] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 640
 INFO [2020-01-19 00:28:52,412] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 790
 INFO [2020-01-19 00:28:52,413] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 703
 INFO [2020-01-19 00:28:52,413] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 609
 INFO [2020-01-19 00:28:52,414] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 607
 INFO [2020-01-19 00:28:52,414] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 693
 INFO [2020-01-19 00:28:52,418] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_20_piece0 on 172.18.0.6:42495 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:52,421] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_20_piece0 on 172.18.0.8:44045 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:52,422] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_20_piece0 on 172.18.0.7:37317 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:52,425] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_20_piece0 on zeppelin:33185 in memory (size: 5.8 KB, free: 366.2 MB)
 INFO [2020-01-19 00:28:52,435] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 912
 INFO [2020-01-19 00:28:52,435] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 747
 INFO [2020-01-19 00:28:52,437] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 614
 INFO [2020-01-19 00:28:52,437] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 868
 INFO [2020-01-19 00:28:52,438] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 830
 INFO [2020-01-19 00:28:52,438] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 591
 INFO [2020-01-19 00:28:52,438] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 743
 INFO [2020-01-19 00:28:52,438] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 695
 INFO [2020-01-19 00:28:52,438] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 697
 INFO [2020-01-19 00:28:52,438] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 804
 INFO [2020-01-19 00:28:52,438] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 681
 INFO [2020-01-19 00:28:52,438] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 923
 INFO [2020-01-19 00:28:52,438] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 582
 INFO [2020-01-19 00:28:52,439] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 676
 INFO [2020-01-19 00:28:52,439] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 805
 INFO [2020-01-19 00:28:52,439] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 847
 INFO [2020-01-19 00:28:52,439] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 909
 INFO [2020-01-19 00:28:52,440] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 590
 INFO [2020-01-19 00:28:52,440] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 903
 INFO [2020-01-19 00:28:52,440] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 617
 INFO [2020-01-19 00:28:52,441] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 481
 INFO [2020-01-19 00:28:52,441] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 872
 INFO [2020-01-19 00:28:52,441] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 613
 INFO [2020-01-19 00:28:52,442] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 748
 INFO [2020-01-19 00:28:52,442] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 724
 INFO [2020-01-19 00:28:52,443] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 726
 INFO [2020-01-19 00:28:52,443] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 657
 INFO [2020-01-19 00:28:52,444] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 725
 INFO [2020-01-19 00:28:52,444] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 867
 INFO [2020-01-19 00:28:52,445] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 780
 INFO [2020-01-19 00:28:52,445] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 626
 INFO [2020-01-19 00:28:52,445] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 645
 INFO [2020-01-19 00:28:52,445] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 796
 INFO [2020-01-19 00:28:52,445] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 766
 INFO [2020-01-19 00:28:52,445] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 487
 INFO [2020-01-19 00:28:52,445] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 888
 INFO [2020-01-19 00:28:52,445] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 922
 INFO [2020-01-19 00:28:52,446] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 584
 INFO [2020-01-19 00:28:52,446] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 740
 INFO [2020-01-19 00:28:52,446] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 674
 INFO [2020-01-19 00:28:52,446] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 594
 INFO [2020-01-19 00:28:52,446] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 482
 INFO [2020-01-19 00:28:52,446] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 667
 INFO [2020-01-19 00:28:52,446] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 788
 INFO [2020-01-19 00:28:52,447] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 678
 INFO [2020-01-19 00:28:52,447] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 685
 INFO [2020-01-19 00:28:52,447] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 829
 INFO [2020-01-19 00:28:52,447] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 672
 INFO [2020-01-19 00:28:52,447] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 569
 INFO [2020-01-19 00:28:52,447] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 775
 INFO [2020-01-19 00:28:52,447] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 904
 INFO [2020-01-19 00:28:52,447] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 694
 INFO [2020-01-19 00:28:52,447] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 608
 INFO [2020-01-19 00:28:52,447] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 763
 INFO [2020-01-19 00:28:52,448] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 573
 INFO [2020-01-19 00:28:52,448] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 632
 INFO [2020-01-19 00:28:52,448] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 476
 INFO [2020-01-19 00:28:52,448] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 791
 INFO [2020-01-19 00:28:52,448] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 699
 INFO [2020-01-19 00:28:52,448] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 777
 INFO [2020-01-19 00:28:52,448] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 673
 INFO [2020-01-19 00:28:52,448] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 603
 INFO [2020-01-19 00:28:52,448] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 729
 INFO [2020-01-19 00:28:52,449] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 886
 INFO [2020-01-19 00:28:52,449] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 581
 INFO [2020-01-19 00:28:52,449] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 675
 INFO [2020-01-19 00:28:52,449] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 906
 INFO [2020-01-19 00:28:52,449] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 843
 INFO [2020-01-19 00:28:52,449] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 878
 INFO [2020-01-19 00:28:52,449] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 755
 INFO [2020-01-19 00:28:52,449] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 580
 INFO [2020-01-19 00:28:52,449] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 622
 INFO [2020-01-19 00:28:52,449] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 630
 INFO [2020-01-19 00:28:52,450] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 692
 INFO [2020-01-19 00:28:52,450] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 778
 INFO [2020-01-19 00:28:52,450] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 808
 INFO [2020-01-19 00:28:52,450] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 773
 INFO [2020-01-19 00:28:52,450] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 841
 INFO [2020-01-19 00:28:52,450] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 879
 INFO [2020-01-19 00:28:52,450] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 658
 INFO [2020-01-19 00:28:52,450] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 711
 INFO [2020-01-19 00:28:52,450] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 753
 INFO [2020-01-19 00:28:52,451] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 576
 INFO [2020-01-19 00:28:52,451] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 662
 INFO [2020-01-19 00:28:52,451] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 716
 INFO [2020-01-19 00:28:52,451] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 745
 INFO [2020-01-19 00:28:52,451] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 566
 INFO [2020-01-19 00:28:52,451] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 774
 INFO [2020-01-19 00:28:52,451] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 862
 INFO [2020-01-19 00:28:52,451] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 570
 INFO [2020-01-19 00:28:52,451] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 702
 INFO [2020-01-19 00:28:52,451] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 483
 INFO [2020-01-19 00:28:52,453] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 851
 INFO [2020-01-19 00:28:52,453] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 855
 INFO [2020-01-19 00:28:52,453] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 653
 INFO [2020-01-19 00:28:52,454] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 733
 INFO [2020-01-19 00:28:52,454] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 704
 INFO [2020-01-19 00:28:52,454] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 767
 INFO [2020-01-19 00:28:52,454] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 731
 INFO [2020-01-19 00:28:52,454] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 877
 INFO [2020-01-19 00:28:52,454] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 918
 INFO [2020-01-19 00:28:52,454] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 727
 INFO [2020-01-19 00:28:52,455] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 739
 INFO [2020-01-19 00:28:52,456] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 574
 INFO [2020-01-19 00:28:52,459] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 650
 INFO [2020-01-19 00:28:52,459] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 905
 INFO [2020-01-19 00:28:52,459] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 663
 INFO [2020-01-19 00:28:52,459] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 690
 INFO [2020-01-19 00:28:52,459] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 770
 INFO [2020-01-19 00:28:52,460] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 896
 INFO [2020-01-19 00:28:52,460] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 838
 INFO [2020-01-19 00:28:52,460] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 921
 INFO [2020-01-19 00:28:52,460] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 698
 INFO [2020-01-19 00:28:52,460] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 588
 INFO [2020-01-19 00:28:52,460] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 794
 INFO [2020-01-19 00:28:52,461] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 605
 INFO [2020-01-19 00:28:52,461] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 619
 INFO [2020-01-19 00:28:52,472] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_28_piece0 on 172.18.0.6:42495 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:52,476] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_28_piece0 on 172.18.0.8:44045 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:52,477] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_28_piece0 on 172.18.0.7:37317 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:52,480] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_28_piece0 on zeppelin:33185 in memory (size: 7.2 KB, free: 366.3 MB)
 INFO [2020-01-19 00:28:52,512] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 772
 INFO [2020-01-19 00:28:52,512] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 752
 INFO [2020-01-19 00:28:52,512] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 853
 INFO [2020-01-19 00:28:52,513] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 858
 INFO [2020-01-19 00:28:52,513] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 485
 INFO [2020-01-19 00:28:52,513] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 785
 INFO [2020-01-19 00:28:52,513] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 873
 INFO [2020-01-19 00:28:52,513] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 737
 INFO [2020-01-19 00:28:52,513] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 860
 INFO [2020-01-19 00:28:52,513] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 916
 INFO [2020-01-19 00:28:52,513] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 882
 INFO [2020-01-19 00:28:52,513] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 717
 INFO [2020-01-19 00:28:52,514] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 735
 INFO [2020-01-19 00:28:52,514] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 722
 INFO [2020-01-19 00:28:52,514] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 891
 INFO [2020-01-19 00:28:52,514] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 802
 INFO [2020-01-19 00:28:52,514] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 762
 INFO [2020-01-19 00:28:52,514] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 857
 INFO [2020-01-19 00:28:52,514] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 756
 INFO [2020-01-19 00:28:52,514] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 734
 INFO [2020-01-19 00:28:52,514] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 665
 INFO [2020-01-19 00:28:52,514] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 686
 INFO [2020-01-19 00:28:52,527] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned shuffle 4
 INFO [2020-01-19 00:28:52,527] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 642
 INFO [2020-01-19 00:28:52,527] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 757
 INFO [2020-01-19 00:28:52,532] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_27_piece0 on zeppelin:33185 in memory (size: 4.3 KB, free: 366.3 MB)
 INFO [2020-01-19 00:28:52,538] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_27_piece0 on 172.18.0.6:42495 in memory (size: 4.3 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:52,548] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 741
 INFO [2020-01-19 00:28:52,548] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 782
 INFO [2020-01-19 00:28:52,549] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 781
 INFO [2020-01-19 00:28:52,549] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 827
 INFO [2020-01-19 00:28:52,551] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 585
 INFO [2020-01-19 00:28:52,552] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 895
 INFO [2020-01-19 00:28:52,552] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 919
 INFO [2020-01-19 00:28:52,552] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 920
 INFO [2020-01-19 00:28:52,552] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 589
 INFO [2020-01-19 00:28:52,554] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned shuffle 5
 INFO [2020-01-19 00:28:52,558] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 881
 INFO [2020-01-19 00:28:52,558] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 486
 INFO [2020-01-19 00:28:52,559] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 583
 INFO [2020-01-19 00:28:52,559] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 677
 INFO [2020-01-19 00:28:52,559] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 833
 INFO [2020-01-19 00:28:52,565] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_22_piece0 on zeppelin:33185 in memory (size: 7.2 KB, free: 366.3 MB)
 INFO [2020-01-19 00:28:52,567] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_22_piece0 on 172.18.0.6:42495 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:52,568] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_22_piece0 on 172.18.0.7:37317 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:52,581] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_22_piece0 on 172.18.0.8:44045 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:52,620] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 826
 INFO [2020-01-19 00:28:52,620] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 684
 INFO [2020-01-19 00:28:52,620] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 924
 INFO [2020-01-19 00:28:52,621] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 620
 INFO [2020-01-19 00:28:52,621] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 627
 INFO [2020-01-19 00:28:52,622] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 728
 INFO [2020-01-19 00:28:52,622] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 779
 INFO [2020-01-19 00:28:52,622] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 846
 INFO [2020-01-19 00:28:52,622] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 761
 INFO [2020-01-19 00:28:52,623] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 730
 INFO [2020-01-19 00:28:52,623] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 742
 INFO [2020-01-19 00:28:52,624] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 856
 INFO [2020-01-19 00:28:52,624] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 586
 INFO [2020-01-19 00:28:52,625] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 884
 INFO [2020-01-19 00:28:52,625] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 738
 INFO [2020-01-19 00:28:52,626] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 625
 INFO [2020-01-19 00:28:52,626] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 897
 INFO [2020-01-19 00:28:52,627] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 671
 INFO [2020-01-19 00:28:52,627] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 664
 INFO [2020-01-19 00:28:52,628] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 661
 INFO [2020-01-19 00:28:52,628] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 759
 INFO [2020-01-19 00:28:52,629] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 615
 INFO [2020-01-19 00:28:52,629] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 806
 INFO [2020-01-19 00:28:52,630] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 691
 INFO [2020-01-19 00:28:52,630] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 795
 INFO [2020-01-19 00:28:52,631] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 478
 INFO [2020-01-19 00:28:52,631] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 670
 INFO [2020-01-19 00:28:52,632] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 751
 INFO [2020-01-19 00:28:52,632] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 666
 INFO [2020-01-19 00:28:52,633] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 902
 INFO [2020-01-19 00:28:52,633] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 825
 INFO [2020-01-19 00:28:52,633] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 852
 INFO [2020-01-19 00:28:52,634] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 844
 INFO [2020-01-19 00:28:52,634] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 836
 INFO [2020-01-19 00:28:52,634] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 828
 INFO [2020-01-19 00:28:52,635] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 910
 INFO [2020-01-19 00:28:52,642] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_26_piece0 on 172.18.0.6:42495 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:52,643] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_26_piece0 on 172.18.0.7:37317 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:52,657] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_26_piece0 on 172.18.0.8:44045 in memory (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:52,658] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_26_piece0 on zeppelin:33185 in memory (size: 7.2 KB, free: 366.3 MB)
 INFO [2020-01-19 00:28:52,673] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 798
 INFO [2020-01-19 00:28:52,674] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 746
 INFO [2020-01-19 00:28:52,674] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 901
 INFO [2020-01-19 00:28:52,675] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 575
 INFO [2020-01-19 00:28:52,675] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 593
 INFO [2020-01-19 00:28:52,675] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 754
 INFO [2020-01-19 00:28:52,676] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 484
 INFO [2020-01-19 00:28:52,676] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 654
 INFO [2020-01-19 00:28:52,676] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 880
 INFO [2020-01-19 00:28:52,679] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned shuffle 6
 INFO [2020-01-19 00:28:52,696] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_31_piece0 on 172.18.0.7:37317 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:52,698] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_31_piece0 on zeppelin:33185 in memory (size: 5.8 KB, free: 366.3 MB)
 INFO [2020-01-19 00:28:52,702] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_31_piece0 on 172.18.0.6:42495 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:52,713] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_31_piece0 on 172.18.0.8:44045 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:52,724] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 861
 INFO [2020-01-19 00:28:52,724] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 641
 INFO [2020-01-19 00:28:52,724] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 660
 INFO [2020-01-19 00:28:52,724] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 792
 INFO [2020-01-19 00:28:52,725] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 869
 INFO [2020-01-19 00:28:52,732] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_29_piece0 on 172.18.0.7:37317 in memory (size: 4.3 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:52,737] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_29_piece0 on zeppelin:33185 in memory (size: 4.3 KB, free: 366.3 MB)
 INFO [2020-01-19 00:28:52,745] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 892
 INFO [2020-01-19 00:28:52,746] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 824
 INFO [2020-01-19 00:28:52,746] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 760
 INFO [2020-01-19 00:28:52,747] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 602
 INFO [2020-01-19 00:28:52,748] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned shuffle 3
 INFO [2020-01-19 00:28:52,748] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 604
 INFO [2020-01-19 00:28:52,754] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 621
 INFO [2020-01-19 00:28:52,755] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 631
 INFO [2020-01-19 00:28:52,756] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 708
 INFO [2020-01-19 00:28:52,757] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 800
 INFO [2020-01-19 00:28:52,757] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 578
 INFO [2020-01-19 00:28:52,758] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 701
 INFO [2020-01-19 00:28:52,758] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 864
 INFO [2020-01-19 00:28:52,758] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 639
 INFO [2020-01-19 00:28:52,759] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 744
 INFO [2020-01-19 00:28:52,759] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 789
 INFO [2020-01-19 00:28:52,760] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 783
 INFO [2020-01-19 00:28:52,760] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 572
 INFO [2020-01-19 00:28:52,760] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 705
 INFO [2020-01-19 00:28:52,761] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 700
 INFO [2020-01-19 00:28:52,761] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 907
 INFO [2020-01-19 00:28:52,762] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 732
 INFO [2020-01-19 00:28:52,762] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 634
 INFO [2020-01-19 00:28:52,763] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 659
 INFO [2020-01-19 00:28:52,763] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 714
 INFO [2020-01-19 00:28:52,763] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 680
 INFO [2020-01-19 00:28:52,763] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 638
 INFO [2020-01-19 00:28:52,764] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 655
 INFO [2020-01-19 00:28:52,764] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 611
 INFO [2020-01-19 00:28:52,764] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 765
 INFO [2020-01-19 00:28:52,765] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 596
 INFO [2020-01-19 00:28:52,765] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 865
 INFO [2020-01-19 00:28:52,765] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 803
 INFO [2020-01-19 00:28:52,766] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 477
 INFO [2020-01-19 00:28:52,766] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 768
 INFO [2020-01-19 00:28:52,766] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 840
 INFO [2020-01-19 00:28:52,767] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 809
 INFO [2020-01-19 00:28:52,767] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 736
 INFO [2020-01-19 00:28:52,767] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 616
 INFO [2020-01-19 00:28:52,768] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 859
 INFO [2020-01-19 00:28:52,768] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 597
 INFO [2020-01-19 00:28:52,768] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 682
 INFO [2020-01-19 00:28:52,769] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 837
 INFO [2020-01-19 00:28:52,780] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_30_piece0 on 172.18.0.7:37317 in memory (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:52,781] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_30_piece0 on zeppelin:33185 in memory (size: 5.8 KB, free: 366.3 MB)
 INFO [2020-01-19 00:28:52,796] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 786
 INFO [2020-01-19 00:28:52,796] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 917
 INFO [2020-01-19 00:28:52,797] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 649
 INFO [2020-01-19 00:28:52,797] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 709
 INFO [2020-01-19 00:28:52,798] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 652
 INFO [2020-01-19 00:28:52,798] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 801
 INFO [2020-01-19 00:28:52,798] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 885
 INFO [2020-01-19 00:28:52,812] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_23_piece0 on 172.18.0.8:44045 in memory (size: 4.3 KB, free: 912.3 MB)
 INFO [2020-01-19 00:28:52,813] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_23_piece0 on zeppelin:33185 in memory (size: 4.3 KB, free: 366.3 MB)
 INFO [2020-01-19 00:28:52,830] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 637
 INFO [2020-01-19 00:28:52,831] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 787
 INFO [2020-01-19 00:28:52,832] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 887
 INFO [2020-01-19 00:28:52,832] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 712
 INFO [2020-01-19 00:28:52,833] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 720
 INFO [2020-01-19 00:28:52,833] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 831
 INFO [2020-01-19 00:28:52,834] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 687
 INFO [2020-01-19 00:28:52,834] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 706
 INFO [2020-01-19 00:28:52,834] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 889
 INFO [2020-01-19 00:28:52,835] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 635
 INFO [2020-01-19 00:28:52,835] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 579
 INFO [2020-01-19 00:28:52,835] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 610
 INFO [2020-01-19 00:28:52,836] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 669
 INFO [2020-01-19 00:28:52,836] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 646
 INFO [2020-01-19 00:28:52,836] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 871
 INFO [2020-01-19 00:28:52,837] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 866
 INFO [2020-01-19 00:28:52,837] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 564
 INFO [2020-01-19 00:28:52,837] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 797
 INFO [2020-01-19 00:28:52,837] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 683
 INFO [2020-01-19 00:28:52,838] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 749
 INFO [2020-01-19 00:29:52,372] ({pool-2-thread-16} SchedulerFactory.java[jobStarted]:114) - Job 20200119-002429_1902509145 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:29:52,480] ({pool-2-thread-16} SchedulerFactory.java[jobFinished]:120) - Job 20200119-002429_1902509145 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:30:26,800] ({pool-2-thread-9} SchedulerFactory.java[jobStarted]:114) - Job 20200119-002429_1902509145 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:30:26,905] ({pool-2-thread-9} SchedulerFactory.java[jobFinished]:120) - Job 20200119-002429_1902509145 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:32:00,375] ({pool-2-thread-17} SchedulerFactory.java[jobStarted]:114) - Job 20200119-002952_815014234 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:32:00,428] ({pool-2-thread-17} SchedulerFactory.java[jobFinished]:120) - Job 20200119-002952_815014234 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:32:08,771] ({pool-2-thread-10} SchedulerFactory.java[jobStarted]:114) - Job 20200119-002429_1902509145 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:32:08,842] ({pool-2-thread-10} SchedulerFactory.java[jobFinished]:120) - Job 20200119-002429_1902509145 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:33:09,276] ({pool-2-thread-18} SchedulerFactory.java[jobStarted]:114) - Job 20200119-001021_531760765 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:33:09,392] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: runJob at PythonRDD.scala:153
 INFO [2020-01-19 00:33:09,393] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 25 (runJob at PythonRDD.scala:153) with 1 output partitions
 INFO [2020-01-19 00:33:09,394] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 33 (runJob at PythonRDD.scala:153)
 INFO [2020-01-19 00:33:09,394] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-01-19 00:33:09,395] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-01-19 00:33:09,395] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 33 (PythonRDD[109] at RDD at PythonRDD.scala:53), which has no missing parents
 INFO [2020-01-19 00:33:09,398] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_33 stored as values in memory (estimated size 10.2 KB, free 366.3 MB)
 INFO [2020-01-19 00:33:09,400] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_33_piece0 stored as bytes in memory (estimated size 5.8 KB, free 366.3 MB)
 INFO [2020-01-19 00:33:09,400] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_33_piece0 in memory on zeppelin:33185 (size: 5.8 KB, free: 366.3 MB)
 INFO [2020-01-19 00:33:09,401] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 33 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:33:09,402] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 33 (PythonRDD[109] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))
 INFO [2020-01-19 00:33:09,402] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 33.0 with 1 tasks
 INFO [2020-01-19 00:33:09,403] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 33.0 (TID 94, 172.18.0.6, executor 2, partition 0, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:33:09,427] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_33_piece0 in memory on 172.18.0.6:42495 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:33:09,508] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 33.0 (TID 94) in 105 ms on 172.18.0.6 (executor 2) (1/1)
 INFO [2020-01-19 00:33:09,508] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 33.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:33:09,508] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 33 (runJob at PythonRDD.scala:153) finished in 0.111 s
 INFO [2020-01-19 00:33:09,509] ({Thread-20} Logging.scala[logInfo]:54) - Job 25 finished: runJob at PythonRDD.scala:153, took 0.117006 s
 INFO [2020-01-19 00:33:09,532] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: runJob at PythonRDD.scala:153
 INFO [2020-01-19 00:33:09,533] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 26 (runJob at PythonRDD.scala:153) with 4 output partitions
 INFO [2020-01-19 00:33:09,533] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 34 (runJob at PythonRDD.scala:153)
 INFO [2020-01-19 00:33:09,534] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-01-19 00:33:09,534] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-01-19 00:33:09,534] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 34 (PythonRDD[110] at RDD at PythonRDD.scala:53), which has no missing parents
 INFO [2020-01-19 00:33:09,536] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_34 stored as values in memory (estimated size 10.2 KB, free 366.3 MB)
 INFO [2020-01-19 00:33:09,543] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_34_piece0 stored as bytes in memory (estimated size 5.8 KB, free 366.3 MB)
 INFO [2020-01-19 00:33:09,544] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_34_piece0 in memory on zeppelin:33185 (size: 5.8 KB, free: 366.3 MB)
 INFO [2020-01-19 00:33:09,546] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 34 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:33:09,547] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 4 missing tasks from ResultStage 34 (PythonRDD[110] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(1, 2, 3, 4))
 INFO [2020-01-19 00:33:09,547] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 34.0 with 4 tasks
 INFO [2020-01-19 00:33:09,548] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 34.0 (TID 95, 172.18.0.7, executor 1, partition 1, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:33:09,548] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 1.0 in stage 34.0 (TID 96, 172.18.0.8, executor 0, partition 2, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:33:09,549] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 2.0 in stage 34.0 (TID 97, 172.18.0.6, executor 2, partition 3, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:33:09,549] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 3.0 in stage 34.0 (TID 98, 172.18.0.7, executor 1, partition 4, PROCESS_LOCAL, 7888 bytes)
 INFO [2020-01-19 00:33:09,588] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_34_piece0 in memory on 172.18.0.6:42495 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:33:09,602] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_34_piece0 in memory on 172.18.0.7:37317 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:33:09,621] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_34_piece0 in memory on 172.18.0.8:44045 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:33:09,676] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 2.0 in stage 34.0 (TID 97) in 128 ms on 172.18.0.6 (executor 2) (1/4)
 INFO [2020-01-19 00:33:09,725] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 1.0 in stage 34.0 (TID 96) in 177 ms on 172.18.0.8 (executor 0) (2/4)
 INFO [2020-01-19 00:33:09,811] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 34.0 (TID 95) in 263 ms on 172.18.0.7 (executor 1) (3/4)
 INFO [2020-01-19 00:33:09,812] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 3.0 in stage 34.0 (TID 98) in 263 ms on 172.18.0.7 (executor 1) (4/4)
 INFO [2020-01-19 00:33:09,812] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 34.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:33:09,813] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 34 (runJob at PythonRDD.scala:153) finished in 0.278 s
 INFO [2020-01-19 00:33:09,821] ({Thread-20} Logging.scala[logInfo]:54) - Job 26 finished: runJob at PythonRDD.scala:153, took 0.289295 s
 INFO [2020-01-19 00:33:09,915] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: runJob at PythonRDD.scala:153
 INFO [2020-01-19 00:33:09,920] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 27 (runJob at PythonRDD.scala:153) with 1 output partitions
 INFO [2020-01-19 00:33:09,924] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 35 (runJob at PythonRDD.scala:153)
 INFO [2020-01-19 00:33:09,929] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-01-19 00:33:09,930] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-01-19 00:33:09,933] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 35 (PythonRDD[111] at RDD at PythonRDD.scala:53), which has no missing parents
 INFO [2020-01-19 00:33:09,938] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_35 stored as values in memory (estimated size 10.2 KB, free 366.2 MB)
 INFO [2020-01-19 00:33:09,948] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_35_piece0 stored as bytes in memory (estimated size 5.8 KB, free 366.2 MB)
 INFO [2020-01-19 00:33:09,949] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_35_piece0 in memory on zeppelin:33185 (size: 5.8 KB, free: 366.3 MB)
 INFO [2020-01-19 00:33:09,959] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 35 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:33:09,963] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 35 (PythonRDD[111] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(5))
 INFO [2020-01-19 00:33:09,967] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 35.0 with 1 tasks
 INFO [2020-01-19 00:33:09,976] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 35.0 (TID 99, 172.18.0.7, executor 1, partition 5, PROCESS_LOCAL, 7920 bytes)
 INFO [2020-01-19 00:33:10,005] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_35_piece0 in memory on 172.18.0.7:37317 (size: 5.8 KB, free: 912.3 MB)
 INFO [2020-01-19 00:33:10,129] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 35.0 (TID 99) in 152 ms on 172.18.0.7 (executor 1) (1/1)
 INFO [2020-01-19 00:33:10,129] ({task-result-getter-0} Logging.scala[logInfo]:54) - Removed TaskSet 35.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:33:10,129] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 35 (runJob at PythonRDD.scala:153) finished in 0.192 s
 INFO [2020-01-19 00:33:10,130] ({Thread-20} Logging.scala[logInfo]:54) - Job 27 finished: runJob at PythonRDD.scala:153, took 0.215027 s
 INFO [2020-01-19 00:33:10,281] ({pool-2-thread-18} SchedulerFactory.java[jobFinished]:120) - Job 20200119-001021_531760765 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:33:45,890] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:33:46,032] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: collect at <ipython-input-66-78e246484f6b>:4
 INFO [2020-01-19 00:33:46,035] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Registering RDD 114 (collect at <ipython-input-66-78e246484f6b>:4)
 INFO [2020-01-19 00:33:46,036] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 28 (collect at <ipython-input-66-78e246484f6b>:4) with 1 output partitions
 INFO [2020-01-19 00:33:46,036] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 37 (collect at <ipython-input-66-78e246484f6b>:4)
 INFO [2020-01-19 00:33:46,036] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List(ShuffleMapStage 36)
 INFO [2020-01-19 00:33:46,038] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List(ShuffleMapStage 36)
 INFO [2020-01-19 00:33:46,038] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ShuffleMapStage 36 (MapPartitionsRDD[114] at collect at <ipython-input-66-78e246484f6b>:4), which has no missing parents
 INFO [2020-01-19 00:33:46,044] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_36 stored as values in memory (estimated size 13.6 KB, free 366.2 MB)
 INFO [2020-01-19 00:33:46,045] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_36_piece0 stored as bytes in memory (estimated size 7.2 KB, free 366.2 MB)
 INFO [2020-01-19 00:33:46,046] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_36_piece0 in memory on zeppelin:33185 (size: 7.2 KB, free: 366.3 MB)
 INFO [2020-01-19 00:33:46,046] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 36 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:33:46,046] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 6 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[114] at collect at <ipython-input-66-78e246484f6b>:4) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
 INFO [2020-01-19 00:33:46,047] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 36.0 with 6 tasks
 INFO [2020-01-19 00:33:46,048] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 36.0 (TID 100, 172.18.0.7, executor 1, partition 0, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:33:46,048] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 1.0 in stage 36.0 (TID 101, 172.18.0.6, executor 2, partition 1, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:33:46,049] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 2.0 in stage 36.0 (TID 102, 172.18.0.8, executor 0, partition 2, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:33:46,050] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 3.0 in stage 36.0 (TID 103, 172.18.0.7, executor 1, partition 3, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:33:46,053] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 4.0 in stage 36.0 (TID 104, 172.18.0.6, executor 2, partition 4, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:33:46,054] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 5.0 in stage 36.0 (TID 105, 172.18.0.8, executor 0, partition 5, PROCESS_LOCAL, 7909 bytes)
 INFO [2020-01-19 00:33:46,075] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_36_piece0 in memory on 172.18.0.6:42495 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:33:46,119] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_36_piece0 in memory on 172.18.0.8:44045 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:33:46,130] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_36_piece0 in memory on 172.18.0.7:37317 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:33:46,133] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 1.0 in stage 36.0 (TID 101) in 85 ms on 172.18.0.6 (executor 2) (1/6)
 INFO [2020-01-19 00:33:46,186] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 4.0 in stage 36.0 (TID 104) in 135 ms on 172.18.0.6 (executor 2) (2/6)
 INFO [2020-01-19 00:33:46,208] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 36.0 (TID 100) in 160 ms on 172.18.0.7 (executor 1) (3/6)
 INFO [2020-01-19 00:33:46,209] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 2.0 in stage 36.0 (TID 102) in 160 ms on 172.18.0.8 (executor 0) (4/6)
 INFO [2020-01-19 00:33:46,234] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 3.0 in stage 36.0 (TID 103) in 184 ms on 172.18.0.7 (executor 1) (5/6)
 INFO [2020-01-19 00:33:46,260] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 5.0 in stage 36.0 (TID 105) in 206 ms on 172.18.0.8 (executor 0) (6/6)
 INFO [2020-01-19 00:33:46,260] ({task-result-getter-2} Logging.scala[logInfo]:54) - Removed TaskSet 36.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:33:46,261] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ShuffleMapStage 36 (collect at <ipython-input-66-78e246484f6b>:4) finished in 0.219 s
 INFO [2020-01-19 00:33:46,261] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - looking for newly runnable stages
 INFO [2020-01-19 00:33:46,261] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - running: Set()
 INFO [2020-01-19 00:33:46,262] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - waiting: Set(ResultStage 37)
 INFO [2020-01-19 00:33:46,263] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - failed: Set()
 INFO [2020-01-19 00:33:46,265] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 37 (MapPartitionsRDD[117] at collect at <ipython-input-66-78e246484f6b>:4), which has no missing parents
 INFO [2020-01-19 00:33:46,269] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_37 stored as values in memory (estimated size 8.1 KB, free 366.2 MB)
 INFO [2020-01-19 00:33:46,271] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_37_piece0 stored as bytes in memory (estimated size 4.3 KB, free 366.2 MB)
 INFO [2020-01-19 00:33:46,272] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_37_piece0 in memory on zeppelin:33185 (size: 4.3 KB, free: 366.3 MB)
 INFO [2020-01-19 00:33:46,276] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 37 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:33:46,277] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[117] at collect at <ipython-input-66-78e246484f6b>:4) (first 15 tasks are for partitions Vector(0))
 INFO [2020-01-19 00:33:46,277] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 37.0 with 1 tasks
 INFO [2020-01-19 00:33:46,279] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 37.0 (TID 106, 172.18.0.7, executor 1, partition 0, NODE_LOCAL, 7771 bytes)
 INFO [2020-01-19 00:33:46,289] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_37_piece0 in memory on 172.18.0.7:37317 (size: 4.3 KB, free: 912.3 MB)
 INFO [2020-01-19 00:33:46,294] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Asked to send map output locations for shuffle 8 to 172.18.0.7:47870
 INFO [2020-01-19 00:33:46,316] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 37.0 (TID 106) in 37 ms on 172.18.0.7 (executor 1) (1/1)
 INFO [2020-01-19 00:33:46,318] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 37.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:33:46,319] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 37 (collect at <ipython-input-66-78e246484f6b>:4) finished in 0.053 s
 INFO [2020-01-19 00:33:46,322] ({Thread-20} Logging.scala[logInfo]:54) - Job 28 finished: collect at <ipython-input-66-78e246484f6b>:4, took 0.289437 s
 INFO [2020-01-19 00:33:46,351] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler interpreter_1789863315
 INFO [2020-01-19 00:33:53,357] ({pool-2-thread-19} SchedulerFactory.java[jobStarted]:114) - Job 20200119-000328_1025815385 started by scheduler interpreter_1789863315
 INFO [2020-01-19 00:33:53,460] ({Thread-20} Logging.scala[logInfo]:54) - Starting job: collect at <ipython-input-68-4294447afb77>:4
 INFO [2020-01-19 00:33:53,461] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Registering RDD 120 (collect at <ipython-input-68-4294447afb77>:4)
 INFO [2020-01-19 00:33:53,461] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 29 (collect at <ipython-input-68-4294447afb77>:4) with 1 output partitions
 INFO [2020-01-19 00:33:53,462] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 39 (collect at <ipython-input-68-4294447afb77>:4)
 INFO [2020-01-19 00:33:53,462] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List(ShuffleMapStage 38)
 INFO [2020-01-19 00:33:53,462] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List(ShuffleMapStage 38)
 INFO [2020-01-19 00:33:53,462] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ShuffleMapStage 38 (MapPartitionsRDD[120] at collect at <ipython-input-68-4294447afb77>:4), which has no missing parents
 INFO [2020-01-19 00:33:53,470] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_38 stored as values in memory (estimated size 13.6 KB, free 366.2 MB)
 INFO [2020-01-19 00:33:53,471] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_38_piece0 stored as bytes in memory (estimated size 7.2 KB, free 366.2 MB)
 INFO [2020-01-19 00:33:53,472] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_38_piece0 in memory on zeppelin:33185 (size: 7.2 KB, free: 366.3 MB)
 INFO [2020-01-19 00:33:53,472] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 38 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:33:53,473] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 6 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[120] at collect at <ipython-input-68-4294447afb77>:4) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
 INFO [2020-01-19 00:33:53,473] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 38.0 with 6 tasks
 INFO [2020-01-19 00:33:53,474] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 38.0 (TID 107, 172.18.0.6, executor 2, partition 0, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:33:53,475] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 1.0 in stage 38.0 (TID 108, 172.18.0.8, executor 0, partition 1, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:33:53,476] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 2.0 in stage 38.0 (TID 109, 172.18.0.7, executor 1, partition 2, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:33:53,477] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 3.0 in stage 38.0 (TID 110, 172.18.0.6, executor 2, partition 3, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:33:53,477] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 4.0 in stage 38.0 (TID 111, 172.18.0.8, executor 0, partition 4, PROCESS_LOCAL, 7877 bytes)
 INFO [2020-01-19 00:33:53,478] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 5.0 in stage 38.0 (TID 112, 172.18.0.7, executor 1, partition 5, PROCESS_LOCAL, 7909 bytes)
 INFO [2020-01-19 00:33:53,500] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_38_piece0 in memory on 172.18.0.8:44045 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:33:53,511] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_38_piece0 in memory on 172.18.0.6:42495 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:33:53,542] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_38_piece0 in memory on 172.18.0.7:37317 (size: 7.2 KB, free: 912.3 MB)
 INFO [2020-01-19 00:33:53,580] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 5.0 in stage 38.0 (TID 112) in 102 ms on 172.18.0.7 (executor 1) (1/6)
 INFO [2020-01-19 00:33:53,589] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 4.0 in stage 38.0 (TID 111) in 112 ms on 172.18.0.8 (executor 0) (2/6)
 INFO [2020-01-19 00:33:53,594] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 1.0 in stage 38.0 (TID 108) in 119 ms on 172.18.0.8 (executor 0) (3/6)
 INFO [2020-01-19 00:33:53,604] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 38.0 (TID 107) in 130 ms on 172.18.0.6 (executor 2) (4/6)
 INFO [2020-01-19 00:33:53,629] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 2.0 in stage 38.0 (TID 109) in 153 ms on 172.18.0.7 (executor 1) (5/6)
 INFO [2020-01-19 00:33:53,638] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 3.0 in stage 38.0 (TID 110) in 161 ms on 172.18.0.6 (executor 2) (6/6)
 INFO [2020-01-19 00:33:53,638] ({task-result-getter-1} Logging.scala[logInfo]:54) - Removed TaskSet 38.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:33:53,639] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ShuffleMapStage 38 (collect at <ipython-input-68-4294447afb77>:4) finished in 0.173 s
 INFO [2020-01-19 00:33:53,640] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - looking for newly runnable stages
 INFO [2020-01-19 00:33:53,640] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - running: Set()
 INFO [2020-01-19 00:33:53,640] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - waiting: Set(ResultStage 39)
 INFO [2020-01-19 00:33:53,641] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - failed: Set()
 INFO [2020-01-19 00:33:53,642] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 39 (MapPartitionsRDD[123] at collect at <ipython-input-68-4294447afb77>:4), which has no missing parents
 INFO [2020-01-19 00:33:53,644] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_39 stored as values in memory (estimated size 8.1 KB, free 366.2 MB)
 INFO [2020-01-19 00:33:53,646] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_39_piece0 stored as bytes in memory (estimated size 4.3 KB, free 366.2 MB)
 INFO [2020-01-19 00:33:53,646] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_39_piece0 in memory on zeppelin:33185 (size: 4.3 KB, free: 366.3 MB)
 INFO [2020-01-19 00:33:53,647] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 39 from broadcast at DAGScheduler.scala:1161
 INFO [2020-01-19 00:33:53,647] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[123] at collect at <ipython-input-68-4294447afb77>:4) (first 15 tasks are for partitions Vector(0))
 INFO [2020-01-19 00:33:53,648] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 39.0 with 1 tasks
 INFO [2020-01-19 00:33:53,655] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 39.0 (TID 113, 172.18.0.6, executor 2, partition 0, NODE_LOCAL, 7771 bytes)
 INFO [2020-01-19 00:33:53,670] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_39_piece0 in memory on 172.18.0.6:42495 (size: 4.3 KB, free: 912.3 MB)
 INFO [2020-01-19 00:33:53,679] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Asked to send map output locations for shuffle 9 to 172.18.0.6:54036
 INFO [2020-01-19 00:33:53,699] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 39.0 (TID 113) in 44 ms on 172.18.0.6 (executor 2) (1/1)
 INFO [2020-01-19 00:33:53,699] ({task-result-getter-2} Logging.scala[logInfo]:54) - Removed TaskSet 39.0, whose tasks have all completed, from pool 
 INFO [2020-01-19 00:33:53,699] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 39 (collect at <ipython-input-68-4294447afb77>:4) finished in 0.057 s
 INFO [2020-01-19 00:33:53,700] ({Thread-20} Logging.scala[logInfo]:54) - Job 29 finished: collect at <ipython-input-68-4294447afb77>:4, took 0.239472 s
 INFO [2020-01-19 00:33:53,749] ({pool-2-thread-19} SchedulerFactory.java[jobFinished]:120) - Job 20200119-000328_1025815385 finished by scheduler interpreter_1789863315
